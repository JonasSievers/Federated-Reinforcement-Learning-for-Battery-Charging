{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient \n",
    "\n",
    "This notebook implements the DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import ddpg\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import utils.Dataloader as DL\n",
    "import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of training iterations. It determines how many times the agent will go through the entire training process, \n",
    "adjusting its policy and value functions based on collected experiences. A sufficient number of iterations is crucial \n",
    "to allow the agent to learn and adapt to the environment, improving its performance over time.\n",
    "\"\"\"\n",
    "num_iterations = 1500\n",
    "customer = 1\n",
    "\n",
    "# Params for collect\n",
    "\"\"\"\n",
    "Defines the number of initial steps where the agent collects experiences randomly before the training begins. \n",
    "This helps to populate the replay buffer with diverse initial data.\n",
    "A well-populated replay buffer provides a diverse set of experiences for the agent to learn from, \n",
    "enhancing the stability and effectiveness of training.\n",
    "\"\"\"\n",
    "initial_collect_steps = 1000\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of steps the agent takes to collect experiences in each training iteration. \n",
    "It controls the balance between exploration and exploitation during training.\n",
    "Adequate exploration is necessary for discovering optimal policies. \n",
    "Adjusting this parameter impacts how often the agent explores its environment and updates its knowledge.\n",
    "\"\"\"\n",
    "collect_steps_per_iteration = 2000\n",
    "\n",
    "\"\"\"\n",
    "Sets the capacity of the replay buffer, a memory structure storing past experiences for the agent to sample during training.\n",
    "A sufficiently large replay buffer allows the agent to store and learn from a diverse set of experiences, \n",
    "mitigating issues related to correlated data and improving sample efficiency.\n",
    "\"\"\"\n",
    "replay_buffer_capacity = 1000000\n",
    "\n",
    "\"\"\"\n",
    "Determines the standard deviation of the Ornstein-Uhlenbeck process, which introduces exploration noise in the action space.\n",
    "Exploration noise aids the agent in exploring its action space,\n",
    "preventing it from getting stuck in local optima and promoting more robust learning.\n",
    "\"\"\"\n",
    "ou_stddev = 0.2\n",
    "\n",
    "\"\"\"\n",
    "Introduces a damping term to the Ornstein-Uhlenbeck process, influencing the exploration noise.\n",
    "Damping helps control the intensity of exploration noise, \n",
    "allowing a balance between exploration and exploitation based on the task's requirements.\n",
    "\"\"\"\n",
    "ou_damping = 0.15\n",
    "\n",
    "# Params for target update\n",
    "\"\"\"\n",
    "Represents the soft update coefficient for updating target networks, \n",
    "determining the degree to which the target networks track the main networks.\n",
    "Soft updates help stabilize training by slowly blending target values, \n",
    "preventing abrupt changes and improving the convergence of the learning process.\n",
    "\"\"\"\n",
    "target_update_tau = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Defines how often the target networks are updated in terms of training steps.\n",
    "Controlling the update frequency balances stability and responsiveness, \n",
    "preventing the target networks from lagging too far behind or updating too frequently.\n",
    "\"\"\"\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for train\n",
    "\"\"\"\n",
    "Specifies the number of gradient descent steps taken on the training batch in each training iteration.\n",
    "Adjusting this parameter impacts the convergence speed of the training process, \n",
    "influencing how much the agent learns from each collected batch of experiences.\n",
    "\"\"\"\n",
    "train_steps_per_iteration = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the size of the training batch sampled from the replay buffer.\n",
    "The batch size affects the efficiency of training; \n",
    "a well-chosen size balances computational efficiency and the stability of the learning process.\n",
    "\"\"\"\n",
    "batch_size = 48 * 7\n",
    "\n",
    "\"\"\"\n",
    "Specifies the learning rate for the actor (policy) network during gradient descent.\n",
    "The learning rate controls the size of the step taken during optimization. \n",
    "A suitable learning rate ensures the model converges effectively without overshooting or getting stuck in local minima.\n",
    "\"\"\"\n",
    "actor_learning_rate = 1e-4\n",
    "\n",
    "\"\"\"\n",
    "Defines the learning rate for the critic (Q-value) network during gradient descent.\n",
    "Similar to the actor learning rate, an appropriate critic learning rate influences \n",
    "the convergence and stability of the critic network, which plays a crucial role in estimating Q-values.\n",
    "\"\"\"\n",
    "critic_learning_rate = 1e-3\n",
    "\n",
    "\"\"\"\n",
    "An optional parameter for clipping the gradient of the Q-value with respect to actions.\n",
    "Clipping gradients can prevent large updates that may destabilize training, \n",
    "acting as a form of regularization and improving the robustness of the learning process.\n",
    "\"\"\"\n",
    "dqda_clipping = None\n",
    "\n",
    "\"\"\"\n",
    "Specifies the loss function for temporal difference (TD) errors, \n",
    "representing the discrepancy between predicted and actual Q-values.\n",
    "The choice of loss function influences how the agent updates its value estimates. \n",
    "Huber loss, as specified here, is robust to outliers and provides a balance between mean squared error and mean absolute error.\n",
    "\"\"\"\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "\n",
    "\"\"\"\n",
    "Represents the discount factor applied to future rewards in the Q-value estimation.\n",
    "Discounting future rewards emphasizes the importance of immediate rewards, e\n",
    "nabling the agent to make more informed decisions that consider both short-term and long-term consequences.\n",
    "\"\"\"\n",
    "gamma = 0.99\n",
    "\n",
    "\"\"\"\n",
    "Scales the rewards during training.Scaling rewards helps to control the impact of reward magnitudes on the learning process, \n",
    "preventing issues related to overly large or small rewards.\n",
    "\"\"\"\n",
    "reward_scale_factor = 1.0\n",
    "\n",
    "\"\"\"An optional parameter for clipping gradients during training.\"\"\"\n",
    "gradient_clipping = None\n",
    "\n",
    "# Params for eval and checkpoints\n",
    "\"\"\"\n",
    "Specifies the number of episodes used for evaluating the agent's performance.\n",
    "Evaluating the agent's performance provides insights into its generalization \n",
    "capabilities and allows for monitoring progress over time.\"\"\"\n",
    "num_eval_episodes = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the frequency (in iterations) at which evaluations are performed.\n",
    "Regular evaluations help track the agent's progress, enabling the identification of potential issues and providing \n",
    "a basis for comparison between different training iterations.\n",
    "\"\"\"\n",
    "eval_interval = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = DL.get_customer_data(DL.loadData('../../data/load1011.csv'), DL.loadPrice('../../data/price.csv'), customer)\n",
    "data_eval = DL.get_customer_data(DL.loadData('../../data/load1112.csv'), DL.loadPrice('../../data/price.csv'), customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0:30</th>\n",
       "      <th>1:00</th>\n",
       "      <th>1:30</th>\n",
       "      <th>2:00</th>\n",
       "      <th>2:30</th>\n",
       "      <th>3:00</th>\n",
       "      <th>3:30</th>\n",
       "      <th>4:00</th>\n",
       "      <th>4:30</th>\n",
       "      <th>5:00</th>\n",
       "      <th>...</th>\n",
       "      <th>19:30</th>\n",
       "      <th>20:00</th>\n",
       "      <th>20:30</th>\n",
       "      <th>21:00</th>\n",
       "      <th>21:30</th>\n",
       "      <th>22:00</th>\n",
       "      <th>22:30</th>\n",
       "      <th>23:00</th>\n",
       "      <th>23:30</th>\n",
       "      <th>0:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0:30  1:00  1:30  2:00  2:30  3:00  3:30  4:00  4:30  5:00  ...  19:30  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "360   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "361   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "362   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "363   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "364   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "     20:00  20:30  21:00  21:30  22:00  22:30  23:00  23:30  0:00  \n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...   ...  \n",
       "360    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "361    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "362    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "363    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "364    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "\n",
       "[365 rows x 48 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare runner\n",
    "\n",
    "# Get or create the global step variable, which is a counter for the number of training steps\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# Create TensorFlow environments for training and evaluation using custom environment settings\n",
    "tf_env_train = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_train))\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_eval))\n",
    "\n",
    "## Define the actor network, responsible for generating actions based on observations\n",
    "actor_net = ddpg.actor_network.ActorNetwork(\n",
    "    input_tensor_spec=tf_env_train.observation_spec(),\n",
    "    output_tensor_spec=tf_env_train.action_spec(), \n",
    "    fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\n",
    "\n",
    "# Define the critic network, responsible for estimating the Q-values for state-action pairs\n",
    "critic_net = ddpg.critic_network.CriticNetwork(\n",
    "    input_tensor_spec=(tf_env_train.observation_spec(), tf_env_train.action_spec()),\n",
    "    joint_fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\n",
    "\n",
    "# Create a DDPG agent using the defined actor and critic networks, along with other parameters\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    tf_env_train.time_step_spec(),\n",
    "    tf_env_train.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate),\n",
    "    ou_stddev=ou_stddev, # Standard deviation for Ornstein-Uhlenbeck noise\n",
    "    ou_damping=ou_damping, # Damping term for Ornstein-Uhlenbeck noise\n",
    "    target_update_tau=target_update_tau, # Soft update coefficient for target networks\n",
    "    target_update_period=target_update_period, # Frequency of updating target networks\n",
    "    dqda_clipping=dqda_clipping, # Optional clipping of the gradient of Q-value with respect to actions\n",
    "    td_errors_loss_fn=td_errors_loss_fn, # Loss function for temporal difference errors\n",
    "    gamma=gamma, # Discount factor for future rewards\n",
    "    reward_scale_factor=reward_scale_factor, # Scaling factor for rewards during training\n",
    "    gradient_clipping=gradient_clipping, # Optional clipping of gradients during training\n",
    "    debug_summaries=False, # Disable debug summaries\n",
    "    summarize_grads_and_vars=False,  # Disable summarizing gradients and variables\n",
    "    train_step_counter=global_step,  # Use the global step as the train step counter\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env_train.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps,\n",
    ")\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration,\n",
    ")\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir='checkpoints/ddpg' + str(customer) + '/',\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    logdir='./log/ddpg' + str(customer) + '/', flush_millis=10000\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better performance\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:103: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:104: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Collect initial replay data\n",
    "initial_collect_driver.run()\n",
    "\n",
    "time_step = tf_env_train.reset()\n",
    "policy_state = collect_policy.get_initial_state(tf_env_train.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: Loss = 79.50125885009766\n",
      "step = 2: Loss = 80.4145736694336\n",
      "step = 3: Loss = 79.5152587890625\n",
      "step = 4: Loss = 78.52670288085938\n",
      "step = 5: Loss = 78.52630615234375\n",
      "step = 6: Loss = 77.98403930664062\n",
      "step = 7: Loss = 78.41439056396484\n",
      "step = 8: Loss = 76.90274047851562\n",
      "step = 9: Loss = 72.32514190673828\n",
      "step = 10: Loss = 72.81550598144531\n",
      "step = 11: Loss = 69.62451934814453\n",
      "step = 12: Loss = 67.4272232055664\n",
      "step = 13: Loss = 65.33473205566406\n",
      "step = 14: Loss = 62.390892028808594\n",
      "step = 15: Loss = 56.90119934082031\n",
      "step = 16: Loss = 53.05763244628906\n",
      "step = 17: Loss = 49.19469451904297\n",
      "step = 18: Loss = 43.41055679321289\n",
      "step = 19: Loss = 38.903324127197266\n",
      "step = 20: Loss = 35.6302490234375\n",
      "step = 21: Loss = 29.642175674438477\n",
      "step = 22: Loss = 26.0750789642334\n",
      "step = 23: Loss = 21.122026443481445\n",
      "step = 24: Loss = 17.91564178466797\n",
      "step = 25: Loss = 16.63941764831543\n",
      "step = 26: Loss = 17.601346969604492\n",
      "step = 27: Loss = 19.070890426635742\n",
      "step = 28: Loss = 20.009828567504883\n",
      "step = 29: Loss = 23.77846908569336\n",
      "step = 30: Loss = 23.502214431762695\n",
      "step = 31: Loss = 23.676597595214844\n",
      "step = 32: Loss = 25.796716690063477\n",
      "step = 33: Loss = 28.63698959350586\n",
      "step = 34: Loss = 27.782602310180664\n",
      "step = 35: Loss = 28.38564109802246\n",
      "step = 36: Loss = 26.138912200927734\n",
      "step = 37: Loss = 23.092777252197266\n",
      "step = 38: Loss = 24.623716354370117\n",
      "step = 39: Loss = 23.783618927001953\n",
      "step = 40: Loss = 23.9758358001709\n",
      "step = 41: Loss = 21.833620071411133\n",
      "step = 42: Loss = 21.306209564208984\n",
      "step = 43: Loss = 22.11489486694336\n",
      "step = 44: Loss = 21.69640350341797\n",
      "step = 45: Loss = 20.427778244018555\n",
      "step = 46: Loss = 19.818756103515625\n",
      "step = 47: Loss = 20.31951332092285\n",
      "step = 48: Loss = 20.51333999633789\n",
      "step = 49: Loss = 20.04022216796875\n",
      "step = 50: Loss = 19.865774154663086\n",
      "step = 51: Loss = 19.733684539794922\n",
      "step = 52: Loss = 18.910734176635742\n",
      "step = 53: Loss = 20.948810577392578\n",
      "step = 54: Loss = 19.484106063842773\n",
      "step = 55: Loss = 19.475505828857422\n",
      "step = 56: Loss = 18.24666976928711\n",
      "step = 57: Loss = 19.644527435302734\n",
      "step = 58: Loss = 19.249664306640625\n",
      "step = 59: Loss = 20.028196334838867\n",
      "step = 60: Loss = 17.67824935913086\n",
      "step = 61: Loss = 19.36278533935547\n",
      "step = 62: Loss = 19.27734375\n",
      "step = 63: Loss = 19.00724983215332\n",
      "step = 64: Loss = 17.447376251220703\n",
      "step = 65: Loss = 19.194419860839844\n",
      "step = 66: Loss = 18.6337890625\n",
      "step = 67: Loss = 16.74483299255371\n",
      "step = 68: Loss = 19.334640502929688\n",
      "step = 69: Loss = 18.66040802001953\n",
      "step = 70: Loss = 18.975261688232422\n",
      "step = 71: Loss = 19.292247772216797\n",
      "step = 72: Loss = 18.677913665771484\n",
      "step = 73: Loss = 19.3416805267334\n",
      "step = 74: Loss = 17.445093154907227\n",
      "step = 75: Loss = 19.43625831604004\n",
      "step = 76: Loss = 19.913570404052734\n",
      "step = 77: Loss = 19.186243057250977\n",
      "step = 78: Loss = 20.598121643066406\n",
      "step = 79: Loss = 20.521249771118164\n",
      "step = 80: Loss = 21.963342666625977\n",
      "step = 81: Loss = 21.765357971191406\n",
      "step = 82: Loss = 21.311100006103516\n",
      "step = 83: Loss = 21.817638397216797\n",
      "step = 84: Loss = 21.33331298828125\n",
      "step = 85: Loss = 22.675418853759766\n",
      "step = 86: Loss = 21.33787727355957\n",
      "step = 87: Loss = 23.390239715576172\n",
      "step = 88: Loss = 23.259462356567383\n",
      "step = 89: Loss = 21.683696746826172\n",
      "step = 90: Loss = 23.217731475830078\n",
      "step = 91: Loss = 22.270660400390625\n",
      "step = 92: Loss = 22.607601165771484\n",
      "step = 93: Loss = 21.264007568359375\n",
      "step = 94: Loss = 21.746871948242188\n",
      "step = 95: Loss = 22.8306941986084\n",
      "step = 96: Loss = 20.738521575927734\n",
      "step = 97: Loss = 19.829513549804688\n",
      "step = 98: Loss = 20.107471466064453\n",
      "step = 99: Loss = 19.389026641845703\n",
      "step = 100: Loss = 19.541784286499023\n",
      "step = 101: Loss = 18.941205978393555\n",
      "step = 102: Loss = 18.90287971496582\n",
      "step = 103: Loss = 17.751676559448242\n",
      "step = 104: Loss = 17.20463752746582\n",
      "step = 105: Loss = 17.359724044799805\n",
      "step = 106: Loss = 17.961679458618164\n",
      "step = 107: Loss = 18.987808227539062\n",
      "step = 108: Loss = 19.65546989440918\n",
      "step = 109: Loss = 22.359373092651367\n",
      "step = 110: Loss = 22.464244842529297\n",
      "step = 111: Loss = 26.041141510009766\n",
      "step = 112: Loss = 30.455780029296875\n",
      "step = 113: Loss = 34.9947624206543\n",
      "step = 114: Loss = 38.19435119628906\n",
      "step = 115: Loss = 42.87156295776367\n",
      "step = 116: Loss = 44.08736801147461\n",
      "step = 117: Loss = 51.82927703857422\n",
      "step = 118: Loss = 59.59650802612305\n",
      "step = 119: Loss = 59.834449768066406\n",
      "step = 120: Loss = 69.35824584960938\n",
      "step = 121: Loss = 75.45903015136719\n",
      "step = 122: Loss = 79.84803009033203\n",
      "step = 123: Loss = 84.70082092285156\n",
      "step = 124: Loss = 89.68423461914062\n",
      "step = 125: Loss = 85.26026916503906\n",
      "step = 126: Loss = 94.56049346923828\n",
      "step = 127: Loss = 86.37332916259766\n",
      "step = 128: Loss = 92.77986145019531\n",
      "step = 129: Loss = 93.77684020996094\n",
      "step = 130: Loss = 93.9835205078125\n",
      "step = 131: Loss = 90.80266571044922\n",
      "step = 132: Loss = 87.52274322509766\n",
      "step = 133: Loss = 81.70545959472656\n",
      "step = 134: Loss = 81.05044555664062\n",
      "step = 135: Loss = 84.28959655761719\n",
      "step = 136: Loss = 78.7233657836914\n",
      "step = 137: Loss = 73.07579040527344\n",
      "step = 138: Loss = 73.3409194946289\n",
      "step = 139: Loss = 72.3275146484375\n",
      "step = 140: Loss = 70.09104919433594\n",
      "step = 141: Loss = 78.1668930053711\n",
      "step = 142: Loss = 73.18670654296875\n",
      "step = 143: Loss = 86.14366912841797\n",
      "step = 144: Loss = 82.25749969482422\n",
      "step = 145: Loss = 79.5091781616211\n",
      "step = 146: Loss = 83.77687072753906\n",
      "step = 147: Loss = 84.8690414428711\n",
      "step = 148: Loss = 94.47328186035156\n",
      "step = 149: Loss = 93.91897583007812\n",
      "step = 150: Loss = 87.58706665039062\n",
      "step = 151: Loss = 91.55117797851562\n",
      "step = 152: Loss = 99.04153442382812\n",
      "step = 153: Loss = 103.45843505859375\n",
      "step = 154: Loss = 98.20040130615234\n",
      "step = 155: Loss = 111.59676361083984\n",
      "step = 156: Loss = 116.97718048095703\n",
      "step = 157: Loss = 112.95987701416016\n",
      "step = 158: Loss = 112.64144897460938\n",
      "step = 159: Loss = 108.35059356689453\n",
      "step = 160: Loss = 119.62340545654297\n",
      "step = 161: Loss = 126.38095092773438\n",
      "step = 162: Loss = 124.21574401855469\n",
      "step = 163: Loss = 123.42135620117188\n",
      "step = 164: Loss = 132.77906799316406\n",
      "step = 165: Loss = 119.02181243896484\n",
      "step = 166: Loss = 121.54957580566406\n",
      "step = 167: Loss = 137.28802490234375\n",
      "step = 168: Loss = 123.10395812988281\n",
      "step = 169: Loss = 125.54795837402344\n",
      "step = 170: Loss = 117.88201141357422\n",
      "step = 171: Loss = 118.64501190185547\n",
      "step = 172: Loss = 117.16505432128906\n",
      "step = 173: Loss = 112.38690948486328\n",
      "step = 174: Loss = 108.537109375\n",
      "step = 175: Loss = 111.96554565429688\n",
      "step = 176: Loss = 123.07157135009766\n",
      "step = 177: Loss = 122.64647674560547\n",
      "step = 178: Loss = 134.72677612304688\n",
      "step = 179: Loss = 125.11605072021484\n",
      "step = 180: Loss = 112.8349838256836\n",
      "step = 181: Loss = 121.29317474365234\n",
      "step = 182: Loss = 136.32118225097656\n",
      "step = 183: Loss = 125.20140075683594\n",
      "step = 184: Loss = 120.3382339477539\n",
      "step = 185: Loss = 128.56365966796875\n",
      "step = 186: Loss = 122.08131408691406\n",
      "step = 187: Loss = 116.92111206054688\n",
      "step = 188: Loss = 127.53501892089844\n",
      "step = 189: Loss = 118.53335571289062\n",
      "step = 190: Loss = 115.81666564941406\n",
      "step = 191: Loss = 118.37512969970703\n",
      "step = 192: Loss = 118.49578094482422\n",
      "step = 193: Loss = 120.19789123535156\n",
      "step = 194: Loss = 123.72543334960938\n",
      "step = 195: Loss = 124.13896179199219\n",
      "step = 196: Loss = 120.29474639892578\n",
      "step = 197: Loss = 112.45269775390625\n",
      "step = 198: Loss = 124.43124389648438\n",
      "step = 199: Loss = 107.73193359375\n",
      "step = 200: Loss = 105.86752319335938\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x00000223D945BF70>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x00000223D945BF70>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 201: Loss = 113.12714385986328\n",
      "step = 202: Loss = 100.06489562988281\n",
      "step = 203: Loss = 103.91789245605469\n",
      "step = 204: Loss = 104.47799682617188\n",
      "step = 205: Loss = 107.44830322265625\n",
      "step = 206: Loss = 114.959228515625\n",
      "step = 207: Loss = 113.83258056640625\n",
      "step = 208: Loss = 122.4582290649414\n",
      "step = 209: Loss = 122.365234375\n",
      "step = 210: Loss = 105.21317291259766\n",
      "step = 211: Loss = 104.69337463378906\n",
      "step = 212: Loss = 86.36556243896484\n",
      "step = 213: Loss = 82.1441650390625\n",
      "step = 214: Loss = 87.51250457763672\n",
      "step = 215: Loss = 89.24739074707031\n",
      "step = 216: Loss = 103.83921813964844\n",
      "step = 217: Loss = 105.44619750976562\n",
      "step = 218: Loss = 101.47386932373047\n",
      "step = 219: Loss = 106.13478088378906\n",
      "step = 220: Loss = 108.65823364257812\n",
      "step = 221: Loss = 121.10267639160156\n",
      "step = 222: Loss = 98.63333129882812\n",
      "step = 223: Loss = 108.56477355957031\n",
      "step = 224: Loss = 96.67900848388672\n",
      "step = 225: Loss = 93.70817565917969\n",
      "step = 226: Loss = 98.99117279052734\n",
      "step = 227: Loss = 98.17721557617188\n",
      "step = 228: Loss = 89.46099090576172\n",
      "step = 229: Loss = 82.98048400878906\n",
      "step = 230: Loss = 85.38264465332031\n",
      "step = 231: Loss = 87.0433120727539\n",
      "step = 232: Loss = 76.56468963623047\n",
      "step = 233: Loss = 89.11559295654297\n",
      "step = 234: Loss = 101.01994323730469\n",
      "step = 235: Loss = 88.64166259765625\n",
      "step = 236: Loss = 87.2582778930664\n",
      "step = 237: Loss = 93.52350616455078\n",
      "step = 238: Loss = 99.3652572631836\n",
      "step = 239: Loss = 92.15003204345703\n",
      "step = 240: Loss = 78.73300170898438\n",
      "step = 241: Loss = 84.70807647705078\n",
      "step = 242: Loss = 73.88530731201172\n",
      "step = 243: Loss = 81.94557189941406\n",
      "step = 244: Loss = 72.6657485961914\n",
      "step = 245: Loss = 72.66285705566406\n",
      "step = 246: Loss = 84.39749145507812\n",
      "step = 247: Loss = 88.15880584716797\n",
      "step = 248: Loss = 72.51642608642578\n",
      "step = 249: Loss = 80.05360412597656\n",
      "step = 250: Loss = 77.85417938232422\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x00000223D92F3970>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x00000223D92F3970>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 251: Loss = 71.80006408691406\n",
      "step = 252: Loss = 79.00994110107422\n",
      "step = 253: Loss = 68.23233795166016\n",
      "step = 254: Loss = 73.49784851074219\n",
      "step = 255: Loss = 72.03121185302734\n",
      "step = 256: Loss = 75.61028289794922\n",
      "step = 257: Loss = 73.59909057617188\n",
      "step = 258: Loss = 72.09276580810547\n",
      "step = 259: Loss = 74.91763305664062\n",
      "step = 260: Loss = 65.12734985351562\n",
      "step = 261: Loss = 64.36395263671875\n",
      "step = 262: Loss = 62.60291290283203\n",
      "step = 263: Loss = 68.51422119140625\n",
      "step = 264: Loss = 60.00006103515625\n",
      "step = 265: Loss = 63.49409866333008\n",
      "step = 266: Loss = 69.47858428955078\n",
      "step = 267: Loss = 66.09001159667969\n",
      "step = 268: Loss = 66.2735366821289\n",
      "step = 269: Loss = 62.6861686706543\n",
      "step = 270: Loss = 70.53594970703125\n",
      "step = 271: Loss = 59.17848205566406\n",
      "step = 272: Loss = 53.91301345825195\n",
      "step = 273: Loss = 56.84518051147461\n",
      "step = 274: Loss = 52.91572189331055\n",
      "step = 275: Loss = 66.93075561523438\n",
      "step = 276: Loss = 67.72945404052734\n",
      "step = 277: Loss = 69.32662200927734\n",
      "step = 278: Loss = 70.48014831542969\n",
      "step = 279: Loss = 92.02519226074219\n",
      "step = 280: Loss = 71.3924560546875\n",
      "step = 281: Loss = 59.488521575927734\n",
      "step = 282: Loss = 63.82290267944336\n",
      "step = 283: Loss = 61.30535888671875\n",
      "step = 284: Loss = 57.045894622802734\n",
      "step = 285: Loss = 60.62564468383789\n",
      "step = 286: Loss = 67.708984375\n",
      "step = 287: Loss = 59.974308013916016\n",
      "step = 288: Loss = 55.94805145263672\n",
      "step = 289: Loss = 67.1435546875\n",
      "step = 290: Loss = 55.08967208862305\n",
      "step = 291: Loss = 50.572532653808594\n",
      "step = 292: Loss = 45.273399353027344\n",
      "step = 293: Loss = 61.88871383666992\n",
      "step = 294: Loss = 59.37569046020508\n",
      "step = 295: Loss = 65.79615020751953\n",
      "step = 296: Loss = 63.44983673095703\n",
      "step = 297: Loss = 56.590614318847656\n",
      "step = 298: Loss = 49.96542739868164\n",
      "step = 299: Loss = 58.879119873046875\n",
      "step = 300: Loss = 65.56395721435547\n",
      "step = 301: Loss = 48.1690788269043\n",
      "step = 302: Loss = 57.63806915283203\n",
      "step = 303: Loss = 53.68354797363281\n",
      "step = 304: Loss = 56.65523910522461\n",
      "step = 305: Loss = 49.508399963378906\n",
      "step = 306: Loss = 50.283836364746094\n",
      "step = 307: Loss = 49.072959899902344\n",
      "step = 308: Loss = 43.69468688964844\n",
      "step = 309: Loss = 43.5852165222168\n",
      "step = 310: Loss = 47.923397064208984\n",
      "step = 311: Loss = 53.543479919433594\n",
      "step = 312: Loss = 51.34173583984375\n",
      "step = 313: Loss = 66.0120620727539\n",
      "step = 314: Loss = 55.2991828918457\n",
      "step = 315: Loss = 47.67459487915039\n",
      "step = 316: Loss = 49.979530334472656\n",
      "step = 317: Loss = 44.052330017089844\n",
      "step = 318: Loss = 51.335487365722656\n",
      "step = 319: Loss = 36.042335510253906\n",
      "step = 320: Loss = 49.76244354248047\n",
      "step = 321: Loss = 49.42660903930664\n",
      "step = 322: Loss = 50.19405746459961\n",
      "step = 323: Loss = 40.76856231689453\n",
      "step = 324: Loss = 43.928321838378906\n",
      "step = 325: Loss = 53.47339630126953\n",
      "step = 326: Loss = 43.06167221069336\n",
      "step = 327: Loss = 46.40404510498047\n",
      "step = 328: Loss = 43.24241638183594\n",
      "step = 329: Loss = 45.35418701171875\n",
      "step = 330: Loss = 45.81064224243164\n",
      "step = 331: Loss = 36.718284606933594\n",
      "step = 332: Loss = 52.00346374511719\n",
      "step = 333: Loss = 39.55708312988281\n",
      "step = 334: Loss = 45.96807861328125\n",
      "step = 335: Loss = 40.409523010253906\n",
      "step = 336: Loss = 39.734092712402344\n",
      "step = 337: Loss = 38.55432891845703\n",
      "step = 338: Loss = 45.558982849121094\n",
      "step = 339: Loss = 44.94980239868164\n",
      "step = 340: Loss = 51.63468551635742\n",
      "step = 341: Loss = 47.77497863769531\n",
      "step = 342: Loss = 36.486778259277344\n",
      "step = 343: Loss = 37.6999626159668\n",
      "step = 344: Loss = 34.52066421508789\n",
      "step = 345: Loss = 30.652774810791016\n",
      "step = 346: Loss = 39.3065185546875\n",
      "step = 347: Loss = 41.841522216796875\n",
      "step = 348: Loss = 46.39888381958008\n",
      "step = 349: Loss = 42.572052001953125\n",
      "step = 350: Loss = 44.96366882324219\n",
      "step = 351: Loss = 37.12497329711914\n",
      "step = 352: Loss = 36.5999641418457\n",
      "step = 353: Loss = 35.6966438293457\n",
      "step = 354: Loss = 41.65217971801758\n",
      "step = 355: Loss = 41.21145248413086\n",
      "step = 356: Loss = 40.68439483642578\n",
      "step = 357: Loss = 42.91663360595703\n",
      "step = 358: Loss = 44.02639389038086\n",
      "step = 359: Loss = 43.13625717163086\n",
      "step = 360: Loss = 36.23786544799805\n",
      "step = 361: Loss = 36.92566680908203\n",
      "step = 362: Loss = 32.172000885009766\n",
      "step = 363: Loss = 34.253257751464844\n",
      "step = 364: Loss = 36.78752136230469\n",
      "step = 365: Loss = 33.47412109375\n",
      "step = 366: Loss = 36.02701950073242\n",
      "step = 367: Loss = 35.33877182006836\n",
      "step = 368: Loss = 35.52845001220703\n",
      "step = 369: Loss = 39.82069396972656\n",
      "step = 370: Loss = 39.54767990112305\n",
      "step = 371: Loss = 33.77705001831055\n",
      "step = 372: Loss = 29.358434677124023\n",
      "step = 373: Loss = 34.06833267211914\n",
      "step = 374: Loss = 35.591835021972656\n",
      "step = 375: Loss = 30.43608283996582\n",
      "step = 376: Loss = 31.78589630126953\n",
      "step = 377: Loss = 26.457427978515625\n",
      "step = 378: Loss = 31.85135269165039\n",
      "step = 379: Loss = 30.716459274291992\n",
      "step = 380: Loss = 25.965055465698242\n",
      "step = 381: Loss = 30.821455001831055\n",
      "step = 382: Loss = 32.808048248291016\n",
      "step = 383: Loss = 31.774202346801758\n",
      "step = 384: Loss = 28.670244216918945\n",
      "step = 385: Loss = 33.50993728637695\n",
      "step = 386: Loss = 32.66763687133789\n",
      "step = 387: Loss = 25.408485412597656\n",
      "step = 388: Loss = 28.233867645263672\n",
      "step = 389: Loss = 25.689844131469727\n",
      "step = 390: Loss = 29.806854248046875\n",
      "step = 391: Loss = 24.822101593017578\n",
      "step = 392: Loss = 23.530166625976562\n",
      "step = 393: Loss = 25.75025177001953\n",
      "step = 394: Loss = 26.598350524902344\n",
      "step = 395: Loss = 26.624439239501953\n",
      "step = 396: Loss = 26.94951629638672\n",
      "step = 397: Loss = 27.439481735229492\n",
      "step = 398: Loss = 25.055452346801758\n",
      "step = 399: Loss = 27.055192947387695\n",
      "step = 400: Loss = 27.09465980529785\n",
      "step = 401: Loss = 26.5892333984375\n",
      "step = 402: Loss = 27.95256996154785\n",
      "step = 403: Loss = 29.1324520111084\n",
      "step = 404: Loss = 24.883705139160156\n",
      "step = 405: Loss = 24.081207275390625\n",
      "step = 406: Loss = 25.7845401763916\n",
      "step = 407: Loss = 23.49007797241211\n",
      "step = 408: Loss = 22.148345947265625\n",
      "step = 409: Loss = 21.969249725341797\n",
      "step = 410: Loss = 28.787914276123047\n",
      "step = 411: Loss = 23.49848747253418\n",
      "step = 412: Loss = 23.56000518798828\n",
      "step = 413: Loss = 23.754737854003906\n",
      "step = 414: Loss = 23.917272567749023\n",
      "step = 415: Loss = 24.482486724853516\n",
      "step = 416: Loss = 23.677156448364258\n",
      "step = 417: Loss = 23.9572696685791\n",
      "step = 418: Loss = 24.49833869934082\n",
      "step = 419: Loss = 26.29144859313965\n",
      "step = 420: Loss = 25.01029396057129\n",
      "step = 421: Loss = 21.247116088867188\n",
      "step = 422: Loss = 23.436176300048828\n",
      "step = 423: Loss = 28.03455924987793\n",
      "step = 424: Loss = 27.246185302734375\n",
      "step = 425: Loss = 23.719749450683594\n",
      "step = 426: Loss = 24.469144821166992\n",
      "step = 427: Loss = 24.003204345703125\n",
      "step = 428: Loss = 24.757869720458984\n",
      "step = 429: Loss = 22.28668212890625\n",
      "step = 430: Loss = 21.813255310058594\n",
      "step = 431: Loss = 23.637968063354492\n",
      "step = 432: Loss = 24.223867416381836\n",
      "step = 433: Loss = 23.928556442260742\n",
      "step = 434: Loss = 25.80739974975586\n",
      "step = 435: Loss = 24.696487426757812\n",
      "step = 436: Loss = 25.350221633911133\n",
      "step = 437: Loss = 25.487102508544922\n",
      "step = 438: Loss = 25.473796844482422\n",
      "step = 439: Loss = 22.36150550842285\n",
      "step = 440: Loss = 17.736572265625\n",
      "step = 441: Loss = 22.471712112426758\n",
      "step = 442: Loss = 18.91957664489746\n",
      "step = 443: Loss = 21.015703201293945\n",
      "step = 444: Loss = 23.894948959350586\n",
      "step = 445: Loss = 23.572542190551758\n",
      "step = 446: Loss = 17.541566848754883\n",
      "step = 447: Loss = 23.696704864501953\n",
      "step = 448: Loss = 20.620372772216797\n",
      "step = 449: Loss = 21.871177673339844\n",
      "step = 450: Loss = 22.10152244567871\n",
      "step = 451: Loss = 21.6639461517334\n",
      "step = 452: Loss = 24.94327735900879\n",
      "step = 453: Loss = 20.479915618896484\n",
      "step = 454: Loss = 21.1639404296875\n",
      "step = 455: Loss = 24.458049774169922\n",
      "step = 456: Loss = 25.685871124267578\n",
      "step = 457: Loss = 21.835615158081055\n",
      "step = 458: Loss = 19.4017391204834\n",
      "step = 459: Loss = 20.752769470214844\n",
      "step = 460: Loss = 20.231632232666016\n",
      "step = 461: Loss = 21.040876388549805\n",
      "step = 462: Loss = 22.386089324951172\n",
      "step = 463: Loss = 18.439571380615234\n",
      "step = 464: Loss = 20.02202606201172\n",
      "step = 465: Loss = 23.170574188232422\n",
      "step = 466: Loss = 19.65064239501953\n",
      "step = 467: Loss = 22.151287078857422\n",
      "step = 468: Loss = 19.043312072753906\n",
      "step = 469: Loss = 19.78849983215332\n",
      "step = 470: Loss = 18.882755279541016\n",
      "step = 471: Loss = 16.773418426513672\n",
      "step = 472: Loss = 18.67111587524414\n",
      "step = 473: Loss = 18.031816482543945\n",
      "step = 474: Loss = 16.665571212768555\n",
      "step = 475: Loss = 20.348812103271484\n",
      "step = 476: Loss = 19.437965393066406\n",
      "step = 477: Loss = 14.849763870239258\n",
      "step = 478: Loss = 17.500715255737305\n",
      "step = 479: Loss = 22.18523406982422\n",
      "step = 480: Loss = 16.872133255004883\n",
      "step = 481: Loss = 17.521392822265625\n",
      "step = 482: Loss = 18.170562744140625\n",
      "step = 483: Loss = 13.350859642028809\n",
      "step = 484: Loss = 17.02560806274414\n",
      "step = 485: Loss = 16.458763122558594\n",
      "step = 486: Loss = 14.411239624023438\n",
      "step = 487: Loss = 17.329416275024414\n",
      "step = 488: Loss = 16.916940689086914\n",
      "step = 489: Loss = 16.60951042175293\n",
      "step = 490: Loss = 16.62962532043457\n",
      "step = 491: Loss = 19.236722946166992\n",
      "step = 492: Loss = 15.226929664611816\n",
      "step = 493: Loss = 17.727176666259766\n",
      "step = 494: Loss = 16.535097122192383\n",
      "step = 495: Loss = 15.923357009887695\n",
      "step = 496: Loss = 17.102249145507812\n",
      "step = 497: Loss = 15.755471229553223\n",
      "step = 498: Loss = 17.0205078125\n",
      "step = 499: Loss = 17.811809539794922\n",
      "step = 500: Loss = 16.086570739746094\n",
      "step = 501: Loss = 15.098453521728516\n",
      "step = 502: Loss = 15.042648315429688\n",
      "step = 503: Loss = 17.02998161315918\n",
      "step = 504: Loss = 17.488996505737305\n",
      "step = 505: Loss = 15.977384567260742\n",
      "step = 506: Loss = 17.02425193786621\n",
      "step = 507: Loss = 15.701618194580078\n",
      "step = 508: Loss = 15.423653602600098\n",
      "step = 509: Loss = 17.02288246154785\n",
      "step = 510: Loss = 15.416153907775879\n",
      "step = 511: Loss = 16.448713302612305\n",
      "step = 512: Loss = 14.25252628326416\n",
      "step = 513: Loss = 16.02120018005371\n",
      "step = 514: Loss = 14.601703643798828\n",
      "step = 515: Loss = 17.66115379333496\n",
      "step = 516: Loss = 13.980006217956543\n",
      "step = 517: Loss = 14.774084091186523\n",
      "step = 518: Loss = 14.000028610229492\n",
      "step = 519: Loss = 13.21509075164795\n",
      "step = 520: Loss = 15.924038887023926\n",
      "step = 521: Loss = 13.343344688415527\n",
      "step = 522: Loss = 14.515344619750977\n",
      "step = 523: Loss = 15.386428833007812\n",
      "step = 524: Loss = 14.367130279541016\n",
      "step = 525: Loss = 13.785965919494629\n",
      "step = 526: Loss = 15.275200843811035\n",
      "step = 527: Loss = 14.62591552734375\n",
      "step = 528: Loss = 12.087024688720703\n",
      "step = 529: Loss = 13.513579368591309\n",
      "step = 530: Loss = 15.199312210083008\n",
      "step = 531: Loss = 12.735429763793945\n",
      "step = 532: Loss = 14.338384628295898\n",
      "step = 533: Loss = 13.218334197998047\n",
      "step = 534: Loss = 15.51364517211914\n",
      "step = 535: Loss = 14.196770668029785\n",
      "step = 536: Loss = 13.472150802612305\n",
      "step = 537: Loss = 13.575779914855957\n",
      "step = 538: Loss = 15.248185157775879\n",
      "step = 539: Loss = 15.198151588439941\n",
      "step = 540: Loss = 14.802085876464844\n",
      "step = 541: Loss = 13.7014741897583\n",
      "step = 542: Loss = 15.315576553344727\n",
      "step = 543: Loss = 13.702982902526855\n",
      "step = 544: Loss = 10.450803756713867\n",
      "step = 545: Loss = 12.820899963378906\n",
      "step = 546: Loss = 12.311473846435547\n",
      "step = 547: Loss = 12.0531005859375\n",
      "step = 548: Loss = 14.114590644836426\n",
      "step = 549: Loss = 12.359241485595703\n",
      "step = 550: Loss = 11.694622993469238\n",
      "step = 551: Loss = 13.368938446044922\n",
      "step = 552: Loss = 13.463770866394043\n",
      "step = 553: Loss = 14.662732124328613\n",
      "step = 554: Loss = 16.99295997619629\n",
      "step = 555: Loss = 13.135441780090332\n",
      "step = 556: Loss = 10.08741569519043\n",
      "step = 557: Loss = 13.319084167480469\n",
      "step = 558: Loss = 12.046893119812012\n",
      "step = 559: Loss = 11.633415222167969\n",
      "step = 560: Loss = 10.841751098632812\n",
      "step = 561: Loss = 10.482850074768066\n",
      "step = 562: Loss = 11.309043884277344\n",
      "step = 563: Loss = 10.849964141845703\n",
      "step = 564: Loss = 11.974559783935547\n",
      "step = 565: Loss = 11.525712966918945\n",
      "step = 566: Loss = 13.752811431884766\n",
      "step = 567: Loss = 12.059974670410156\n",
      "step = 568: Loss = 14.103565216064453\n",
      "step = 569: Loss = 10.915470123291016\n",
      "step = 570: Loss = 12.007192611694336\n",
      "step = 571: Loss = 11.884478569030762\n",
      "step = 572: Loss = 16.13469123840332\n",
      "step = 573: Loss = 11.963164329528809\n",
      "step = 574: Loss = 13.135843276977539\n",
      "step = 575: Loss = 11.897639274597168\n",
      "step = 576: Loss = 12.679698944091797\n",
      "step = 577: Loss = 11.132529258728027\n",
      "step = 578: Loss = 12.1336030960083\n",
      "step = 579: Loss = 11.661304473876953\n",
      "step = 580: Loss = 11.832601547241211\n",
      "step = 581: Loss = 9.864176750183105\n",
      "step = 582: Loss = 11.898021697998047\n",
      "step = 583: Loss = 11.43515682220459\n",
      "step = 584: Loss = 10.39351749420166\n",
      "step = 585: Loss = 13.357709884643555\n",
      "step = 586: Loss = 12.201593399047852\n",
      "step = 587: Loss = 10.151021003723145\n",
      "step = 588: Loss = 10.874053955078125\n",
      "step = 589: Loss = 9.712784767150879\n",
      "step = 590: Loss = 10.485514640808105\n",
      "step = 591: Loss = 9.569984436035156\n",
      "step = 592: Loss = 11.38055419921875\n",
      "step = 593: Loss = 9.907767295837402\n",
      "step = 594: Loss = 10.97175407409668\n",
      "step = 595: Loss = 10.242568969726562\n",
      "step = 596: Loss = 11.430355072021484\n",
      "step = 597: Loss = 10.6963472366333\n",
      "step = 598: Loss = 12.222136497497559\n",
      "step = 599: Loss = 12.035953521728516\n",
      "step = 600: Loss = 11.760924339294434\n",
      "step = 601: Loss = 10.092596054077148\n",
      "step = 602: Loss = 11.185721397399902\n",
      "step = 603: Loss = 11.81182861328125\n",
      "step = 604: Loss = 11.539665222167969\n",
      "step = 605: Loss = 11.530835151672363\n",
      "step = 606: Loss = 11.381637573242188\n",
      "step = 607: Loss = 9.956992149353027\n",
      "step = 608: Loss = 11.189125061035156\n",
      "step = 609: Loss = 10.356950759887695\n",
      "step = 610: Loss = 9.350528717041016\n",
      "step = 611: Loss = 8.973565101623535\n",
      "step = 612: Loss = 9.315462112426758\n",
      "step = 613: Loss = 11.84390640258789\n",
      "step = 614: Loss = 11.607855796813965\n",
      "step = 615: Loss = 11.430303573608398\n",
      "step = 616: Loss = 9.586274147033691\n",
      "step = 617: Loss = 10.578692436218262\n",
      "step = 618: Loss = 10.595626831054688\n",
      "step = 619: Loss = 10.49288272857666\n",
      "step = 620: Loss = 9.891668319702148\n",
      "step = 621: Loss = 10.224584579467773\n",
      "step = 622: Loss = 12.539073944091797\n",
      "step = 623: Loss = 12.432456970214844\n",
      "step = 624: Loss = 10.825785636901855\n",
      "step = 625: Loss = 9.979581832885742\n",
      "step = 626: Loss = 12.028545379638672\n",
      "step = 627: Loss = 11.596729278564453\n",
      "step = 628: Loss = 9.546676635742188\n",
      "step = 629: Loss = 9.74353313446045\n",
      "step = 630: Loss = 10.709999084472656\n",
      "step = 631: Loss = 13.207925796508789\n",
      "step = 632: Loss = 12.9951753616333\n",
      "step = 633: Loss = 11.952691078186035\n",
      "step = 634: Loss = 11.131757736206055\n",
      "step = 635: Loss = 11.850519180297852\n",
      "step = 636: Loss = 10.338851928710938\n",
      "step = 637: Loss = 10.294569969177246\n",
      "step = 638: Loss = 10.539105415344238\n",
      "step = 639: Loss = 9.725425720214844\n",
      "step = 640: Loss = 11.39743709564209\n",
      "step = 641: Loss = 9.591582298278809\n",
      "step = 642: Loss = 10.969600677490234\n",
      "step = 643: Loss = 12.472061157226562\n",
      "step = 644: Loss = 10.569384574890137\n",
      "step = 645: Loss = 8.692319869995117\n",
      "step = 646: Loss = 11.859944343566895\n",
      "step = 647: Loss = 9.736942291259766\n",
      "step = 648: Loss = 9.377252578735352\n",
      "step = 649: Loss = 11.016324043273926\n",
      "step = 650: Loss = 9.985387802124023\n",
      "step = 651: Loss = 10.424821853637695\n",
      "step = 652: Loss = 10.868820190429688\n",
      "step = 653: Loss = 10.012513160705566\n",
      "step = 654: Loss = 9.27754020690918\n",
      "step = 655: Loss = 10.104171752929688\n",
      "step = 656: Loss = 10.787720680236816\n",
      "step = 657: Loss = 9.166435241699219\n",
      "step = 658: Loss = 10.541522979736328\n",
      "step = 659: Loss = 10.243526458740234\n",
      "step = 660: Loss = 10.253694534301758\n",
      "step = 661: Loss = 10.333020210266113\n",
      "step = 662: Loss = 9.811957359313965\n",
      "step = 663: Loss = 9.437629699707031\n",
      "step = 664: Loss = 9.405176162719727\n",
      "step = 665: Loss = 8.363401412963867\n",
      "step = 666: Loss = 9.577352523803711\n",
      "step = 667: Loss = 9.9888334274292\n",
      "step = 668: Loss = 8.235603332519531\n",
      "step = 669: Loss = 10.271029472351074\n",
      "step = 670: Loss = 10.159682273864746\n",
      "step = 671: Loss = 8.372589111328125\n",
      "step = 672: Loss = 8.73355770111084\n",
      "step = 673: Loss = 8.388689041137695\n",
      "step = 674: Loss = 9.998556137084961\n",
      "step = 675: Loss = 9.56396484375\n",
      "step = 676: Loss = 9.052312850952148\n",
      "step = 677: Loss = 8.978434562683105\n",
      "step = 678: Loss = 8.621021270751953\n",
      "step = 679: Loss = 7.8332719802856445\n",
      "step = 680: Loss = 10.992741584777832\n",
      "step = 681: Loss = 12.1018705368042\n",
      "step = 682: Loss = 8.999603271484375\n",
      "step = 683: Loss = 9.268627166748047\n",
      "step = 684: Loss = 8.768549919128418\n",
      "step = 685: Loss = 10.303719520568848\n",
      "step = 686: Loss = 10.825973510742188\n",
      "step = 687: Loss = 10.68161678314209\n",
      "step = 688: Loss = 9.498390197753906\n",
      "step = 689: Loss = 9.530835151672363\n",
      "step = 690: Loss = 10.587371826171875\n",
      "step = 691: Loss = 9.97733211517334\n",
      "step = 692: Loss = 9.021455764770508\n",
      "step = 693: Loss = 12.544408798217773\n",
      "step = 694: Loss = 8.600606918334961\n",
      "step = 695: Loss = 9.717950820922852\n",
      "step = 696: Loss = 8.184175491333008\n",
      "step = 697: Loss = 7.573265075683594\n",
      "step = 698: Loss = 7.443692207336426\n",
      "step = 699: Loss = 8.596412658691406\n",
      "step = 700: Loss = 9.84507942199707\n",
      "step = 701: Loss = 9.466185569763184\n",
      "step = 702: Loss = 9.742074966430664\n",
      "step = 703: Loss = 9.897561073303223\n",
      "step = 704: Loss = 9.352579116821289\n",
      "step = 705: Loss = 10.868982315063477\n",
      "step = 706: Loss = 9.614469528198242\n",
      "step = 707: Loss = 7.356250762939453\n",
      "step = 708: Loss = 8.001768112182617\n",
      "step = 709: Loss = 9.360673904418945\n",
      "step = 710: Loss = 9.486129760742188\n",
      "step = 711: Loss = 8.717901229858398\n",
      "step = 712: Loss = 10.139196395874023\n",
      "step = 713: Loss = 8.23648452758789\n",
      "step = 714: Loss = 8.858622550964355\n",
      "step = 715: Loss = 8.17608642578125\n",
      "step = 716: Loss = 9.536849021911621\n",
      "step = 717: Loss = 8.48680305480957\n",
      "step = 718: Loss = 9.365689277648926\n",
      "step = 719: Loss = 8.466910362243652\n",
      "step = 720: Loss = 9.292692184448242\n",
      "step = 721: Loss = 7.109145164489746\n",
      "step = 722: Loss = 7.508507251739502\n",
      "step = 723: Loss = 7.92294979095459\n",
      "step = 724: Loss = 7.632193565368652\n",
      "step = 725: Loss = 7.610232353210449\n",
      "step = 726: Loss = 7.876953125\n",
      "step = 727: Loss = 6.612817764282227\n",
      "step = 728: Loss = 8.937729835510254\n",
      "step = 729: Loss = 8.951666831970215\n",
      "step = 730: Loss = 7.578745365142822\n",
      "step = 731: Loss = 8.974447250366211\n",
      "step = 732: Loss = 8.004674911499023\n",
      "step = 733: Loss = 9.800603866577148\n",
      "step = 734: Loss = 8.98542594909668\n",
      "step = 735: Loss = 8.845138549804688\n",
      "step = 736: Loss = 7.946722984313965\n",
      "step = 737: Loss = 8.015933990478516\n",
      "step = 738: Loss = 7.520108222961426\n",
      "step = 739: Loss = 8.323495864868164\n",
      "step = 740: Loss = 8.643861770629883\n",
      "step = 741: Loss = 8.316293716430664\n",
      "step = 742: Loss = 8.44078540802002\n",
      "step = 743: Loss = 7.848100662231445\n",
      "step = 744: Loss = 8.495328903198242\n",
      "step = 745: Loss = 8.190896987915039\n",
      "step = 746: Loss = 8.319921493530273\n",
      "step = 747: Loss = 7.856912612915039\n",
      "step = 748: Loss = 8.120712280273438\n",
      "step = 749: Loss = 8.345821380615234\n",
      "step = 750: Loss = 8.521589279174805\n",
      "step = 751: Loss = 7.057734489440918\n",
      "step = 752: Loss = 9.768775939941406\n",
      "step = 753: Loss = 8.323817253112793\n",
      "step = 754: Loss = 7.867412567138672\n",
      "step = 755: Loss = 7.418511390686035\n",
      "step = 756: Loss = 8.772710800170898\n",
      "step = 757: Loss = 7.034853935241699\n",
      "step = 758: Loss = 7.356904029846191\n",
      "step = 759: Loss = 7.410023212432861\n",
      "step = 760: Loss = 8.001913070678711\n",
      "step = 761: Loss = 8.364176750183105\n",
      "step = 762: Loss = 7.991077899932861\n",
      "step = 763: Loss = 8.123760223388672\n",
      "step = 764: Loss = 8.415057182312012\n",
      "step = 765: Loss = 8.299118041992188\n",
      "step = 766: Loss = 7.312628746032715\n",
      "step = 767: Loss = 7.344865798950195\n",
      "step = 768: Loss = 8.77609634399414\n",
      "step = 769: Loss = 7.443885803222656\n",
      "step = 770: Loss = 8.221108436584473\n",
      "step = 771: Loss = 7.606740951538086\n",
      "step = 772: Loss = 7.886484146118164\n",
      "step = 773: Loss = 9.198259353637695\n",
      "step = 774: Loss = 8.545480728149414\n",
      "step = 775: Loss = 7.983807563781738\n",
      "step = 776: Loss = 7.39417839050293\n",
      "step = 777: Loss = 7.550689697265625\n",
      "step = 778: Loss = 7.96834135055542\n",
      "step = 779: Loss = 7.741114616394043\n",
      "step = 780: Loss = 8.047626495361328\n",
      "step = 781: Loss = 6.513177871704102\n",
      "step = 782: Loss = 8.272350311279297\n",
      "step = 783: Loss = 6.783071994781494\n",
      "step = 784: Loss = 5.994071006774902\n",
      "step = 785: Loss = 6.733758449554443\n",
      "step = 786: Loss = 6.738063812255859\n",
      "step = 787: Loss = 7.38554573059082\n",
      "step = 788: Loss = 6.584423065185547\n",
      "step = 789: Loss = 7.448366165161133\n",
      "step = 790: Loss = 6.032533168792725\n",
      "step = 791: Loss = 6.627138137817383\n",
      "step = 792: Loss = 8.01880931854248\n",
      "step = 793: Loss = 6.963930130004883\n",
      "step = 794: Loss = 6.144876003265381\n",
      "step = 795: Loss = 6.588186264038086\n",
      "step = 796: Loss = 6.158999919891357\n",
      "step = 797: Loss = 7.42025089263916\n",
      "step = 798: Loss = 7.5069169998168945\n",
      "step = 799: Loss = 6.377836227416992\n",
      "step = 800: Loss = 6.450638771057129\n",
      "step = 801: Loss = 5.82628059387207\n",
      "step = 802: Loss = 5.8407440185546875\n",
      "step = 803: Loss = 6.1017351150512695\n",
      "step = 804: Loss = 6.651336669921875\n",
      "step = 805: Loss = 6.438854217529297\n",
      "step = 806: Loss = 7.0272674560546875\n",
      "step = 807: Loss = 6.631654262542725\n",
      "step = 808: Loss = 6.7353363037109375\n",
      "step = 809: Loss = 5.316640853881836\n",
      "step = 810: Loss = 7.385606288909912\n",
      "step = 811: Loss = 5.877908706665039\n",
      "step = 812: Loss = 7.0652008056640625\n",
      "step = 813: Loss = 7.298612594604492\n",
      "step = 814: Loss = 6.6096391677856445\n",
      "step = 815: Loss = 7.186955451965332\n",
      "step = 816: Loss = 7.150147914886475\n",
      "step = 817: Loss = 8.215089797973633\n",
      "step = 818: Loss = 7.620058536529541\n",
      "step = 819: Loss = 7.381008625030518\n",
      "step = 820: Loss = 5.5058746337890625\n",
      "step = 821: Loss = 6.682021617889404\n",
      "step = 822: Loss = 6.234895706176758\n",
      "step = 823: Loss = 5.888542652130127\n",
      "step = 824: Loss = 7.553038120269775\n",
      "step = 825: Loss = 6.301486015319824\n",
      "step = 826: Loss = 6.449342250823975\n",
      "step = 827: Loss = 6.055559158325195\n",
      "step = 828: Loss = 6.310054779052734\n",
      "step = 829: Loss = 7.353748798370361\n",
      "step = 830: Loss = 7.2131547927856445\n",
      "step = 831: Loss = 7.244557857513428\n",
      "step = 832: Loss = 6.201398849487305\n",
      "step = 833: Loss = 6.2421770095825195\n",
      "step = 834: Loss = 6.366269588470459\n",
      "step = 835: Loss = 7.161565780639648\n",
      "step = 836: Loss = 5.55084753036499\n",
      "step = 837: Loss = 6.216469764709473\n",
      "step = 838: Loss = 7.313948631286621\n",
      "step = 839: Loss = 5.979182243347168\n",
      "step = 840: Loss = 6.211747169494629\n",
      "step = 841: Loss = 7.728155136108398\n",
      "step = 842: Loss = 7.547123908996582\n",
      "step = 843: Loss = 6.4501190185546875\n",
      "step = 844: Loss = 5.986903190612793\n",
      "step = 845: Loss = 6.194669723510742\n",
      "step = 846: Loss = 5.889938831329346\n",
      "step = 847: Loss = 6.703495025634766\n",
      "step = 848: Loss = 6.513652801513672\n",
      "step = 849: Loss = 6.699077129364014\n",
      "step = 850: Loss = 6.437349319458008\n",
      "step = 851: Loss = 6.19609260559082\n",
      "step = 852: Loss = 5.670131683349609\n",
      "step = 853: Loss = 6.503119468688965\n",
      "step = 854: Loss = 6.089852333068848\n",
      "step = 855: Loss = 5.990070343017578\n",
      "step = 856: Loss = 5.942846775054932\n",
      "step = 857: Loss = 5.5634918212890625\n",
      "step = 858: Loss = 8.005496978759766\n",
      "step = 859: Loss = 6.918919563293457\n",
      "step = 860: Loss = 6.211193084716797\n",
      "step = 861: Loss = 6.441708087921143\n",
      "step = 862: Loss = 6.1233906745910645\n",
      "step = 863: Loss = 6.96354866027832\n",
      "step = 864: Loss = 6.091947555541992\n",
      "step = 865: Loss = 5.960284233093262\n",
      "step = 866: Loss = 6.075684547424316\n",
      "step = 867: Loss = 5.6846232414245605\n",
      "step = 868: Loss = 7.460662841796875\n",
      "step = 869: Loss = 6.928366661071777\n",
      "step = 870: Loss = 6.029947280883789\n",
      "step = 871: Loss = 7.5408430099487305\n",
      "step = 872: Loss = 6.802298545837402\n",
      "step = 873: Loss = 6.206030368804932\n",
      "step = 874: Loss = 5.916753768920898\n",
      "step = 875: Loss = 6.816685676574707\n",
      "step = 876: Loss = 7.178678512573242\n",
      "step = 877: Loss = 5.91655158996582\n",
      "step = 878: Loss = 6.016966342926025\n",
      "step = 879: Loss = 5.708785057067871\n",
      "step = 880: Loss = 5.551892280578613\n",
      "step = 881: Loss = 6.50161075592041\n",
      "step = 882: Loss = 6.223231315612793\n",
      "step = 883: Loss = 6.658079624176025\n",
      "step = 884: Loss = 5.878454208374023\n",
      "step = 885: Loss = 6.098214149475098\n",
      "step = 886: Loss = 6.299332618713379\n",
      "step = 887: Loss = 6.000604629516602\n",
      "step = 888: Loss = 5.956225395202637\n",
      "step = 889: Loss = 6.290409088134766\n",
      "step = 890: Loss = 6.802302360534668\n",
      "step = 891: Loss = 6.785482406616211\n",
      "step = 892: Loss = 5.742929458618164\n",
      "step = 893: Loss = 5.680804252624512\n",
      "step = 894: Loss = 6.450538635253906\n",
      "step = 895: Loss = 6.339637756347656\n",
      "step = 896: Loss = 5.379244327545166\n",
      "step = 897: Loss = 6.546401023864746\n",
      "step = 898: Loss = 6.39918327331543\n",
      "step = 899: Loss = 5.376438140869141\n",
      "step = 900: Loss = 5.557674407958984\n",
      "step = 901: Loss = 5.789093971252441\n",
      "step = 902: Loss = 5.791711807250977\n",
      "step = 903: Loss = 5.680086135864258\n",
      "step = 904: Loss = 6.00446081161499\n",
      "step = 905: Loss = 5.762293815612793\n",
      "step = 906: Loss = 5.775628089904785\n",
      "step = 907: Loss = 5.660584449768066\n",
      "step = 908: Loss = 5.6264777183532715\n",
      "step = 909: Loss = 5.517523765563965\n",
      "step = 910: Loss = 6.45274543762207\n",
      "step = 911: Loss = 6.527351379394531\n",
      "step = 912: Loss = 5.664334297180176\n",
      "step = 913: Loss = 5.190680980682373\n",
      "step = 914: Loss = 5.47165584564209\n",
      "step = 915: Loss = 6.35184907913208\n",
      "step = 916: Loss = 5.314689636230469\n",
      "step = 917: Loss = 6.301499366760254\n",
      "step = 918: Loss = 6.553600311279297\n",
      "step = 919: Loss = 6.728543281555176\n",
      "step = 920: Loss = 5.618805885314941\n",
      "step = 921: Loss = 6.059361934661865\n",
      "step = 922: Loss = 6.103418827056885\n",
      "step = 923: Loss = 5.887993812561035\n",
      "step = 924: Loss = 6.7371625900268555\n",
      "step = 925: Loss = 5.072646141052246\n",
      "step = 926: Loss = 5.689725875854492\n",
      "step = 927: Loss = 5.957963943481445\n",
      "step = 928: Loss = 6.246728897094727\n",
      "step = 929: Loss = 6.105762958526611\n",
      "step = 930: Loss = 5.234560489654541\n",
      "step = 931: Loss = 6.057422637939453\n",
      "step = 932: Loss = 6.847684860229492\n",
      "step = 933: Loss = 6.043686389923096\n",
      "step = 934: Loss = 6.403200149536133\n",
      "step = 935: Loss = 7.04970121383667\n",
      "step = 936: Loss = 6.23803186416626\n",
      "step = 937: Loss = 6.554003715515137\n",
      "step = 938: Loss = 6.020968437194824\n",
      "step = 939: Loss = 6.163991451263428\n",
      "step = 940: Loss = 6.36602783203125\n",
      "step = 941: Loss = 5.440707206726074\n",
      "step = 942: Loss = 5.744229316711426\n",
      "step = 943: Loss = 5.220017433166504\n",
      "step = 944: Loss = 5.984819412231445\n",
      "step = 945: Loss = 5.17336368560791\n",
      "step = 946: Loss = 5.699038505554199\n",
      "step = 947: Loss = 6.035552024841309\n",
      "step = 948: Loss = 5.732444763183594\n",
      "step = 949: Loss = 6.002717971801758\n",
      "step = 950: Loss = 5.378759384155273\n",
      "step = 951: Loss = 5.805486679077148\n",
      "step = 952: Loss = 5.692818641662598\n",
      "step = 953: Loss = 5.468050956726074\n",
      "step = 954: Loss = 5.71270751953125\n",
      "step = 955: Loss = 6.177794933319092\n",
      "step = 956: Loss = 5.329146862030029\n",
      "step = 957: Loss = 6.42880916595459\n",
      "step = 958: Loss = 6.310510635375977\n",
      "step = 959: Loss = 5.7159881591796875\n",
      "step = 960: Loss = 6.202569007873535\n",
      "step = 961: Loss = 6.915900230407715\n",
      "step = 962: Loss = 6.51695442199707\n",
      "step = 963: Loss = 5.7774434089660645\n",
      "step = 964: Loss = 5.639400005340576\n",
      "step = 965: Loss = 6.433928489685059\n",
      "step = 966: Loss = 6.121323585510254\n",
      "step = 967: Loss = 5.9145402908325195\n",
      "step = 968: Loss = 5.808478355407715\n",
      "step = 969: Loss = 5.8701581954956055\n",
      "step = 970: Loss = 5.167161464691162\n",
      "step = 971: Loss = 4.956040382385254\n",
      "step = 972: Loss = 5.898886203765869\n",
      "step = 973: Loss = 5.919953346252441\n",
      "step = 974: Loss = 6.696527004241943\n",
      "step = 975: Loss = 5.460005760192871\n",
      "step = 976: Loss = 5.964244365692139\n",
      "step = 977: Loss = 5.586322784423828\n",
      "step = 978: Loss = 5.624380588531494\n",
      "step = 979: Loss = 5.910494327545166\n",
      "step = 980: Loss = 5.792123317718506\n",
      "step = 981: Loss = 5.493394374847412\n",
      "step = 982: Loss = 4.704676628112793\n",
      "step = 983: Loss = 5.041086196899414\n",
      "step = 984: Loss = 6.507565498352051\n",
      "step = 985: Loss = 5.5467000007629395\n",
      "step = 986: Loss = 5.687935829162598\n",
      "step = 987: Loss = 6.4418416023254395\n",
      "step = 988: Loss = 4.835610389709473\n",
      "step = 989: Loss = 5.708827972412109\n",
      "step = 990: Loss = 5.989147663116455\n",
      "step = 991: Loss = 6.847840309143066\n",
      "step = 992: Loss = 5.5159196853637695\n",
      "step = 993: Loss = 6.402896404266357\n",
      "step = 994: Loss = 5.193475723266602\n",
      "step = 995: Loss = 5.342554092407227\n",
      "step = 996: Loss = 5.949955463409424\n",
      "step = 997: Loss = 6.148221015930176\n",
      "step = 998: Loss = 5.6003570556640625\n",
      "step = 999: Loss = 6.06400203704834\n",
      "step = 1000: Loss = 5.634807586669922\n",
      "step = 1001: Loss = 5.373274803161621\n",
      "step = 1002: Loss = 6.199997901916504\n",
      "step = 1003: Loss = 5.5751495361328125\n",
      "step = 1004: Loss = 6.405198574066162\n",
      "step = 1005: Loss = 5.933802127838135\n",
      "step = 1006: Loss = 5.010655403137207\n",
      "step = 1007: Loss = 5.533576011657715\n",
      "step = 1008: Loss = 5.626971244812012\n",
      "step = 1009: Loss = 5.213188171386719\n",
      "step = 1010: Loss = 5.158970355987549\n",
      "step = 1011: Loss = 5.466595649719238\n",
      "step = 1012: Loss = 6.355784893035889\n",
      "step = 1013: Loss = 5.4845356941223145\n",
      "step = 1014: Loss = 5.811001777648926\n",
      "step = 1015: Loss = 5.950345039367676\n",
      "step = 1016: Loss = 5.726113319396973\n",
      "step = 1017: Loss = 6.016067981719971\n",
      "step = 1018: Loss = 5.708934783935547\n",
      "step = 1019: Loss = 6.204169273376465\n",
      "step = 1020: Loss = 5.9516520500183105\n",
      "step = 1021: Loss = 6.636709213256836\n",
      "step = 1022: Loss = 5.373715877532959\n",
      "step = 1023: Loss = 5.843390464782715\n",
      "step = 1024: Loss = 6.312769889831543\n",
      "step = 1025: Loss = 5.249967575073242\n",
      "step = 1026: Loss = 5.833620071411133\n",
      "step = 1027: Loss = 6.743008136749268\n",
      "step = 1028: Loss = 6.06411600112915\n",
      "step = 1029: Loss = 5.7401323318481445\n",
      "step = 1030: Loss = 5.918858051300049\n",
      "step = 1031: Loss = 5.575272560119629\n",
      "step = 1032: Loss = 4.968104362487793\n",
      "step = 1033: Loss = 5.732181072235107\n",
      "step = 1034: Loss = 5.20616340637207\n",
      "step = 1035: Loss = 5.072849750518799\n",
      "step = 1036: Loss = 5.7614874839782715\n",
      "step = 1037: Loss = 6.0364508628845215\n",
      "step = 1038: Loss = 5.747371196746826\n",
      "step = 1039: Loss = 5.84799861907959\n",
      "step = 1040: Loss = 6.815837860107422\n",
      "step = 1041: Loss = 6.51126766204834\n",
      "step = 1042: Loss = 5.313076496124268\n",
      "step = 1043: Loss = 6.9386372566223145\n",
      "step = 1044: Loss = 5.048471450805664\n",
      "step = 1045: Loss = 6.0079169273376465\n",
      "step = 1046: Loss = 5.515159606933594\n",
      "step = 1047: Loss = 6.237117767333984\n",
      "step = 1048: Loss = 6.301578998565674\n",
      "step = 1049: Loss = 5.459392547607422\n",
      "step = 1050: Loss = 6.364070892333984\n",
      "step = 1051: Loss = 6.6663103103637695\n",
      "step = 1052: Loss = 6.9900031089782715\n",
      "step = 1053: Loss = 5.355511665344238\n",
      "step = 1054: Loss = 7.047602653503418\n",
      "step = 1055: Loss = 5.389636039733887\n",
      "step = 1056: Loss = 5.550792694091797\n",
      "step = 1057: Loss = 6.110179424285889\n",
      "step = 1058: Loss = 5.886091232299805\n",
      "step = 1059: Loss = 5.864280700683594\n",
      "step = 1060: Loss = 5.926754951477051\n",
      "step = 1061: Loss = 5.659045219421387\n",
      "step = 1062: Loss = 6.261928558349609\n",
      "step = 1063: Loss = 5.641131401062012\n",
      "step = 1064: Loss = 5.465871810913086\n",
      "step = 1065: Loss = 6.615097999572754\n",
      "step = 1066: Loss = 6.036044120788574\n",
      "step = 1067: Loss = 6.7414984703063965\n",
      "step = 1068: Loss = 6.5672607421875\n",
      "step = 1069: Loss = 6.771232604980469\n",
      "step = 1070: Loss = 5.777129173278809\n",
      "step = 1071: Loss = 6.371370315551758\n",
      "step = 1072: Loss = 5.503569602966309\n",
      "step = 1073: Loss = 6.781388282775879\n",
      "step = 1074: Loss = 6.030117034912109\n",
      "step = 1075: Loss = 6.228667259216309\n",
      "step = 1076: Loss = 6.625423431396484\n",
      "step = 1077: Loss = 5.788622856140137\n",
      "step = 1078: Loss = 6.1465253829956055\n",
      "step = 1079: Loss = 5.8796467781066895\n",
      "step = 1080: Loss = 4.737897872924805\n",
      "step = 1081: Loss = 6.17258358001709\n",
      "step = 1082: Loss = 5.301212310791016\n",
      "step = 1083: Loss = 6.361488342285156\n",
      "step = 1084: Loss = 6.921584129333496\n",
      "step = 1085: Loss = 5.565004348754883\n",
      "step = 1086: Loss = 5.411228179931641\n",
      "step = 1087: Loss = 5.561247825622559\n",
      "step = 1088: Loss = 5.920442581176758\n",
      "step = 1089: Loss = 5.731423377990723\n",
      "step = 1090: Loss = 5.9683332443237305\n",
      "step = 1091: Loss = 7.059424877166748\n",
      "step = 1092: Loss = 6.473916053771973\n",
      "step = 1093: Loss = 5.59257698059082\n",
      "step = 1094: Loss = 6.609433174133301\n",
      "step = 1095: Loss = 6.5107526779174805\n",
      "step = 1096: Loss = 5.915868759155273\n",
      "step = 1097: Loss = 5.559207916259766\n",
      "step = 1098: Loss = 6.481935501098633\n",
      "step = 1099: Loss = 6.26148796081543\n",
      "step = 1100: Loss = 5.659767150878906\n",
      "step = 1101: Loss = 5.244989395141602\n",
      "step = 1102: Loss = 5.856317520141602\n",
      "step = 1103: Loss = 5.189547061920166\n",
      "step = 1104: Loss = 4.9048261642456055\n",
      "step = 1105: Loss = 5.6277923583984375\n",
      "step = 1106: Loss = 5.548792839050293\n",
      "step = 1107: Loss = 5.803065299987793\n",
      "step = 1108: Loss = 6.819690227508545\n",
      "step = 1109: Loss = 4.959193706512451\n",
      "step = 1110: Loss = 5.560781478881836\n",
      "step = 1111: Loss = 5.8240580558776855\n",
      "step = 1112: Loss = 5.280232906341553\n",
      "step = 1113: Loss = 5.9653825759887695\n",
      "step = 1114: Loss = 5.125310897827148\n",
      "step = 1115: Loss = 5.802590370178223\n",
      "step = 1116: Loss = 4.921238899230957\n",
      "step = 1117: Loss = 6.349742889404297\n",
      "step = 1118: Loss = 6.318763256072998\n",
      "step = 1119: Loss = 5.71396017074585\n",
      "step = 1120: Loss = 6.115235328674316\n",
      "step = 1121: Loss = 6.451754570007324\n",
      "step = 1122: Loss = 5.670948505401611\n",
      "step = 1123: Loss = 6.653682708740234\n",
      "step = 1124: Loss = 6.022747039794922\n",
      "step = 1125: Loss = 5.354556560516357\n",
      "step = 1126: Loss = 5.12646484375\n",
      "step = 1127: Loss = 5.3420023918151855\n",
      "step = 1128: Loss = 5.93021821975708\n",
      "step = 1129: Loss = 5.731520175933838\n",
      "step = 1130: Loss = 5.8824615478515625\n",
      "step = 1131: Loss = 5.485318660736084\n",
      "step = 1132: Loss = 5.3671555519104\n",
      "step = 1133: Loss = 6.2866339683532715\n",
      "step = 1134: Loss = 5.746743202209473\n",
      "step = 1135: Loss = 7.102593421936035\n",
      "step = 1136: Loss = 5.596046447753906\n",
      "step = 1137: Loss = 4.868520736694336\n",
      "step = 1138: Loss = 5.149635314941406\n",
      "step = 1139: Loss = 5.310920715332031\n",
      "step = 1140: Loss = 5.8566131591796875\n",
      "step = 1141: Loss = 6.552577018737793\n",
      "step = 1142: Loss = 5.835653781890869\n",
      "step = 1143: Loss = 5.499573230743408\n",
      "step = 1144: Loss = 5.415530204772949\n",
      "step = 1145: Loss = 5.276220321655273\n",
      "step = 1146: Loss = 4.581576347351074\n",
      "step = 1147: Loss = 6.250543594360352\n",
      "step = 1148: Loss = 5.665493011474609\n",
      "step = 1149: Loss = 5.415737628936768\n",
      "step = 1150: Loss = 6.348361968994141\n",
      "step = 1151: Loss = 5.405112266540527\n",
      "step = 1152: Loss = 5.186738967895508\n",
      "step = 1153: Loss = 6.035956859588623\n",
      "step = 1154: Loss = 5.410325050354004\n",
      "step = 1155: Loss = 5.444624900817871\n",
      "step = 1156: Loss = 5.2011003494262695\n",
      "step = 1157: Loss = 6.122177600860596\n",
      "step = 1158: Loss = 5.92385196685791\n",
      "step = 1159: Loss = 5.856215953826904\n",
      "step = 1160: Loss = 5.157252788543701\n",
      "step = 1161: Loss = 5.525313854217529\n",
      "step = 1162: Loss = 5.3444108963012695\n",
      "step = 1163: Loss = 5.651632785797119\n",
      "step = 1164: Loss = 5.397179126739502\n",
      "step = 1165: Loss = 5.889456748962402\n",
      "step = 1166: Loss = 5.8821563720703125\n",
      "step = 1167: Loss = 5.3017730712890625\n",
      "step = 1168: Loss = 5.683935165405273\n",
      "step = 1169: Loss = 5.608830451965332\n",
      "step = 1170: Loss = 5.931257247924805\n",
      "step = 1171: Loss = 5.469510078430176\n",
      "step = 1172: Loss = 4.5544586181640625\n",
      "step = 1173: Loss = 5.253081321716309\n",
      "step = 1174: Loss = 5.990566253662109\n",
      "step = 1175: Loss = 5.518136024475098\n",
      "step = 1176: Loss = 6.524590015411377\n",
      "step = 1177: Loss = 5.34788703918457\n",
      "step = 1178: Loss = 5.932426452636719\n",
      "step = 1179: Loss = 6.052751541137695\n",
      "step = 1180: Loss = 5.513882160186768\n",
      "step = 1181: Loss = 4.974844455718994\n",
      "step = 1182: Loss = 5.397658824920654\n",
      "step = 1183: Loss = 5.732373237609863\n",
      "step = 1184: Loss = 5.453937530517578\n",
      "step = 1185: Loss = 5.461267948150635\n",
      "step = 1186: Loss = 4.975067138671875\n",
      "step = 1187: Loss = 6.283487796783447\n",
      "step = 1188: Loss = 6.357161045074463\n",
      "step = 1189: Loss = 5.199014186859131\n",
      "step = 1190: Loss = 5.264389514923096\n",
      "step = 1191: Loss = 5.943310737609863\n",
      "step = 1192: Loss = 5.879337310791016\n",
      "step = 1193: Loss = 5.419013500213623\n",
      "step = 1194: Loss = 5.026834487915039\n",
      "step = 1195: Loss = 5.017807960510254\n",
      "step = 1196: Loss = 5.1022491455078125\n",
      "step = 1197: Loss = 5.634568214416504\n",
      "step = 1198: Loss = 5.315698623657227\n",
      "step = 1199: Loss = 5.367769241333008\n",
      "step = 1200: Loss = 4.744801044464111\n",
      "step = 1201: Loss = 5.386532783508301\n",
      "step = 1202: Loss = 5.847961902618408\n",
      "step = 1203: Loss = 5.906886100769043\n",
      "step = 1204: Loss = 5.472019195556641\n",
      "step = 1205: Loss = 6.696993827819824\n",
      "step = 1206: Loss = 5.835482120513916\n",
      "step = 1207: Loss = 5.976527214050293\n",
      "step = 1208: Loss = 5.262131690979004\n",
      "step = 1209: Loss = 5.217678070068359\n",
      "step = 1210: Loss = 5.950508117675781\n",
      "step = 1211: Loss = 6.234224319458008\n",
      "step = 1212: Loss = 5.67741060256958\n",
      "step = 1213: Loss = 5.3875274658203125\n",
      "step = 1214: Loss = 5.657896518707275\n",
      "step = 1215: Loss = 5.878213882446289\n",
      "step = 1216: Loss = 5.259008884429932\n",
      "step = 1217: Loss = 5.616771697998047\n",
      "step = 1218: Loss = 5.456279754638672\n",
      "step = 1219: Loss = 6.15689754486084\n",
      "step = 1220: Loss = 5.572955131530762\n",
      "step = 1221: Loss = 4.981184959411621\n",
      "step = 1222: Loss = 6.209356784820557\n",
      "step = 1223: Loss = 5.583827018737793\n",
      "step = 1224: Loss = 6.301012992858887\n",
      "step = 1225: Loss = 4.797233581542969\n",
      "step = 1226: Loss = 5.432307720184326\n",
      "step = 1227: Loss = 5.646844387054443\n",
      "step = 1228: Loss = 6.132460594177246\n",
      "step = 1229: Loss = 6.209282875061035\n",
      "step = 1230: Loss = 4.9121809005737305\n",
      "step = 1231: Loss = 5.7191362380981445\n",
      "step = 1232: Loss = 5.912400245666504\n",
      "step = 1233: Loss = 6.501984596252441\n",
      "step = 1234: Loss = 5.77220344543457\n",
      "step = 1235: Loss = 6.2574944496154785\n",
      "step = 1236: Loss = 5.542445182800293\n",
      "step = 1237: Loss = 6.306487083435059\n",
      "step = 1238: Loss = 6.4040679931640625\n",
      "step = 1239: Loss = 5.884938716888428\n",
      "step = 1240: Loss = 5.895848274230957\n",
      "step = 1241: Loss = 5.1018571853637695\n",
      "step = 1242: Loss = 5.624794006347656\n",
      "step = 1243: Loss = 6.811133861541748\n",
      "step = 1244: Loss = 6.0791015625\n",
      "step = 1245: Loss = 6.228149890899658\n",
      "step = 1246: Loss = 5.487966537475586\n",
      "step = 1247: Loss = 5.4093475341796875\n",
      "step = 1248: Loss = 6.606189250946045\n",
      "step = 1249: Loss = 6.471591949462891\n",
      "step = 1250: Loss = 5.668827056884766\n",
      "step = 1251: Loss = 5.737778663635254\n",
      "step = 1252: Loss = 4.927895545959473\n",
      "step = 1253: Loss = 5.7823357582092285\n",
      "step = 1254: Loss = 6.836372375488281\n",
      "step = 1255: Loss = 6.046629905700684\n",
      "step = 1256: Loss = 6.36335563659668\n",
      "step = 1257: Loss = 6.123348236083984\n",
      "step = 1258: Loss = 5.237312316894531\n",
      "step = 1259: Loss = 6.3866376876831055\n",
      "step = 1260: Loss = 6.279205322265625\n",
      "step = 1261: Loss = 5.568816661834717\n",
      "step = 1262: Loss = 4.938647270202637\n",
      "step = 1263: Loss = 5.467052936553955\n",
      "step = 1264: Loss = 5.338892936706543\n",
      "step = 1265: Loss = 5.566642761230469\n",
      "step = 1266: Loss = 5.26285982131958\n",
      "step = 1267: Loss = 4.967723846435547\n",
      "step = 1268: Loss = 5.256528854370117\n",
      "step = 1269: Loss = 5.194281578063965\n",
      "step = 1270: Loss = 5.826470375061035\n",
      "step = 1271: Loss = 5.091355323791504\n",
      "step = 1272: Loss = 5.84666633605957\n",
      "step = 1273: Loss = 6.207265853881836\n",
      "step = 1274: Loss = 5.554520606994629\n",
      "step = 1275: Loss = 5.652887344360352\n",
      "step = 1276: Loss = 4.788171768188477\n",
      "step = 1277: Loss = 5.629858016967773\n",
      "step = 1278: Loss = 5.760599136352539\n",
      "step = 1279: Loss = 5.580663204193115\n",
      "step = 1280: Loss = 5.6344828605651855\n",
      "step = 1281: Loss = 5.410747051239014\n",
      "step = 1282: Loss = 5.244414329528809\n",
      "step = 1283: Loss = 5.186026096343994\n",
      "step = 1284: Loss = 4.730319976806641\n",
      "step = 1285: Loss = 5.0739359855651855\n",
      "step = 1286: Loss = 6.523413181304932\n",
      "step = 1287: Loss = 5.530041694641113\n",
      "step = 1288: Loss = 5.971103668212891\n",
      "step = 1289: Loss = 5.879003524780273\n",
      "step = 1290: Loss = 5.406426429748535\n",
      "step = 1291: Loss = 7.640153884887695\n",
      "step = 1292: Loss = 5.427158832550049\n",
      "step = 1293: Loss = 5.714786529541016\n",
      "step = 1294: Loss = 6.263053894042969\n",
      "step = 1295: Loss = 6.832179546356201\n",
      "step = 1296: Loss = 6.257112503051758\n",
      "step = 1297: Loss = 5.973845958709717\n",
      "step = 1298: Loss = 6.53951358795166\n",
      "step = 1299: Loss = 5.165686130523682\n",
      "step = 1300: Loss = 5.1156907081604\n",
      "step = 1301: Loss = 5.916715621948242\n",
      "step = 1302: Loss = 5.130973815917969\n",
      "step = 1303: Loss = 6.102076530456543\n",
      "step = 1304: Loss = 6.112051963806152\n",
      "step = 1305: Loss = 4.785391330718994\n",
      "step = 1306: Loss = 5.088796615600586\n",
      "step = 1307: Loss = 6.121002197265625\n",
      "step = 1308: Loss = 4.684773921966553\n",
      "step = 1309: Loss = 6.096843719482422\n",
      "step = 1310: Loss = 4.806872844696045\n",
      "step = 1311: Loss = 5.79592752456665\n",
      "step = 1312: Loss = 4.828028202056885\n",
      "step = 1313: Loss = 6.275416374206543\n",
      "step = 1314: Loss = 5.940351963043213\n",
      "step = 1315: Loss = 5.800115585327148\n",
      "step = 1316: Loss = 5.267422199249268\n",
      "step = 1317: Loss = 5.818900108337402\n",
      "step = 1318: Loss = 6.25803279876709\n",
      "step = 1319: Loss = 7.376559257507324\n",
      "step = 1320: Loss = 5.634873390197754\n",
      "step = 1321: Loss = 6.284728050231934\n",
      "step = 1322: Loss = 5.539740085601807\n",
      "step = 1323: Loss = 5.924739837646484\n",
      "step = 1324: Loss = 6.064213752746582\n",
      "step = 1325: Loss = 5.013899803161621\n",
      "step = 1326: Loss = 5.692254066467285\n",
      "step = 1327: Loss = 5.388958930969238\n",
      "step = 1328: Loss = 4.836430072784424\n",
      "step = 1329: Loss = 5.337536811828613\n",
      "step = 1330: Loss = 6.236376762390137\n",
      "step = 1331: Loss = 5.677943706512451\n",
      "step = 1332: Loss = 5.687660217285156\n",
      "step = 1333: Loss = 5.663271427154541\n",
      "step = 1334: Loss = 5.716634750366211\n",
      "step = 1335: Loss = 5.350531101226807\n",
      "step = 1336: Loss = 5.500321865081787\n",
      "step = 1337: Loss = 6.428020477294922\n",
      "step = 1338: Loss = 6.081483364105225\n",
      "step = 1339: Loss = 6.001720905303955\n",
      "step = 1340: Loss = 6.048461437225342\n",
      "step = 1341: Loss = 5.011748313903809\n",
      "step = 1342: Loss = 5.802070617675781\n",
      "step = 1343: Loss = 5.771381378173828\n",
      "step = 1344: Loss = 6.212809085845947\n",
      "step = 1345: Loss = 5.681856632232666\n",
      "step = 1346: Loss = 5.573379039764404\n",
      "step = 1347: Loss = 6.033994674682617\n",
      "step = 1348: Loss = 5.655924320220947\n",
      "step = 1349: Loss = 5.525757312774658\n",
      "step = 1350: Loss = 5.598850250244141\n",
      "step = 1351: Loss = 4.934884071350098\n",
      "step = 1352: Loss = 4.871499061584473\n",
      "step = 1353: Loss = 5.676393508911133\n",
      "step = 1354: Loss = 5.343526363372803\n",
      "step = 1355: Loss = 4.451056957244873\n",
      "step = 1356: Loss = 5.054220199584961\n",
      "step = 1357: Loss = 4.958337783813477\n",
      "step = 1358: Loss = 5.8966851234436035\n",
      "step = 1359: Loss = 5.646304130554199\n",
      "step = 1360: Loss = 5.783141136169434\n",
      "step = 1361: Loss = 5.515918731689453\n",
      "step = 1362: Loss = 4.962776184082031\n",
      "step = 1363: Loss = 5.414833068847656\n",
      "step = 1364: Loss = 5.183637619018555\n",
      "step = 1365: Loss = 5.154351711273193\n",
      "step = 1366: Loss = 4.890348434448242\n",
      "step = 1367: Loss = 5.644606590270996\n",
      "step = 1368: Loss = 5.312704563140869\n",
      "step = 1369: Loss = 5.47963809967041\n",
      "step = 1370: Loss = 5.121030807495117\n",
      "step = 1371: Loss = 5.307638168334961\n",
      "step = 1372: Loss = 6.20166540145874\n",
      "step = 1373: Loss = 6.004453659057617\n",
      "step = 1374: Loss = 4.9332146644592285\n",
      "step = 1375: Loss = 5.326812744140625\n",
      "step = 1376: Loss = 5.2637224197387695\n",
      "step = 1377: Loss = 4.403692245483398\n",
      "step = 1378: Loss = 5.874335765838623\n",
      "step = 1379: Loss = 4.983085632324219\n",
      "step = 1380: Loss = 5.237677097320557\n",
      "step = 1381: Loss = 5.716738224029541\n",
      "step = 1382: Loss = 5.792624473571777\n",
      "step = 1383: Loss = 5.763377666473389\n",
      "step = 1384: Loss = 5.32100248336792\n",
      "step = 1385: Loss = 4.881214141845703\n",
      "step = 1386: Loss = 4.671929359436035\n",
      "step = 1387: Loss = 5.677016258239746\n",
      "step = 1388: Loss = 5.4544854164123535\n",
      "step = 1389: Loss = 6.34664249420166\n",
      "step = 1390: Loss = 6.109560489654541\n",
      "step = 1391: Loss = 5.2670512199401855\n",
      "step = 1392: Loss = 5.7699761390686035\n",
      "step = 1393: Loss = 5.247653961181641\n",
      "step = 1394: Loss = 6.107872486114502\n",
      "step = 1395: Loss = 5.711875915527344\n",
      "step = 1396: Loss = 5.409732341766357\n",
      "step = 1397: Loss = 5.456355094909668\n",
      "step = 1398: Loss = 5.523313522338867\n",
      "step = 1399: Loss = 5.436994552612305\n",
      "step = 1400: Loss = 6.388477802276611\n",
      "step = 1401: Loss = 5.705740451812744\n",
      "step = 1402: Loss = 6.42650032043457\n",
      "step = 1403: Loss = 5.727313041687012\n",
      "step = 1404: Loss = 5.271009922027588\n",
      "step = 1405: Loss = 5.741796493530273\n",
      "step = 1406: Loss = 5.402077674865723\n",
      "step = 1407: Loss = 5.89614725112915\n",
      "step = 1408: Loss = 6.438398838043213\n",
      "step = 1409: Loss = 6.286226272583008\n",
      "step = 1410: Loss = 5.80303955078125\n",
      "step = 1411: Loss = 5.759739398956299\n",
      "step = 1412: Loss = 6.096578598022461\n",
      "step = 1413: Loss = 5.573097229003906\n",
      "step = 1414: Loss = 5.552721977233887\n",
      "step = 1415: Loss = 4.432208061218262\n",
      "step = 1416: Loss = 6.029956817626953\n",
      "step = 1417: Loss = 5.37636661529541\n",
      "step = 1418: Loss = 6.015378952026367\n",
      "step = 1419: Loss = 6.565176010131836\n",
      "step = 1420: Loss = 6.177069664001465\n",
      "step = 1421: Loss = 5.491866111755371\n",
      "step = 1422: Loss = 5.772538661956787\n",
      "step = 1423: Loss = 5.541202068328857\n",
      "step = 1424: Loss = 5.809800624847412\n",
      "step = 1425: Loss = 5.567571640014648\n",
      "step = 1426: Loss = 5.439839839935303\n",
      "step = 1427: Loss = 5.377002716064453\n",
      "step = 1428: Loss = 5.291523456573486\n",
      "step = 1429: Loss = 5.123852729797363\n",
      "step = 1430: Loss = 5.735780715942383\n",
      "step = 1431: Loss = 5.183518886566162\n",
      "step = 1432: Loss = 5.502418041229248\n",
      "step = 1433: Loss = 5.087677001953125\n",
      "step = 1434: Loss = 4.813303470611572\n",
      "step = 1435: Loss = 4.949656963348389\n",
      "step = 1436: Loss = 5.150671005249023\n",
      "step = 1437: Loss = 5.15218448638916\n",
      "step = 1438: Loss = 5.149323463439941\n",
      "step = 1439: Loss = 6.591513633728027\n",
      "step = 1440: Loss = 5.649667739868164\n",
      "step = 1441: Loss = 5.129276275634766\n",
      "step = 1442: Loss = 5.228725433349609\n",
      "step = 1443: Loss = 5.7767181396484375\n",
      "step = 1444: Loss = 4.926716327667236\n",
      "step = 1445: Loss = 6.1155877113342285\n",
      "step = 1446: Loss = 5.691287517547607\n",
      "step = 1447: Loss = 5.319511413574219\n",
      "step = 1448: Loss = 5.765836715698242\n",
      "step = 1449: Loss = 5.895837783813477\n",
      "step = 1450: Loss = 5.741899490356445\n",
      "step = 1451: Loss = 5.7885565757751465\n",
      "step = 1452: Loss = 5.181833267211914\n",
      "step = 1453: Loss = 5.660522937774658\n",
      "step = 1454: Loss = 5.720053672790527\n",
      "step = 1455: Loss = 6.295058727264404\n",
      "step = 1456: Loss = 5.739134311676025\n",
      "step = 1457: Loss = 6.124293804168701\n",
      "step = 1458: Loss = 6.151200294494629\n",
      "step = 1459: Loss = 5.664882183074951\n",
      "step = 1460: Loss = 4.89330530166626\n",
      "step = 1461: Loss = 5.39370584487915\n",
      "step = 1462: Loss = 7.410363674163818\n",
      "step = 1463: Loss = 6.626032829284668\n",
      "step = 1464: Loss = 5.950742721557617\n",
      "step = 1465: Loss = 5.916231155395508\n",
      "step = 1466: Loss = 5.419703006744385\n",
      "step = 1467: Loss = 5.6917338371276855\n",
      "step = 1468: Loss = 7.3301591873168945\n",
      "step = 1469: Loss = 5.567564010620117\n",
      "step = 1470: Loss = 6.139403343200684\n",
      "step = 1471: Loss = 5.776682376861572\n",
      "step = 1472: Loss = 6.208823204040527\n",
      "step = 1473: Loss = 6.029279708862305\n",
      "step = 1474: Loss = 5.808829307556152\n",
      "step = 1475: Loss = 5.884568691253662\n",
      "step = 1476: Loss = 5.588780879974365\n",
      "step = 1477: Loss = 5.397878170013428\n",
      "step = 1478: Loss = 5.910947322845459\n",
      "step = 1479: Loss = 6.32360315322876\n",
      "step = 1480: Loss = 5.5796661376953125\n",
      "step = 1481: Loss = 5.761686325073242\n",
      "step = 1482: Loss = 5.3000288009643555\n",
      "step = 1483: Loss = 5.524505615234375\n",
      "step = 1484: Loss = 5.171125888824463\n",
      "step = 1485: Loss = 5.397831916809082\n",
      "step = 1486: Loss = 5.698977947235107\n",
      "step = 1487: Loss = 6.342461585998535\n",
      "step = 1488: Loss = 5.703762531280518\n",
      "step = 1489: Loss = 6.053927421569824\n",
      "step = 1490: Loss = 6.182268142700195\n",
      "step = 1491: Loss = 5.342776298522949\n",
      "step = 1492: Loss = 6.342498779296875\n",
      "step = 1493: Loss = 6.55289363861084\n",
      "step = 1494: Loss = 5.773107051849365\n",
      "step = 1495: Loss = 5.8871307373046875\n",
      "step = 1496: Loss = 5.834343433380127\n",
      "step = 1497: Loss = 5.537221908569336\n",
      "step = 1498: Loss = 5.901727199554443\n",
      "step = 1499: Loss = 5.356698036193848\n",
      "step = 1500: Loss = 5.565321922302246\n",
      "step = 1501: Loss = 5.9714250564575195\n"
     ]
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# pipeline which will feed data to the agent\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "\n",
    "with tf.compat.v2.summary.record_if(True):\n",
    "    metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        tf_env_eval,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics')\n",
    "\n",
    "    # Train and evaluate\n",
    "    while global_step.numpy() <= num_iterations:\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "        experience, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience)\n",
    "        print('step = {0}: Loss = {1}'.format(global_step.numpy(), train_loss.loss))\n",
    "        with eval_summary_writer.as_default():\n",
    "            tf.summary.scalar(name='loss', data=train_loss.loss, step=global_step)\n",
    "        if global_step.numpy() % eval_interval == 0:\n",
    "            train_checkpointer.save(global_step)\n",
    "            metric_utils.eager_compute(\n",
    "                eval_metrics,\n",
    "                tf_env_eval,\n",
    "                eval_policy,\n",
    "                num_episodes=num_eval_episodes,\n",
    "                train_step=global_step,\n",
    "                summary_writer=eval_summary_writer,\n",
    "                summary_prefix='Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:103: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:104: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "data_test = DL.get_customer_data(DL.loadData('../../data/load1213.csv'),\n",
    "                                         DL.loadPrice('../../data/price.csv'), customer)\n",
    "tf_env_test = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_test, test=True))\n",
    "time_step_test = tf_env_test.reset()\n",
    "\n",
    "while not time_step_test.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step_test)\n",
    "    time_step_test = tf_env_test.step(action_step.action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
