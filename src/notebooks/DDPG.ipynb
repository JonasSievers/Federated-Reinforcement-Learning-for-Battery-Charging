{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient \n",
    "\n",
    "This notebook implements the DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import ddpg\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import utils.Dataloader as DL\n",
    "import utils.actorNetworkCustom as actornet\n",
    "import utils.criticNetworkCustom as criticnet\n",
    "import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of training iterations. It determines how many times the agent will go through the entire training process, \n",
    "adjusting its policy and value functions based on collected experiences. A sufficient number of iterations is crucial \n",
    "to allow the agent to learn and adapt to the environment, improving its performance over time.\n",
    "\"\"\"\n",
    "num_iterations = 1500\n",
    "customer = 1\n",
    "\n",
    "# Params for collect\n",
    "\"\"\"\n",
    "Defines the number of initial steps where the agent collects experiences randomly before the training begins. \n",
    "This helps to populate the replay buffer with diverse initial data.\n",
    "A well-populated replay buffer provides a diverse set of experiences for the agent to learn from, \n",
    "enhancing the stability and effectiveness of training.\n",
    "\"\"\"\n",
    "initial_collect_steps = 1000\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of steps the agent takes to collect experiences in each training iteration. \n",
    "It controls the balance between exploration and exploitation during training.\n",
    "Adequate exploration is necessary for discovering optimal policies. \n",
    "Adjusting this parameter impacts how often the agent explores its environment and updates its knowledge.\n",
    "\"\"\"\n",
    "collect_steps_per_iteration = 2000\n",
    "\n",
    "\"\"\"\n",
    "Sets the capacity of the replay buffer, a memory structure storing past experiences for the agent to sample during training.\n",
    "A sufficiently large replay buffer allows the agent to store and learn from a diverse set of experiences, \n",
    "mitigating issues related to correlated data and improving sample efficiency.\n",
    "\"\"\"\n",
    "replay_buffer_capacity = 1000000\n",
    "\n",
    "\"\"\"\n",
    "Determines the standard deviation of the Ornstein-Uhlenbeck process, which introduces exploration noise in the action space.\n",
    "Exploration noise aids the agent in exploring its action space,\n",
    "preventing it from getting stuck in local optima and promoting more robust learning.\n",
    "\"\"\"\n",
    "ou_stddev = 0.2\n",
    "\n",
    "\"\"\"\n",
    "Introduces a damping term to the Ornstein-Uhlenbeck process, influencing the exploration noise.\n",
    "Damping helps control the intensity of exploration noise, \n",
    "allowing a balance between exploration and exploitation based on the task's requirements.\n",
    "\"\"\"\n",
    "ou_damping = 0.15\n",
    "\n",
    "# Params for target update\n",
    "\"\"\"\n",
    "Represents the soft update coefficient for updating target networks, \n",
    "determining the degree to which the target networks track the main networks.\n",
    "Soft updates help stabilize training by slowly blending target values, \n",
    "preventing abrupt changes and improving the convergence of the learning process.\n",
    "\"\"\"\n",
    "target_update_tau = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Defines how often the target networks are updated in terms of training steps.\n",
    "Controlling the update frequency balances stability and responsiveness, \n",
    "preventing the target networks from lagging too far behind or updating too frequently.\n",
    "\"\"\"\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for train\n",
    "\"\"\"\n",
    "Specifies the number of gradient descent steps taken on the training batch in each training iteration.\n",
    "Adjusting this parameter impacts the convergence speed of the training process, \n",
    "influencing how much the agent learns from each collected batch of experiences.\n",
    "\"\"\"\n",
    "train_steps_per_iteration = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the size of the training batch sampled from the replay buffer.\n",
    "The batch size affects the efficiency of training; \n",
    "a well-chosen size balances computational efficiency and the stability of the learning process.\n",
    "\"\"\"\n",
    "batch_size = 48 * 7\n",
    "\n",
    "\"\"\"\n",
    "Specifies the learning rate for the actor (policy) network during gradient descent.\n",
    "The learning rate controls the size of the step taken during optimization. \n",
    "A suitable learning rate ensures the model converges effectively without overshooting or getting stuck in local minima.\n",
    "\"\"\"\n",
    "actor_learning_rate = 1e-4\n",
    "\n",
    "\"\"\"\n",
    "Defines the learning rate for the critic (Q-value) network during gradient descent.\n",
    "Similar to the actor learning rate, an appropriate critic learning rate influences \n",
    "the convergence and stability of the critic network, which plays a crucial role in estimating Q-values.\n",
    "\"\"\"\n",
    "critic_learning_rate = 1e-3\n",
    "\n",
    "\"\"\"\n",
    "An optional parameter for clipping the gradient of the Q-value with respect to actions.\n",
    "Clipping gradients can prevent large updates that may destabilize training, \n",
    "acting as a form of regularization and improving the robustness of the learning process.\n",
    "\"\"\"\n",
    "dqda_clipping = None\n",
    "\n",
    "\"\"\"\n",
    "Specifies the loss function for temporal difference (TD) errors, \n",
    "representing the discrepancy between predicted and actual Q-values.\n",
    "The choice of loss function influences how the agent updates its value estimates. \n",
    "Huber loss, as specified here, is robust to outliers and provides a balance between mean squared error and mean absolute error.\n",
    "\"\"\"\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "\n",
    "\"\"\"\n",
    "Represents the discount factor applied to future rewards in the Q-value estimation.\n",
    "Discounting future rewards emphasizes the importance of immediate rewards, e\n",
    "nabling the agent to make more informed decisions that consider both short-term and long-term consequences.\n",
    "\"\"\"\n",
    "gamma = 0.99\n",
    "\n",
    "\"\"\"\n",
    "Scales the rewards during training.Scaling rewards helps to control the impact of reward magnitudes on the learning process, \n",
    "preventing issues related to overly large or small rewards.\n",
    "\"\"\"\n",
    "reward_scale_factor = 1.0\n",
    "\n",
    "\"\"\"An optional parameter for clipping gradients during training.\"\"\"\n",
    "gradient_clipping = None\n",
    "\n",
    "# Params for eval and checkpoints\n",
    "\"\"\"\n",
    "Specifies the number of episodes used for evaluating the agent's performance.\n",
    "Evaluating the agent's performance provides insights into its generalization \n",
    "capabilities and allows for monitoring progress over time.\"\"\"\n",
    "num_eval_episodes = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the frequency (in iterations) at which evaluations are performed.\n",
    "Regular evaluations help track the agent's progress, enabling the identification of potential issues and providing \n",
    "a basis for comparison between different training iterations.\n",
    "\"\"\"\n",
    "eval_interval = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = DL.get_customer_data(DL.loadData('../../data/load1011.csv'), DL.loadPrice('../../data/price.csv'), customer)\n",
    "data_eval = DL.get_customer_data(DL.loadData('../../data/load1112.csv'), DL.loadPrice('../../data/price.csv'), customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0:30</th>\n",
       "      <th>1:00</th>\n",
       "      <th>1:30</th>\n",
       "      <th>2:00</th>\n",
       "      <th>2:30</th>\n",
       "      <th>3:00</th>\n",
       "      <th>3:30</th>\n",
       "      <th>4:00</th>\n",
       "      <th>4:30</th>\n",
       "      <th>5:00</th>\n",
       "      <th>...</th>\n",
       "      <th>19:30</th>\n",
       "      <th>20:00</th>\n",
       "      <th>20:30</th>\n",
       "      <th>21:00</th>\n",
       "      <th>21:30</th>\n",
       "      <th>22:00</th>\n",
       "      <th>22:30</th>\n",
       "      <th>23:00</th>\n",
       "      <th>23:30</th>\n",
       "      <th>0:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0:30  1:00  1:30  2:00  2:30  3:00  3:30  4:00  4:30  5:00  ...  19:30  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "360   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "361   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "362   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "363   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "364   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "     20:00  20:30  21:00  21:30  22:00  22:30  23:00  23:30  0:00  \n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...   ...  \n",
       "360    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "361    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "362    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "363    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "364    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "\n",
       "[365 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible network architectures\n",
    "dense_layers = [\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "]\n",
    "\n",
    "cnn_layers = [\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=1, name=\"conv1\"),\n",
    "    tf.keras.layers.Conv1D(filters=48, kernel_size=1, name=\"conv2\"),\n",
    "    tf.keras.layers.Conv1D(filters=48, kernel_size=1, name=\"conv3\"),\n",
    "    tf.keras.layers.Flatten(name=\"flatten4\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "]\n",
    "\n",
    "lstm_layers = [\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.LSTM(units=64, return_sequences=True, name='lstm1'),\n",
    "    tf.keras.layers.LSTM(units=64, return_sequences=True, name='lstm2'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "]\n",
    "\n",
    "bilstm_layers = [\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(units=4, activation='relu', name='dense'),\n",
    "]\n",
    "\n",
    "resnet_layers = [\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Add(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "]\n",
    "\n",
    "ensemble_layers = [\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used to create a copy of a Keras layer. Here's a step-by-step explanation:\n",
    "\n",
    "1. layer.__class__.__name__: This extracts the class name of the original layer. For example, if the original layer is an instance of tf.keras.layers.Conv1D, this would be 'Conv1D'.\n",
    "2. layer.get_config(): This retrieves the configuration of the original layer. The configuration includes all the parameters used to initialize the layer, such as the number of filters, kernel size, activation function, etc.\n",
    "3. {'class_name': layer.__class__.__name__, 'config': layer.get_config()}: This creates a dictionary containing the class name and configuration of the original layer.\n",
    "4. tf.keras.layers.deserialize(...): This function is part of the Keras API and is used to instantiate a layer from its class name and configuration. It essentially reconstructs a layer based on the provided information.\n",
    "\"\"\"\n",
    "def copy_layer(layer):\n",
    "    return tf.keras.layers.deserialize({'class_name': layer.__class__.__name__, 'config': layer.get_config()})\n",
    "\n",
    "#CHANGE: dense_layers, cnn_layers\n",
    "#ensemble_layers, resnet_layers, bilstm_layers, lstm_layers, dense_layers, cnn_layers\n",
    "custom_layers = cnn_layers\n",
    "\n",
    "# Create copies of the original layers\n",
    "actor_layers = [copy_layer(layer) for layer in custom_layers]\n",
    "target_actor_layers = [copy_layer(layer) for layer in custom_layers]\n",
    "critic_layers = [copy_layer(layer) for layer in custom_layers]\n",
    "target_critic_layers = [copy_layer(layer) for layer in custom_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec:  BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(3.4028235e+38, dtype=float32))\n",
      "action_spec:  BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2.3, dtype=float32), maximum=array(12.5, dtype=float32))\n",
      "observation_spec:  BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(3.4028235e+38, dtype=float32))\n",
      "action_spec:  BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2.3, dtype=float32), maximum=array(12.5, dtype=float32))\n",
      "state:  (1, 64)\n",
      "Custom Layer dense_6 Input Shape: (1, 64)\n",
      "Custom Layer dense_6 Output Shape: (1, 16)\n",
      "Custom Layer conv1 Input Shape: (1, 16)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'CriticNetworkCustom' (type CriticNetworkCustom).\n\nInput 0 of layer \"conv1\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (1, 16)\n\nCall arguments received by layer 'CriticNetworkCustom' (type CriticNetworkCustom):\n  • observations=('tf.Tensor(shape=(1, 4), dtype=float32)', 'tf.Tensor(shape=(1, 1), dtype=float32)')\n  • step_type=tf.Tensor(shape=(1,), dtype=int32)\n  • network_state=()\n  In call to configurable 'DdpgAgent' (<class 'tf_agents.agents.ddpg.ddpg_agent.DdpgAgent'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\DDPG.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m\"\"\"# Define the critic network, responsible for estimating the Q-values for state-action pairs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mcritic_net = ddpg.critic_network.CriticNetwork(\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m    input_tensor_spec=(tf_env_train.observation_spec(), tf_env_train.action_spec()),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m    joint_fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m    activation_fn=tf.keras.activations.relu\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m)\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Create a DDPG agent using the defined actor and critic networks, along with other parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m tf_agent \u001b[39m=\u001b[39m ddpg_agent\u001b[39m.\u001b[39;49mDdpgAgent(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     tf_env_train\u001b[39m.\u001b[39;49mtime_step_spec(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     tf_env_train\u001b[39m.\u001b[39;49maction_spec(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     actor_network\u001b[39m=\u001b[39;49mactor_net,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     target_actor_network\u001b[39m=\u001b[39;49mtarget_actor_net,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     critic_network\u001b[39m=\u001b[39;49mcritic_net,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     target_critic_network\u001b[39m=\u001b[39;49mtarget_critic_net,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     actor_optimizer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mAdamOptimizer(learning_rate\u001b[39m=\u001b[39;49mactor_learning_rate),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     critic_optimizer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mAdamOptimizer(learning_rate\u001b[39m=\u001b[39;49mcritic_learning_rate),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     ou_stddev\u001b[39m=\u001b[39;49mou_stddev, \u001b[39m# Standard deviation for Ornstein-Uhlenbeck noise\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     ou_damping\u001b[39m=\u001b[39;49mou_damping, \u001b[39m# Damping term for Ornstein-Uhlenbeck noise\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     target_update_tau\u001b[39m=\u001b[39;49mtarget_update_tau, \u001b[39m# Soft update coefficient for target networks\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     target_update_period\u001b[39m=\u001b[39;49mtarget_update_period, \u001b[39m# Frequency of updating target networks\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     dqda_clipping\u001b[39m=\u001b[39;49mdqda_clipping, \u001b[39m# Optional clipping of the gradient of Q-value with respect to actions\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     td_errors_loss_fn\u001b[39m=\u001b[39;49mtd_errors_loss_fn, \u001b[39m# Loss function for temporal difference errors\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     gamma\u001b[39m=\u001b[39;49mgamma, \u001b[39m# Discount factor for future rewards\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     reward_scale_factor\u001b[39m=\u001b[39;49mreward_scale_factor, \u001b[39m# Scaling factor for rewards during training\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     gradient_clipping\u001b[39m=\u001b[39;49mgradient_clipping, \u001b[39m# Optional clipping of gradients during training\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     debug_summaries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m# Disable debug summaries\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     summarize_grads_and_vars\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Disable summarizing gradients and variables\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     train_step_counter\u001b[39m=\u001b[39;49mglobal_step,  \u001b[39m# Use the global step as the train step counter\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/DDPG.ipynb#X10sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m tf_agent\u001b[39m.\u001b[39minitialize()\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\gin\\config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[1;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\gin\\utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[0;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\gin\\config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39mnew_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\agents\\ddpg\\ddpg_agent.py:142\u001b[0m, in \u001b[0;36mDdpgAgent.__init__\u001b[1;34m(self, time_step_spec, action_spec, actor_network, critic_network, actor_optimizer, critic_optimizer, ou_stddev, ou_damping, target_actor_network, target_critic_network, target_update_tau, target_update_period, dqda_clipping, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, name)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_critic_network \u001b[39m=\u001b[39m critic_network\n\u001b[0;32m    141\u001b[0m critic_input_spec \u001b[39m=\u001b[39m (time_step_spec\u001b[39m.\u001b[39mobservation, action_spec)\n\u001b[1;32m--> 142\u001b[0m critic_network\u001b[39m.\u001b[39;49mcreate_variables(critic_input_spec)\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m target_critic_network:\n\u001b[0;32m    144\u001b[0m   target_critic_network\u001b[39m.\u001b[39mcreate_variables(critic_input_spec)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\networks\\network.py:223\u001b[0m, in \u001b[0;36mNetwork.create_variables\u001b[1;34m(self, input_tensor_spec, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m initial_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_initial_state(batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    222\u001b[0m step_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfill((\u001b[39m1\u001b[39m,), time_step\u001b[39m.\u001b[39mStepType\u001b[39m.\u001b[39mFIRST)\n\u001b[1;32m--> 223\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\n\u001b[0;32m    224\u001b[0m     random_input, step_type\u001b[39m=\u001b[39mstep_type, network_state\u001b[39m=\u001b[39minitial_state, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_calc_unbatched_spec\u001b[39m(x):\n\u001b[0;32m    228\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Build Network output spec by removing previously added batch dimension.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m    Specs without batch dimension representing x.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\networks\\network.py:440\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    433\u001b[0m     \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mis_tensor(network_state)\n\u001b[0;32m    434\u001b[0m     \u001b[39mand\u001b[39;00m network_state \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, ())\n\u001b[0;32m    435\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnetwork_state\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m call_argspec\u001b[39m.\u001b[39margs\n\u001b[0;32m    436\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m call_argspec\u001b[39m.\u001b[39mkeywords\n\u001b[0;32m    437\u001b[0m ):\n\u001b[0;32m    438\u001b[0m   normalized_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mnetwork_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 440\u001b[0m outputs, new_state \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(Network, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnormalized_kwargs)  \u001b[39m# pytype: disable=attribute-error  # typed-keras\u001b[39;00m\n\u001b[0;32m    442\u001b[0m nest_utils\u001b[39m.\u001b[39massert_matching_dtypes_and_inner_shapes(\n\u001b[0;32m    443\u001b[0m     new_state,\n\u001b[0;32m    444\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_spec,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     specs_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`state_spec`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    451\u001b[0m \u001b[39mreturn\u001b[39;00m outputs, new_state\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\utils\\criticNetworkCustom.py:59\u001b[0m, in \u001b[0;36mCriticNetworkCustom.call\u001b[1;34m(self, observations, step_type, network_state)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_layers:\n\u001b[0;32m     58\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCustom Layer \u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m Input Shape:\u001b[39m\u001b[39m\"\u001b[39m, state\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 59\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_custom_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlayer\u001b[39m.\u001b[39;49mname\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)(state)\n\u001b[0;32m     60\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCustom Layer \u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m Output Shape:\u001b[39m\u001b[39m\"\u001b[39m, state\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     62\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOwn layers done\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'CriticNetworkCustom' (type CriticNetworkCustom).\n\nInput 0 of layer \"conv1\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (1, 16)\n\nCall arguments received by layer 'CriticNetworkCustom' (type CriticNetworkCustom):\n  • observations=('tf.Tensor(shape=(1, 4), dtype=float32)', 'tf.Tensor(shape=(1, 1), dtype=float32)')\n  • step_type=tf.Tensor(shape=(1,), dtype=int32)\n  • network_state=()\n  In call to configurable 'DdpgAgent' (<class 'tf_agents.agents.ddpg.ddpg_agent.DdpgAgent'>)"
     ]
    }
   ],
   "source": [
    "# Prepare runner\n",
    "\n",
    "# Get or create the global step variable, which is a counter for the number of training steps\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# Create TensorFlow environments for training and evaluation using custom environment settings\n",
    "tf_env_train = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_train))\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_eval))\n",
    "\n",
    "## Define the actor network, responsible for generating actions based on observations\n",
    "actor_net = actornet.ActorNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=actor_layers,\n",
    "    use_ensemble=False,\n",
    ")\n",
    "\n",
    "target_actor_net = actornet.ActorNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=target_actor_layers,\n",
    "    use_ensemble=False,\n",
    ")\n",
    "\n",
    "critic_net = criticnet.CriticNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=critic_layers,\n",
    "    name='CriticNetworkCustom',\n",
    ")\n",
    "\n",
    "# Optionally, you can create a target critic network for stability in DDPG\n",
    "target_critic_net = criticnet.CriticNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=target_critic_layers,\n",
    "    name='TargetCriticNetworkCustom',\n",
    ")\n",
    "\n",
    "\"\"\"# Define the critic network, responsible for estimating the Q-values for state-action pairs\n",
    "critic_net = ddpg.critic_network.CriticNetwork(\n",
    "    input_tensor_spec=(tf_env_train.observation_spec(), tf_env_train.action_spec()),\n",
    "    joint_fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "# Create a DDPG agent using the defined actor and critic networks, along with other parameters\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    tf_env_train.time_step_spec(),\n",
    "    tf_env_train.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    target_actor_network=target_actor_net,\n",
    "    critic_network=critic_net,\n",
    "    target_critic_network=target_critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate),\n",
    "    ou_stddev=ou_stddev, # Standard deviation for Ornstein-Uhlenbeck noise\n",
    "    ou_damping=ou_damping, # Damping term for Ornstein-Uhlenbeck noise\n",
    "    target_update_tau=target_update_tau, # Soft update coefficient for target networks\n",
    "    target_update_period=target_update_period, # Frequency of updating target networks\n",
    "    dqda_clipping=dqda_clipping, # Optional clipping of the gradient of Q-value with respect to actions\n",
    "    td_errors_loss_fn=td_errors_loss_fn, # Loss function for temporal difference errors\n",
    "    gamma=gamma, # Discount factor for future rewards\n",
    "    reward_scale_factor=reward_scale_factor, # Scaling factor for rewards during training\n",
    "    gradient_clipping=gradient_clipping, # Optional clipping of gradients during training\n",
    "    debug_summaries=False, # Disable debug summaries\n",
    "    summarize_grads_and_vars=False,  # Disable summarizing gradients and variables\n",
    "    train_step_counter=global_step,  # Use the global step as the train step counter\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env_train.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps,\n",
    ")\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration,\n",
    ")\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir='checkpoints/ddpg' + str(customer) + '/',\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    logdir='./log/ddpg' + str(customer) + '/', flush_millis=10000\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better performance\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Collect initial replay data\n",
    "initial_collect_driver.run()\n",
    "\n",
    "time_step = tf_env_train.reset()\n",
    "policy_state = collect_policy.get_initial_state(tf_env_train.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1501: Loss = 1.9332886934280396\n"
     ]
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# pipeline which will feed data to the agent\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "with tf.compat.v2.summary.record_if(True):\n",
    "    metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        tf_env_eval,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics')\n",
    "    # Train and evaluate\n",
    "    while global_step.numpy() <= num_iterations:\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "        experience, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience)\n",
    "        print('step = {0}: Loss = {1}'.format(global_step.numpy(), train_loss.loss))\n",
    "        with eval_summary_writer.as_default():\n",
    "            tf.summary.scalar(name='loss', data=train_loss.loss, step=global_step)\n",
    "        if global_step.numpy() % eval_interval == 0:\n",
    "            train_checkpointer.save(global_step)\n",
    "            metric_utils.eager_compute(\n",
    "                eval_metrics,\n",
    "                tf_env_eval,\n",
    "                eval_policy,\n",
    "                num_episodes=num_eval_episodes,\n",
    "                train_step=global_step,\n",
    "                summary_writer=eval_summary_writer,\n",
    "                summary_prefix='Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "data_test = DL.get_customer_data(DL.loadData('../../data/load1213.csv'),\n",
    "                                         DL.loadPrice('../../data/price.csv'), customer)\n",
    "tf_env_test = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_test, test=True))\n",
    "time_step_test = tf_env_test.reset()\n",
    "\n",
    "while not time_step_test.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step_test)\n",
    "    time_step_test = tf_env_test.step(action_step.action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
