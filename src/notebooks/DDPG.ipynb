{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient \n",
    "\n",
    "This notebook implements the DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonas-sievers\u001b[0m (\u001b[33mipe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\wandb\\run-20231211_171605-qzbwikl9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ipe/DDPG/runs/qzbwikl9' target=\"_blank\">001</a></strong> to <a href='https://wandb.ai/ipe/DDPG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ipe/DDPG' target=\"_blank\">https://wandb.ai/ipe/DDPG</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ipe/DDPG/runs/qzbwikl9' target=\"_blank\">https://wandb.ai/ipe/DDPG/runs/qzbwikl9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import ddpg\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import utils.dataloader as DL\n",
    "from utils.modelgenerator import *\n",
    "import utils.actorNetworkCustom as actornet\n",
    "import utils.criticNetworkCustom as criticnet\n",
    "import Environment\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"DDPG\",name=\"001\")\n",
    "wandb_callback = WandbCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specifies the number of training iterations. It determines how many times the agent will go through the entire training process, \n",
    "adjusting its policy and value functions based on collected experiences. A sufficient number of iterations is crucial \n",
    "to allow the agent to learn and adapt to the environment, improving its performance over time.\n",
    "\"\"\"\n",
    "num_iterations = 1500\n",
    "customer = 1\n",
    "\n",
    "# Params for collect\n",
    "\"\"\"\n",
    "Defines the number of initial steps where the agent collects experiences randomly before the training begins. \n",
    "This helps to populate the replay buffer with diverse initial data.\n",
    "A well-populated replay buffer provides a diverse set of experiences for the agent to learn from, \n",
    "enhancing the stability and effectiveness of training.\n",
    "\"\"\"\n",
    "initial_collect_steps = 1000\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of steps the agent takes to collect experiences in each training iteration. \n",
    "It controls the balance between exploration and exploitation during training.\n",
    "Adequate exploration is necessary for discovering optimal policies. \n",
    "Adjusting this parameter impacts how often the agent explores its environment and updates its knowledge.\n",
    "\"\"\"\n",
    "collect_steps_per_iteration = 2000\n",
    "\n",
    "\"\"\"\n",
    "Sets the capacity of the replay buffer, a memory structure storing past experiences for the agent to sample during training.\n",
    "A sufficiently large replay buffer allows the agent to store and learn from a diverse set of experiences, \n",
    "mitigating issues related to correlated data and improving sample efficiency.\n",
    "\"\"\"\n",
    "replay_buffer_capacity = 1000000\n",
    "\n",
    "\"\"\"\n",
    "Determines the standard deviation of the Ornstein-Uhlenbeck process, which introduces exploration noise in the action space.\n",
    "Exploration noise aids the agent in exploring its action space,\n",
    "preventing it from getting stuck in local optima and promoting more robust learning.\n",
    "\"\"\"\n",
    "ou_stddev = 0.2\n",
    "\n",
    "\"\"\"\n",
    "Introduces a damping term to the Ornstein-Uhlenbeck process, influencing the exploration noise.\n",
    "Damping helps control the intensity of exploration noise, \n",
    "allowing a balance between exploration and exploitation based on the task's requirements.\n",
    "\"\"\"\n",
    "ou_damping = 0.15\n",
    "\n",
    "# Params for target update\n",
    "\"\"\"\n",
    "Represents the soft update coefficient for updating target networks, \n",
    "determining the degree to which the target networks track the main networks.\n",
    "Soft updates help stabilize training by slowly blending target values, \n",
    "preventing abrupt changes and improving the convergence of the learning process.\n",
    "\"\"\"\n",
    "target_update_tau = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Defines how often the target networks are updated in terms of training steps.\n",
    "Controlling the update frequency balances stability and responsiveness, \n",
    "preventing the target networks from lagging too far behind or updating too frequently.\n",
    "\"\"\"\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for train\n",
    "\"\"\"\n",
    "Specifies the number of gradient descent steps taken on the training batch in each training iteration.\n",
    "Adjusting this parameter impacts the convergence speed of the training process, \n",
    "influencing how much the agent learns from each collected batch of experiences.\n",
    "\"\"\"\n",
    "train_steps_per_iteration = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the size of the training batch sampled from the replay buffer.\n",
    "The batch size affects the efficiency of training; \n",
    "a well-chosen size balances computational efficiency and the stability of the learning process.\n",
    "\"\"\"\n",
    "batch_size = 48 * 7\n",
    "\n",
    "\"\"\"\n",
    "Specifies the learning rate for the actor (policy) network during gradient descent.\n",
    "The learning rate controls the size of the step taken during optimization. \n",
    "A suitable learning rate ensures the model converges effectively without overshooting or getting stuck in local minima.\n",
    "\"\"\"\n",
    "actor_learning_rate = 1e-4\n",
    "\n",
    "\"\"\"\n",
    "Defines the learning rate for the critic (Q-value) network during gradient descent.\n",
    "Similar to the actor learning rate, an appropriate critic learning rate influences \n",
    "the convergence and stability of the critic network, which plays a crucial role in estimating Q-values.\n",
    "\"\"\"\n",
    "critic_learning_rate = 1e-3\n",
    "\n",
    "\"\"\"\n",
    "An optional parameter for clipping the gradient of the Q-value with respect to actions.\n",
    "Clipping gradients can prevent large updates that may destabilize training, \n",
    "acting as a form of regularization and improving the robustness of the learning process.\n",
    "\"\"\"\n",
    "dqda_clipping = None\n",
    "\n",
    "\"\"\"\n",
    "Specifies the loss function for temporal difference (TD) errors, \n",
    "representing the discrepancy between predicted and actual Q-values.\n",
    "The choice of loss function influences how the agent updates its value estimates. \n",
    "Huber loss, as specified here, is robust to outliers and provides a balance between mean squared error and mean absolute error.\n",
    "\"\"\"\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "\n",
    "\"\"\"\n",
    "Represents the discount factor applied to future rewards in the Q-value estimation.\n",
    "Discounting future rewards emphasizes the importance of immediate rewards, e\n",
    "nabling the agent to make more informed decisions that consider both short-term and long-term consequences.\n",
    "\"\"\"\n",
    "gamma = 0.99\n",
    "\n",
    "\"\"\"\n",
    "Scales the rewards during training.Scaling rewards helps to control the impact of reward magnitudes on the learning process, \n",
    "preventing issues related to overly large or small rewards.\n",
    "\"\"\"\n",
    "reward_scale_factor = 1.0\n",
    "\n",
    "\"\"\"An optional parameter for clipping gradients during training.\"\"\"\n",
    "gradient_clipping = None\n",
    "\n",
    "# Params for eval and checkpoints\n",
    "\"\"\"\n",
    "Specifies the number of episodes used for evaluating the agent's performance.\n",
    "Evaluating the agent's performance provides insights into its generalization \n",
    "capabilities and allows for monitoring progress over time.\"\"\"\n",
    "num_eval_episodes = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the frequency (in iterations) at which evaluations are performed.\n",
    "Regular evaluations help track the agent's progress, enabling the identification of potential issues and providing \n",
    "a basis for comparison between different training iterations.\n",
    "\"\"\"\n",
    "eval_interval = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = DL.get_customer_data(DL.loadData('../../data/load1011.csv'), DL.loadPrice('../../data/price.csv'), customer)\n",
    "data_eval = DL.get_customer_data(DL.loadData('../../data/load1112.csv'), DL.loadPrice('../../data/price.csv'), customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0:30</th>\n",
       "      <th>1:00</th>\n",
       "      <th>1:30</th>\n",
       "      <th>2:00</th>\n",
       "      <th>2:30</th>\n",
       "      <th>3:00</th>\n",
       "      <th>3:30</th>\n",
       "      <th>4:00</th>\n",
       "      <th>4:30</th>\n",
       "      <th>5:00</th>\n",
       "      <th>...</th>\n",
       "      <th>19:30</th>\n",
       "      <th>20:00</th>\n",
       "      <th>20:30</th>\n",
       "      <th>21:00</th>\n",
       "      <th>21:30</th>\n",
       "      <th>22:00</th>\n",
       "      <th>22:30</th>\n",
       "      <th>23:00</th>\n",
       "      <th>23:30</th>\n",
       "      <th>0:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0:30  1:00  1:30  2:00  2:30  3:00  3:30  4:00  4:30  5:00  ...  19:30  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   20:00  20:30  21:00  21:30  22:00  22:30  23:00  23:30  0:00  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = ModelGenerator()\n",
    "# get_dense_layers(), get_bilstm_model(), get_lstm_model(), get_cnn_lstm_model(), get_cnn_model()\n",
    "custom_layers = m1.get_cnn_model()\n",
    "\n",
    "# Create a copy of the layers, so no weights are duplicated\n",
    "def copy_layer(layer):\n",
    "    return tf.keras.layers.deserialize({'class_name': layer.__class__.__name__, 'config': layer.get_config()})\n",
    "\n",
    "# Create copies of the original layers\n",
    "actor_layers = [copy_layer(layer) for layer in custom_layers]\n",
    "target_actor_layers = [copy_layer(layer) for layer in custom_layers]\n",
    "critic_layers = [copy_layer(layer) for layer in custom_layers]\n",
    "target_critic_layers = [copy_layer(layer) for layer in custom_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare runner\n",
    "# Get or create the global step variable, which is a counter for the number of training steps\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "# Create TensorFlow environments for training and evaluation using custom environment settings\n",
    "tf_env_train = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_train))\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_eval))\n",
    "\n",
    "## Define the actor network, responsible for generating actions based on observations\n",
    "actor_net = actornet.ActorNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=actor_layers,\n",
    "    use_ensemble=False,\n",
    ")\n",
    "\n",
    "target_actor_net = actornet.ActorNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=target_actor_layers,\n",
    "    use_ensemble=False,\n",
    ")\n",
    "\n",
    "critic_net = criticnet.CriticNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=critic_layers,\n",
    "    name='CriticNetworkCustom',\n",
    ")\n",
    "\n",
    "# Optionally, you can create a target critic network for stability in DDPG\n",
    "target_critic_net = criticnet.CriticNetworkCustom(\n",
    "    observation_spec=tf_env_train.observation_spec(),\n",
    "    action_spec=tf_env_train.action_spec(),\n",
    "    custom_layers=target_critic_layers,\n",
    "    name='TargetCriticNetworkCustom',\n",
    ")\n",
    "\n",
    "# Create a DDPG agent using the defined actor and critic networks, along with other parameters\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    tf_env_train.time_step_spec(),\n",
    "    tf_env_train.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    target_actor_network=target_actor_net,\n",
    "    critic_network=critic_net,\n",
    "    target_critic_network=target_critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate),\n",
    "    ou_stddev=ou_stddev, # Standard deviation for Ornstein-Uhlenbeck noise\n",
    "    ou_damping=ou_damping, # Damping term for Ornstein-Uhlenbeck noise\n",
    "    target_update_tau=target_update_tau, # Soft update coefficient for target networks\n",
    "    target_update_period=target_update_period, # Frequency of updating target networks\n",
    "    dqda_clipping=dqda_clipping, # Optional clipping of the gradient of Q-value with respect to actions\n",
    "    td_errors_loss_fn=td_errors_loss_fn, # Loss function for temporal difference errors\n",
    "    gamma=gamma, # Discount factor for future rewards\n",
    "    reward_scale_factor=reward_scale_factor, # Scaling factor for rewards during training\n",
    "    gradient_clipping=gradient_clipping, # Optional clipping of gradients during training\n",
    "    debug_summaries=False, # Disable debug summaries\n",
    "    summarize_grads_and_vars=False,  # Disable summarizing gradients and variables\n",
    "    train_step_counter=global_step,  # Use the global step as the train step counter\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env_train.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps,\n",
    ")\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration,\n",
    ")\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir='checkpoints/ddpg' + str(customer) + '/',\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    logdir='./log/ddpg' + str(customer) + '/', flush_millis=10000\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better performance\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Collect initial replay data\n",
    "initial_collect_driver.run()\n",
    "\n",
    "time_step = tf_env_train.reset()\n",
    "policy_state = collect_policy.get_initial_state(tf_env_train.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step:  <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=3000>\n",
      "Global step:  <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=0>\n"
     ]
    }
   ],
   "source": [
    "print(\"Global step: \", global_step)\n",
    "tf.compat.v1.assign(global_step, 0)\n",
    "print(\"Global step: \", global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: Loss = 11.055927276611328\n",
      "step = 2: Loss = 10.21143913269043\n",
      "step = 3: Loss = 10.956745147705078\n",
      "step = 4: Loss = 11.82320785522461\n",
      "step = 5: Loss = 10.700417518615723\n",
      "step = 6: Loss = 10.491310119628906\n",
      "step = 7: Loss = 10.4989013671875\n",
      "step = 8: Loss = 10.785268783569336\n",
      "step = 9: Loss = 11.35443115234375\n",
      "step = 10: Loss = 11.226554870605469\n",
      "step = 11: Loss = 11.742948532104492\n",
      "step = 12: Loss = 11.047452926635742\n",
      "step = 13: Loss = 11.610843658447266\n",
      "step = 14: Loss = 12.078954696655273\n",
      "step = 15: Loss = 11.28012752532959\n",
      "step = 16: Loss = 10.256559371948242\n",
      "step = 17: Loss = 11.53210163116455\n",
      "step = 18: Loss = 10.737626075744629\n",
      "step = 19: Loss = 10.735795021057129\n",
      "step = 20: Loss = 10.240280151367188\n",
      "step = 21: Loss = 10.439294815063477\n",
      "step = 22: Loss = 11.231163024902344\n",
      "step = 23: Loss = 11.667989730834961\n",
      "step = 24: Loss = 11.08132553100586\n",
      "step = 25: Loss = 10.328304290771484\n",
      "step = 26: Loss = 11.4136381149292\n",
      "step = 27: Loss = 10.281063079833984\n",
      "step = 28: Loss = 10.943549156188965\n",
      "step = 29: Loss = 10.607429504394531\n",
      "step = 30: Loss = 11.391555786132812\n",
      "step = 31: Loss = 10.917051315307617\n",
      "step = 32: Loss = 9.931488037109375\n",
      "step = 33: Loss = 10.258880615234375\n",
      "step = 34: Loss = 11.43157958984375\n",
      "step = 35: Loss = 11.767374038696289\n",
      "step = 36: Loss = 11.905436515808105\n",
      "step = 37: Loss = 10.527164459228516\n",
      "step = 38: Loss = 11.472888946533203\n",
      "step = 39: Loss = 11.264871597290039\n",
      "step = 40: Loss = 11.408323287963867\n",
      "step = 41: Loss = 12.022088050842285\n",
      "step = 42: Loss = 10.317440032958984\n",
      "step = 43: Loss = 10.5029296875\n",
      "step = 44: Loss = 11.650562286376953\n",
      "step = 45: Loss = 11.8184232711792\n",
      "step = 46: Loss = 10.97876262664795\n",
      "step = 47: Loss = 11.15268325805664\n",
      "step = 48: Loss = 11.650398254394531\n",
      "step = 49: Loss = 10.504334449768066\n",
      "step = 50: Loss = 10.51056957244873\n",
      "step = 51: Loss = 10.894144058227539\n",
      "step = 52: Loss = 10.647126197814941\n",
      "step = 53: Loss = 10.616222381591797\n",
      "step = 54: Loss = 10.777633666992188\n",
      "step = 55: Loss = 11.476690292358398\n",
      "step = 56: Loss = 11.263799667358398\n",
      "step = 57: Loss = 11.46207046508789\n",
      "step = 58: Loss = 10.867361068725586\n",
      "step = 59: Loss = 11.155937194824219\n",
      "step = 60: Loss = 12.06303882598877\n",
      "step = 61: Loss = 9.95555591583252\n",
      "step = 62: Loss = 10.641650199890137\n",
      "step = 63: Loss = 11.159679412841797\n",
      "step = 64: Loss = 10.363306999206543\n",
      "step = 65: Loss = 11.880701065063477\n",
      "step = 66: Loss = 10.464893341064453\n",
      "step = 67: Loss = 10.674501419067383\n",
      "step = 68: Loss = 10.519171714782715\n",
      "step = 69: Loss = 11.634166717529297\n",
      "step = 70: Loss = 10.959829330444336\n",
      "step = 71: Loss = 11.24056625366211\n",
      "step = 72: Loss = 11.40103530883789\n",
      "step = 73: Loss = 10.4912691116333\n",
      "step = 74: Loss = 10.061141967773438\n",
      "step = 75: Loss = 9.71780014038086\n",
      "step = 76: Loss = 10.37464714050293\n",
      "step = 77: Loss = 10.666014671325684\n",
      "step = 78: Loss = 11.15265941619873\n",
      "step = 79: Loss = 10.957474708557129\n",
      "step = 80: Loss = 9.897207260131836\n",
      "step = 81: Loss = 11.133544921875\n",
      "step = 82: Loss = 10.13235092163086\n",
      "step = 83: Loss = 11.408010482788086\n",
      "step = 84: Loss = 11.140512466430664\n",
      "step = 85: Loss = 11.656383514404297\n",
      "step = 86: Loss = 11.180038452148438\n",
      "step = 87: Loss = 10.422937393188477\n",
      "step = 88: Loss = 11.232017517089844\n",
      "step = 89: Loss = 11.889007568359375\n",
      "step = 90: Loss = 12.085431098937988\n",
      "step = 91: Loss = 10.405478477478027\n",
      "step = 92: Loss = 10.7200288772583\n",
      "step = 93: Loss = 10.454721450805664\n",
      "step = 94: Loss = 10.629839897155762\n",
      "step = 95: Loss = 10.98133373260498\n",
      "step = 96: Loss = 10.770671844482422\n",
      "step = 97: Loss = 10.28950023651123\n",
      "step = 98: Loss = 11.38505744934082\n",
      "step = 99: Loss = 9.985097885131836\n",
      "step = 100: Loss = 10.215940475463867\n",
      "step = 101: Loss = 12.334582328796387\n",
      "step = 102: Loss = 10.814666748046875\n",
      "step = 103: Loss = 11.850308418273926\n",
      "step = 104: Loss = 10.10850715637207\n",
      "step = 105: Loss = 11.731782913208008\n",
      "step = 106: Loss = 11.495696067810059\n",
      "step = 107: Loss = 11.802736282348633\n",
      "step = 108: Loss = 11.208606719970703\n",
      "step = 109: Loss = 9.697154998779297\n",
      "step = 110: Loss = 10.79399299621582\n",
      "step = 111: Loss = 10.838468551635742\n",
      "step = 112: Loss = 10.737364768981934\n",
      "step = 113: Loss = 10.580617904663086\n",
      "step = 114: Loss = 11.914533615112305\n",
      "step = 115: Loss = 11.169431686401367\n",
      "step = 116: Loss = 11.22854995727539\n",
      "step = 117: Loss = 10.723381042480469\n",
      "step = 118: Loss = 10.25619888305664\n",
      "step = 119: Loss = 11.055575370788574\n",
      "step = 120: Loss = 9.770988464355469\n",
      "step = 121: Loss = 10.535175323486328\n",
      "step = 122: Loss = 10.423870086669922\n",
      "step = 123: Loss = 11.030191421508789\n",
      "step = 124: Loss = 10.471458435058594\n",
      "step = 125: Loss = 11.082298278808594\n",
      "step = 126: Loss = 10.275995254516602\n",
      "step = 127: Loss = 10.451333999633789\n",
      "step = 128: Loss = 10.678267478942871\n",
      "step = 129: Loss = 10.00906753540039\n",
      "step = 130: Loss = 10.503510475158691\n",
      "step = 131: Loss = 10.871950149536133\n",
      "step = 132: Loss = 10.591232299804688\n",
      "step = 133: Loss = 9.546426773071289\n",
      "step = 134: Loss = 10.981955528259277\n",
      "step = 135: Loss = 10.887271881103516\n",
      "step = 136: Loss = 10.34420108795166\n",
      "step = 137: Loss = 9.70171070098877\n",
      "step = 138: Loss = 10.091178894042969\n",
      "step = 139: Loss = 9.999171257019043\n",
      "step = 140: Loss = 11.141878128051758\n",
      "step = 141: Loss = 11.295665740966797\n",
      "step = 142: Loss = 10.812033653259277\n",
      "step = 143: Loss = 11.242633819580078\n",
      "step = 144: Loss = 10.585151672363281\n",
      "step = 145: Loss = 11.281990051269531\n",
      "step = 146: Loss = 11.126405715942383\n",
      "step = 147: Loss = 10.1392822265625\n",
      "step = 148: Loss = 9.576473236083984\n",
      "step = 149: Loss = 10.352584838867188\n",
      "step = 150: Loss = 10.896763801574707\n"
     ]
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# pipeline which will feed data to the agent\n",
    "dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "#with tf.compat.v2.summary.record_if(True):\n",
    "    \n",
    "metric_utils.eager_compute(\n",
    "    eval_metrics,\n",
    "    tf_env_eval,\n",
    "    eval_policy,\n",
    "    num_episodes=num_eval_episodes, \n",
    "    train_step=global_step, \n",
    "    summary_writer=eval_summary_writer,\n",
    "    summary_prefix='Metrics'\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "while global_step.numpy() <= num_iterations:\n",
    "        \n",
    "    time_step, policy_state = collect_driver.run(time_step=time_step,policy_state=policy_state,)\n",
    "    \n",
    "    experience, _ = next(iterator)\n",
    "    \n",
    "    train_loss = tf_agent.train(experience)\n",
    "    # Log metrics using WandbCallback\n",
    "    logs = {'train_loss': train_loss.loss.numpy()}  # Add more metrics as needed\n",
    "    wandb_callback.on_epoch_end(global_step.numpy(), logs)\n",
    "    \n",
    "    print('step = {0}: Loss = {1}'.format(global_step.numpy(), train_loss.loss))\n",
    "    \n",
    "    with eval_summary_writer.as_default():\n",
    "        tf.summary.scalar(name='loss', data=train_loss.loss, step=global_step)\n",
    "    if global_step.numpy() % eval_interval == 0:\n",
    "        train_checkpointer.save(global_step)\n",
    "\n",
    "        metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            tf_env_eval,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\save\\critic_weights)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▇██▇▅▅▃▃▄▃▃▃▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3001</td></tr><tr><td>train_loss</td><td>8.78975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">001</strong> at: <a href='https://wandb.ai/ipe/DDPG/runs/rha7egi3' target=\"_blank\">https://wandb.ai/ipe/DDPG/runs/rha7egi3</a><br/>Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231211_165700-rha7egi3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save actor and critic weights to WandB\n",
    "# Get the weights\n",
    "critic_weights = tf_agent._critic_network.get_weights()\n",
    "target_critic_weights = tf_agent._target_critic_network.get_weights()\n",
    "actor_weights = tf_agent._actor_network.get_weights()\n",
    "target_actor_weights = tf_agent._target_actor_network.get_weights()\n",
    "\n",
    "# Create a directory to save individual weights\n",
    "weights_directory = './save/critic_weights'\n",
    "os.makedirs(weights_directory, exist_ok=True)\n",
    "\n",
    "# Save each weight individually\n",
    "for i, weight in enumerate(critic_weights):\n",
    "    weight_path = os.path.join(weights_directory, f'critic_weight_{i}.npy')\n",
    "    np.save(weight_path, weight)\n",
    "\n",
    "for i, weight in enumerate(target_critic_weights):\n",
    "    weight_path = os.path.join(weights_directory, f'target_critic_weight_{i}.npy')\n",
    "    np.save(weight_path, weight)\n",
    "\n",
    "for i, weight in enumerate(actor_weights):\n",
    "    weight_path = os.path.join(weights_directory, f'actor_weight_{i}.npy')\n",
    "    np.save(weight_path, weight)\n",
    "\n",
    "for i, weight in enumerate(target_actor_weights):\n",
    "    weight_path = os.path.join(weights_directory, f'target_actor_weight_{i}.npy')\n",
    "    np.save(weight_path, weight)\n",
    "\n",
    "# Log all weights as an artifact in WandB\n",
    "weights_artifact = wandb.Artifact('all_weights', type='weights')\n",
    "weights_artifact.add_dir(weights_directory)\n",
    "wandb.run.log_artifact(weights_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_weights = [np.load(os.path.join(weights_directory, f'weight_{i}.npy'), allow_pickle=True) for i in range(len(weights))]\n",
    "# Set the loaded weights to the model\n",
    "#tf_agent._critic_network.set_weights(loaded_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "data_test = DL.get_customer_data(DL.loadData('../../data/load1213.csv'),\n",
    "                                         DL.loadPrice('../../data/price.csv'), customer)\n",
    "tf_env_test = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_test, test=True))\n",
    "time_step_test = tf_env_test.reset()\n",
    "\n",
    "while not time_step_test.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step_test)\n",
    "    time_step_test = tf_env_test.step(action_step.action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
