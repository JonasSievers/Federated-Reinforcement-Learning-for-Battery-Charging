{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient \n",
    "\n",
    "This notebook implements the DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import ddpg\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import utils.Dataloader as DL\n",
    "import utils.agentnetworks as agentnets\n",
    "import Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of training iterations. It determines how many times the agent will go through the entire training process, \n",
    "adjusting its policy and value functions based on collected experiences. A sufficient number of iterations is crucial \n",
    "to allow the agent to learn and adapt to the environment, improving its performance over time.\n",
    "\"\"\"\n",
    "num_iterations = 1500\n",
    "customer = 1\n",
    "\n",
    "# Params for collect\n",
    "\"\"\"\n",
    "Defines the number of initial steps where the agent collects experiences randomly before the training begins. \n",
    "This helps to populate the replay buffer with diverse initial data.\n",
    "A well-populated replay buffer provides a diverse set of experiences for the agent to learn from, \n",
    "enhancing the stability and effectiveness of training.\n",
    "\"\"\"\n",
    "initial_collect_steps = 1000\n",
    "\n",
    "\"\"\"\n",
    "Specifies the number of steps the agent takes to collect experiences in each training iteration. \n",
    "It controls the balance between exploration and exploitation during training.\n",
    "Adequate exploration is necessary for discovering optimal policies. \n",
    "Adjusting this parameter impacts how often the agent explores its environment and updates its knowledge.\n",
    "\"\"\"\n",
    "collect_steps_per_iteration = 2000\n",
    "\n",
    "\"\"\"\n",
    "Sets the capacity of the replay buffer, a memory structure storing past experiences for the agent to sample during training.\n",
    "A sufficiently large replay buffer allows the agent to store and learn from a diverse set of experiences, \n",
    "mitigating issues related to correlated data and improving sample efficiency.\n",
    "\"\"\"\n",
    "replay_buffer_capacity = 1000000\n",
    "\n",
    "\"\"\"\n",
    "Determines the standard deviation of the Ornstein-Uhlenbeck process, which introduces exploration noise in the action space.\n",
    "Exploration noise aids the agent in exploring its action space,\n",
    "preventing it from getting stuck in local optima and promoting more robust learning.\n",
    "\"\"\"\n",
    "ou_stddev = 0.2\n",
    "\n",
    "\"\"\"\n",
    "Introduces a damping term to the Ornstein-Uhlenbeck process, influencing the exploration noise.\n",
    "Damping helps control the intensity of exploration noise, \n",
    "allowing a balance between exploration and exploitation based on the task's requirements.\n",
    "\"\"\"\n",
    "ou_damping = 0.15\n",
    "\n",
    "# Params for target update\n",
    "\"\"\"\n",
    "Represents the soft update coefficient for updating target networks, \n",
    "determining the degree to which the target networks track the main networks.\n",
    "Soft updates help stabilize training by slowly blending target values, \n",
    "preventing abrupt changes and improving the convergence of the learning process.\n",
    "\"\"\"\n",
    "target_update_tau = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Defines how often the target networks are updated in terms of training steps.\n",
    "Controlling the update frequency balances stability and responsiveness, \n",
    "preventing the target networks from lagging too far behind or updating too frequently.\n",
    "\"\"\"\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for train\n",
    "\"\"\"\n",
    "Specifies the number of gradient descent steps taken on the training batch in each training iteration.\n",
    "Adjusting this parameter impacts the convergence speed of the training process, \n",
    "influencing how much the agent learns from each collected batch of experiences.\n",
    "\"\"\"\n",
    "train_steps_per_iteration = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the size of the training batch sampled from the replay buffer.\n",
    "The batch size affects the efficiency of training; \n",
    "a well-chosen size balances computational efficiency and the stability of the learning process.\n",
    "\"\"\"\n",
    "batch_size = 48 * 7\n",
    "\n",
    "\"\"\"\n",
    "Specifies the learning rate for the actor (policy) network during gradient descent.\n",
    "The learning rate controls the size of the step taken during optimization. \n",
    "A suitable learning rate ensures the model converges effectively without overshooting or getting stuck in local minima.\n",
    "\"\"\"\n",
    "actor_learning_rate = 1e-4\n",
    "\n",
    "\"\"\"\n",
    "Defines the learning rate for the critic (Q-value) network during gradient descent.\n",
    "Similar to the actor learning rate, an appropriate critic learning rate influences \n",
    "the convergence and stability of the critic network, which plays a crucial role in estimating Q-values.\n",
    "\"\"\"\n",
    "critic_learning_rate = 1e-3\n",
    "\n",
    "\"\"\"\n",
    "An optional parameter for clipping the gradient of the Q-value with respect to actions.\n",
    "Clipping gradients can prevent large updates that may destabilize training, \n",
    "acting as a form of regularization and improving the robustness of the learning process.\n",
    "\"\"\"\n",
    "dqda_clipping = None\n",
    "\n",
    "\"\"\"\n",
    "Specifies the loss function for temporal difference (TD) errors, \n",
    "representing the discrepancy between predicted and actual Q-values.\n",
    "The choice of loss function influences how the agent updates its value estimates. \n",
    "Huber loss, as specified here, is robust to outliers and provides a balance between mean squared error and mean absolute error.\n",
    "\"\"\"\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "\n",
    "\"\"\"\n",
    "Represents the discount factor applied to future rewards in the Q-value estimation.\n",
    "Discounting future rewards emphasizes the importance of immediate rewards, e\n",
    "nabling the agent to make more informed decisions that consider both short-term and long-term consequences.\n",
    "\"\"\"\n",
    "gamma = 0.99\n",
    "\n",
    "\"\"\"\n",
    "Scales the rewards during training.Scaling rewards helps to control the impact of reward magnitudes on the learning process, \n",
    "preventing issues related to overly large or small rewards.\n",
    "\"\"\"\n",
    "reward_scale_factor = 1.0\n",
    "\n",
    "\"\"\"An optional parameter for clipping gradients during training.\"\"\"\n",
    "gradient_clipping = None\n",
    "\n",
    "# Params for eval and checkpoints\n",
    "\"\"\"\n",
    "Specifies the number of episodes used for evaluating the agent's performance.\n",
    "Evaluating the agent's performance provides insights into its generalization \n",
    "capabilities and allows for monitoring progress over time.\"\"\"\n",
    "num_eval_episodes = 1\n",
    "\n",
    "\"\"\"\n",
    "Sets the frequency (in iterations) at which evaluations are performed.\n",
    "Regular evaluations help track the agent's progress, enabling the identification of potential issues and providing \n",
    "a basis for comparison between different training iterations.\n",
    "\"\"\"\n",
    "eval_interval = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = DL.get_customer_data(DL.loadData('../../data/load1011.csv'), DL.loadPrice('../../data/price.csv'), customer)\n",
    "data_eval = DL.get_customer_data(DL.loadData('../../data/load1112.csv'), DL.loadPrice('../../data/price.csv'), customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0:30</th>\n",
       "      <th>1:00</th>\n",
       "      <th>1:30</th>\n",
       "      <th>2:00</th>\n",
       "      <th>2:30</th>\n",
       "      <th>3:00</th>\n",
       "      <th>3:30</th>\n",
       "      <th>4:00</th>\n",
       "      <th>4:30</th>\n",
       "      <th>5:00</th>\n",
       "      <th>...</th>\n",
       "      <th>19:30</th>\n",
       "      <th>20:00</th>\n",
       "      <th>20:30</th>\n",
       "      <th>21:00</th>\n",
       "      <th>21:30</th>\n",
       "      <th>22:00</th>\n",
       "      <th>22:30</th>\n",
       "      <th>23:00</th>\n",
       "      <th>23:30</th>\n",
       "      <th>0:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0:30  1:00  1:30  2:00  2:30  3:00  3:30  4:00  4:30  5:00  ...  19:30  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "360   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "361   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "362   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "363   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "364   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "     20:00  20:30  21:00  21:30  22:00  22:30  23:00  23:30  0:00  \n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...   ...  \n",
       "360    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "361    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "362    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "363    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "364    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   0.0  \n",
       "\n",
       "[365 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare runner\n",
    "\n",
    "# Get or create the global step variable, which is a counter for the number of training steps\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# Create TensorFlow environments for training and evaluation using custom environment settings\n",
    "tf_env_train = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_train))\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_eval))\n",
    "\n",
    "## Define the actor network, responsible for generating actions based on observations\n",
    "\"\"\"actor_net = ddpg.actor_network.ActorNetwork(\n",
    "    input_tensor_spec=tf_env_train.observation_spec(),\n",
    "    output_tensor_spec=tf_env_train.action_spec(), \n",
    "    fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\"\"\"\n",
    "\n",
    "actor_net = agentnets.ActorNetworkCustom(tf_env_train.observation_spec(),\n",
    "                                    tf_env_train.action_spec())\n",
    "\n",
    "# Define the critic network, responsible for estimating the Q-values for state-action pairs\n",
    "critic_net = ddpg.critic_network.CriticNetwork(\n",
    "    input_tensor_spec=(tf_env_train.observation_spec(), tf_env_train.action_spec()),\n",
    "    joint_fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\n",
    "\n",
    "# Create a DDPG agent using the defined actor and critic networks, along with other parameters\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    tf_env_train.time_step_spec(),\n",
    "    tf_env_train.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate),\n",
    "    ou_stddev=ou_stddev, # Standard deviation for Ornstein-Uhlenbeck noise\n",
    "    ou_damping=ou_damping, # Damping term for Ornstein-Uhlenbeck noise\n",
    "    target_update_tau=target_update_tau, # Soft update coefficient for target networks\n",
    "    target_update_period=target_update_period, # Frequency of updating target networks\n",
    "    dqda_clipping=dqda_clipping, # Optional clipping of the gradient of Q-value with respect to actions\n",
    "    td_errors_loss_fn=td_errors_loss_fn, # Loss function for temporal difference errors\n",
    "    gamma=gamma, # Discount factor for future rewards\n",
    "    reward_scale_factor=reward_scale_factor, # Scaling factor for rewards during training\n",
    "    gradient_clipping=gradient_clipping, # Optional clipping of gradients during training\n",
    "    debug_summaries=False, # Disable debug summaries\n",
    "    summarize_grads_and_vars=False,  # Disable summarizing gradients and variables\n",
    "    train_step_counter=global_step,  # Use the global step as the train step counter\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env_train.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps,\n",
    ")\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration,\n",
    ")\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir='checkpoints/ddpg' + str(customer) + '/',\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    logdir='./log/ddpg' + str(customer) + '/', flush_millis=10000\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better performance\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    }
   ],
   "source": [
    "# Collect initial replay data\n",
    "initial_collect_driver.run()\n",
    "\n",
    "time_step = tf_env_train.reset()\n",
    "policy_state = collect_policy.get_initial_state(tf_env_train.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1501: Loss = 33.2111930847168\n"
     ]
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# pipeline which will feed data to the agent\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "\n",
    "with tf.compat.v2.summary.record_if(True):\n",
    "    metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        tf_env_eval,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics')\n",
    "\n",
    "    # Train and evaluate\n",
    "    while global_step.numpy() <= num_iterations:\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "        experience, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience)\n",
    "        print('step = {0}: Loss = {1}'.format(global_step.numpy(), train_loss.loss))\n",
    "        with eval_summary_writer.as_default():\n",
    "            tf.summary.scalar(name='loss', data=train_loss.loss, step=global_step)\n",
    "        if global_step.numpy() % eval_interval == 0:\n",
    "            train_checkpointer.save(global_step)\n",
    "            metric_utils.eager_compute(\n",
    "                eval_metrics,\n",
    "                tf_env_eval,\n",
    "                eval_policy,\n",
    "                num_episodes=num_eval_episodes,\n",
    "                train_step=global_step,\n",
    "                summary_writer=eval_summary_writer,\n",
    "                summary_prefix='Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  load = self._load_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pv = self._pv_data.iloc[self._current_day][self._current_timeslot]\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self._electricity_prices.iloc[(self._current_day * self._max_timeslots) + self._current_timeslot][0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4256\\1998155483.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtf_env_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_py_environment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFPyEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_charge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtime_step_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_env_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtime_step_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtime_step_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_env_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    329\u001b[0m         )\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoundedTensorSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mcheck_tf1_allowed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhas_eager_been_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m \u001b[0mside\u001b[0m \u001b[0minformation\u001b[0m \u001b[0msuch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0maction\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \"\"\"\n\u001b[0;32m    588\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tf_agents_tf_policy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[0mdistribution_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m     actions = tf.nest.map_structure(\n\u001b[0m\u001b[0;32m    591\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mdistribution_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mno\u001b[0m \u001b[0mstructure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mprovided\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstructures\u001b[0m \u001b[0mdo\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m       \u001b[0meach\u001b[0m \u001b[0mother\u001b[0m \u001b[0mby\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m   \"\"\"\n\u001b[1;32m--> 631\u001b[1;33m   return nest_util.map_structure(\n\u001b[0m\u001b[0;32m    632\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m   )\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0meach\u001b[0m \u001b[0mother\u001b[0m \u001b[0mby\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m   \"\"\"\n\u001b[0;32m   1065\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1067\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[0;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m       \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m   )\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"func must be callable, got: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(d)\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m         \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\distributions\\reparameterized_sampling.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(distribution, reparam, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgumbel_softmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGumbelSoftmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m       \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_one_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[0;32m   1201\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mprepended\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m     \"\"\"\n\u001b[0;32m   1204\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1205\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     sample_shape = ps.convert_to_shape_tensor(\n\u001b[0;32m   1179\u001b[0m         ps.cast(sample_shape, tf.int32), name='sample_shape')\n\u001b[0;32m   1180\u001b[0m     sample_shape, n = self._expand_sample_shape_to_vector(\n\u001b[0;32m   1181\u001b[0m         sample_shape, 'sample_shape')\n\u001b[1;32m-> 1182\u001b[1;33m     samples = self._sample_n(\n\u001b[0m\u001b[0;32m   1183\u001b[0m         n, seed=seed() if callable(seed) else seed, **kwargs)\n\u001b[0;32m   1184\u001b[0m     samples = tf.nest.map_structure(\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\deterministic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    158\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_sample_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mseed\u001b[0m  \u001b[1;31m# unused\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     return tf.broadcast_to(\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         ps.concat([[n], self._batch_shape_tensor(loc=loc),\n\u001b[0;32m    164\u001b[0m                    self._event_shape_tensor(loc=loc)],\n",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, shape, name)\u001b[0m\n\u001b[0;32m    880\u001b[0m         _ctx, \"BroadcastTo\", name, input, shape)\n\u001b[0;32m    881\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       _result = _dispatcher_for_broadcast_to(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test\n",
    "data_test = DL.get_customer_data(DL.loadData('../../data/load1213.csv'),\n",
    "                                         DL.loadPrice('../../data/price.csv'), customer)\n",
    "tf_env_test = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_test, test=True))\n",
    "time_step_test = tf_env_test.reset()\n",
    "\n",
    "while not time_step_test.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step_test)\n",
    "    time_step_test = tf_env_test.step(action_step.action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
