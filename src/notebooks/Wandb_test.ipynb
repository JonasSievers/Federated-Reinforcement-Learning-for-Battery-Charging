{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import ddpg\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import utils.Dataloader as DL\n",
    "import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "# Param for iteration\n",
    "num_iterations = 100\n",
    "customer = 1\n",
    "# Params for collect\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 2000\n",
    "replay_buffer_capacity = 1000000\n",
    "ou_stddev = 0.2\n",
    "ou_damping = 0.15\n",
    "\n",
    "# Params for target update\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for train\n",
    "train_steps_per_iteration = 1\n",
    "batch_size = 1\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "dqda_clipping = None\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "gamma = 0.99\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "\n",
    "# Params for eval and checkpoints\n",
    "num_eval_episodes = 1\n",
    "eval_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = DL.get_customer_data(DL.loadData('../../data/load1011.csv'), DL.loadPrice('../../data/price.csv'), customer)\n",
    "data_eval = DL.get_customer_data(DL.loadData('../../data/load1112.csv'), DL.loadPrice('../../data/price.csv'), customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Agent\n",
    "\n",
    "# Get or create the global step variable, which is a counter for the number of training steps\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# Create TensorFlow environments for training and evaluation using custom environment settings\n",
    "tf_env_train = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_train))\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=0.0, data=data_eval))\n",
    "\n",
    "## Define the actor network, responsible for generating actions based on observations\n",
    "actor_net = ddpg.actor_network.ActorNetwork(\n",
    "    input_tensor_spec=tf_env_train.observation_spec(),\n",
    "    output_tensor_spec=tf_env_train.action_spec(), \n",
    "    fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\n",
    "\n",
    "# Define the critic network, responsible for estimating the Q-values for state-action pairs\n",
    "critic_net = ddpg.critic_network.CriticNetwork(\n",
    "    input_tensor_spec=(tf_env_train.observation_spec(), tf_env_train.action_spec()),\n",
    "    joint_fc_layer_params=(400, 300), # Define the architecture of the fully connected layers\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\n",
    "\n",
    "# Create a DDPG agent using the defined actor and critic networks, along with other parameters\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    tf_env_train.time_step_spec(),\n",
    "    tf_env_train.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate),\n",
    "    ou_stddev=ou_stddev, # Standard deviation for Ornstein-Uhlenbeck noise\n",
    "    ou_damping=ou_damping, # Damping term for Ornstein-Uhlenbeck noise\n",
    "    target_update_tau=target_update_tau, # Soft update coefficient for target networks\n",
    "    target_update_period=target_update_period, # Frequency of updating target networks\n",
    "    dqda_clipping=dqda_clipping, # Optional clipping of the gradient of Q-value with respect to actions\n",
    "    td_errors_loss_fn=td_errors_loss_fn, # Loss function for temporal difference errors\n",
    "    gamma=gamma, # Discount factor for future rewards\n",
    "    reward_scale_factor=reward_scale_factor, # Scaling factor for rewards during training\n",
    "    gradient_clipping=gradient_clipping, # Optional clipping of gradients during training\n",
    "    debug_summaries=False, # Disable debug summaries\n",
    "    summarize_grads_and_vars=False,  # Disable summarizing gradients and variables\n",
    "    train_step_counter=global_step,  # Use the global step as the train step counter\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env_train.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps,\n",
    ")\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env_train,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration,\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pascal\\Documents\\Git\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\Wandb_test.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m WandbCallback\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m wandb\u001b[39m.\u001b[39mlogin()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m config\u001b[39m=\u001b[39m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: batch_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mactor_learning_rate\u001b[39m\u001b[39m\"\u001b[39m: actor_learning_rate,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mcritic_learning_rate\u001b[39m\u001b[39m\"\u001b[39m: critic_learning_rate\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pascal/Documents/Git/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/Wandb_test.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m       }\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wandb.keras'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()\n",
    "\n",
    "config={\n",
    "      \"batch_size\": batch_size,\n",
    "      \"actor_learning_rate\": actor_learning_rate,\n",
    "      \"critic_learning_rate\": critic_learning_rate\n",
    "      }\n",
    "wandb.init(\n",
    "    project=\"RL_Test\",\n",
    "    name=\"001\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better performance\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect initial replay data\n",
    "initial_collect_driver.run()\n",
    "\n",
    "time_step = tf_env_train.reset()\n",
    "policy_state = collect_policy.get_initial_state(tf_env_train.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=1):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward[0]\n",
    "        print('Episode Return = {0}'.format(episode_return))\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_dir = os.path.join(\"/\", 'policy')\n",
    "# tf_policy_saver = policy_saver.PolicySaver(eval_policy,1, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# pipeline which will feed data to the agent\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Train and evaluate\n",
    "while global_step.numpy() <= num_iterations:\n",
    "    time_step, policy_state = collect_driver.run(\n",
    "        time_step=time_step,\n",
    "        policy_state=policy_state,\n",
    "    )\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "    # actor_net.save(\"test\")\n",
    "    if (global_step.numpy() % 25 == 0):\n",
    "        avg_return = compute_avg_return(tf_env_eval, eval_policy)\n",
    "        print('step = {0}: loss = {1}, avg_return = {2}'.format(global_step.numpy(), train_loss.loss, avg_return))\n",
    "        wandb.log({\"loss\": train_loss.loss, \"avg_return\": avg_return})\n",
    "        # tf_policy_saver.save(policy_dir)\n",
    "    else:\n",
    "        print('step = {0}: loss = {1}'.format(global_step.numpy(), train_loss.loss))\n",
    "        wandb.log({\"loss\": train_loss.loss})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
