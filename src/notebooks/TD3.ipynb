{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents import ddpg\n",
    "from tf_agents.agents.td3 import td3_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import Dataloader\n",
    "import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param for iteration\n",
    "num_iterations = 10000\n",
    "\n",
    "# Params for collect\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1000\n",
    "replay_buffer_capacity = 1000000\n",
    "ou_stddev = 0.2\n",
    "ou_damping = 0.15\n",
    "\n",
    "# Params for target update\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for train\n",
    "exploration_noise_std = 0.1\n",
    "actor_update_period = 2\n",
    "train_steps_per_iteration = 1\n",
    "batch_size = 1000\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "dqda_clipping = None\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "gamma = 0.99\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "\n",
    "# Params for eval and checkpoints\n",
    "num_eval_episodes = 1\n",
    "eval_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = Dataloader.get_customer_data(Dataloader.loadData('../../data/load1011.csv'),\n",
    "                                          Dataloader.loadPrice('../../data/price.csv'), 1)\n",
    "data_eval = Dataloader.get_customer_data(Dataloader.loadData('../../data/load1112.csv'),\n",
    "                                         Dataloader.loadPrice('../../data/price.csv'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare runner\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=10.0, data=data_train))\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=10.0, data=data_eval))\n",
    "\n",
    "actor_net = ddpg.actor_network.ActorNetwork(input_tensor_spec=tf_env.observation_spec(),\n",
    "                                           output_tensor_spec=tf_env.action_spec(), fc_layer_params=(400, 300),\n",
    "                                           activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "critic_net = ddpg.critic_network.CriticNetwork(input_tensor_spec=(tf_env.observation_spec(), tf_env.action_spec()),\n",
    "                                              joint_fc_layer_params=(400, 300),\n",
    "                                              activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "tf_agent = td3_agent.Td3Agent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate\n",
    "    ),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate\n",
    "    ),\n",
    "    exploration_noise_std=exploration_noise_std,\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    actor_update_period=actor_update_period,\n",
    "    td_errors_loss_fn=td_errors_loss_fn,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    debug_summaries=False,\n",
    "    summarize_grads_and_vars=False,\n",
    "    train_step_counter=global_step,\n",
    ")\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps,\n",
    ")\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration,\n",
    ")\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir='./checkpoints/td3/',\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    logdir='./log/td3/', flush_millis=10000\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:103: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  updates the state based on the action taken, and returns the next TimeStep object,\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:104: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  which encapsulates the new state, reward, and whether the episode has ended.\n",
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\Environment.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  :return: next TimeStep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# For better performance\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Collect initial replay data\n",
    "initial_collect_driver.run()\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# pipeline which will feed data to the agent\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: Loss = 152.95082092285156\n",
      "step = 2: Loss = 151.955810546875\n",
      "step = 3: Loss = 150.98739624023438\n",
      "step = 4: Loss = 151.45814514160156\n",
      "step = 5: Loss = 151.27590942382812\n",
      "step = 6: Loss = 148.87506103515625\n",
      "step = 7: Loss = 149.77540588378906\n",
      "step = 8: Loss = 147.53358459472656\n",
      "step = 9: Loss = 147.81134033203125\n",
      "step = 10: Loss = 144.5395965576172\n",
      "step = 11: Loss = 142.4980010986328\n",
      "step = 12: Loss = 139.12059020996094\n",
      "step = 13: Loss = 136.4272003173828\n",
      "step = 14: Loss = 133.225830078125\n",
      "step = 15: Loss = 130.8792724609375\n",
      "step = 16: Loss = 126.95706939697266\n",
      "step = 17: Loss = 123.0464859008789\n",
      "step = 18: Loss = 118.03276062011719\n",
      "step = 19: Loss = 114.2195816040039\n",
      "step = 20: Loss = 106.9664306640625\n",
      "step = 21: Loss = 102.54523468017578\n",
      "step = 22: Loss = 95.34961700439453\n",
      "step = 23: Loss = 91.01310729980469\n",
      "step = 24: Loss = 86.27420043945312\n",
      "step = 25: Loss = 81.82112884521484\n",
      "step = 26: Loss = 79.2520980834961\n",
      "step = 27: Loss = 80.32582092285156\n",
      "step = 28: Loss = 82.7070083618164\n",
      "step = 29: Loss = 88.86075592041016\n",
      "step = 30: Loss = 94.93733215332031\n",
      "step = 31: Loss = 101.07288360595703\n",
      "step = 32: Loss = 106.03404235839844\n",
      "step = 33: Loss = 108.69760131835938\n",
      "step = 34: Loss = 109.02113342285156\n",
      "step = 35: Loss = 110.90863037109375\n",
      "step = 36: Loss = 107.0372314453125\n",
      "step = 37: Loss = 104.6182861328125\n",
      "step = 38: Loss = 102.73240661621094\n",
      "step = 39: Loss = 100.22161865234375\n",
      "step = 40: Loss = 95.92666625976562\n",
      "step = 41: Loss = 93.32392120361328\n",
      "step = 42: Loss = 89.58677673339844\n",
      "step = 43: Loss = 87.66622161865234\n",
      "step = 44: Loss = 86.16300964355469\n",
      "step = 45: Loss = 83.24624633789062\n",
      "step = 46: Loss = 81.32551574707031\n",
      "step = 47: Loss = 79.84258270263672\n",
      "step = 48: Loss = 79.7269287109375\n",
      "step = 49: Loss = 77.13487243652344\n",
      "step = 50: Loss = 76.50858306884766\n",
      "step = 51: Loss = 76.43623352050781\n",
      "step = 52: Loss = 75.03623962402344\n",
      "step = 53: Loss = 73.3045654296875\n",
      "step = 54: Loss = 72.50299072265625\n",
      "step = 55: Loss = 72.04852294921875\n",
      "step = 56: Loss = 70.881103515625\n",
      "step = 57: Loss = 71.3914794921875\n",
      "step = 58: Loss = 69.84839630126953\n",
      "step = 59: Loss = 68.89283752441406\n",
      "step = 60: Loss = 66.8104248046875\n",
      "step = 61: Loss = 66.69058227539062\n",
      "step = 62: Loss = 66.07258605957031\n",
      "step = 63: Loss = 65.5013198852539\n",
      "step = 64: Loss = 63.741729736328125\n",
      "step = 65: Loss = 63.89512634277344\n",
      "step = 66: Loss = 63.70319366455078\n",
      "step = 67: Loss = 62.90669250488281\n",
      "step = 68: Loss = 62.56156921386719\n",
      "step = 69: Loss = 61.569541931152344\n",
      "step = 70: Loss = 60.15279769897461\n",
      "step = 71: Loss = 58.18531036376953\n",
      "step = 72: Loss = 58.91389846801758\n",
      "step = 73: Loss = 58.5830078125\n",
      "step = 74: Loss = 56.318626403808594\n",
      "step = 75: Loss = 55.85968780517578\n",
      "step = 76: Loss = 54.14673614501953\n",
      "step = 77: Loss = 52.8175048828125\n",
      "step = 78: Loss = 52.326011657714844\n",
      "step = 79: Loss = 50.674598693847656\n",
      "step = 80: Loss = 48.401580810546875\n",
      "step = 81: Loss = 47.88495635986328\n",
      "step = 82: Loss = 46.148101806640625\n",
      "step = 83: Loss = 45.26860809326172\n",
      "step = 84: Loss = 44.74357223510742\n",
      "step = 85: Loss = 42.33761978149414\n",
      "step = 86: Loss = 40.07965087890625\n",
      "step = 87: Loss = 39.365806579589844\n",
      "step = 88: Loss = 37.308345794677734\n",
      "step = 89: Loss = 37.9948844909668\n",
      "step = 90: Loss = 35.7131462097168\n",
      "step = 91: Loss = 34.25453186035156\n",
      "step = 92: Loss = 33.899959564208984\n",
      "step = 93: Loss = 33.574859619140625\n",
      "step = 94: Loss = 32.52019119262695\n",
      "step = 95: Loss = 30.49801254272461\n",
      "step = 96: Loss = 29.704540252685547\n",
      "step = 97: Loss = 28.581769943237305\n",
      "step = 98: Loss = 28.462787628173828\n",
      "step = 99: Loss = 27.05768585205078\n",
      "step = 100: Loss = 26.05679702758789\n",
      "step = 101: Loss = 24.774497985839844\n",
      "step = 102: Loss = 23.765466690063477\n",
      "step = 103: Loss = 22.5234317779541\n",
      "step = 104: Loss = 22.919994354248047\n",
      "step = 105: Loss = 21.33694076538086\n",
      "step = 106: Loss = 20.29052734375\n",
      "step = 107: Loss = 18.93271255493164\n",
      "step = 108: Loss = 19.169719696044922\n",
      "step = 109: Loss = 18.262666702270508\n",
      "step = 110: Loss = 16.978923797607422\n",
      "step = 111: Loss = 17.018596649169922\n",
      "step = 112: Loss = 15.962562561035156\n",
      "step = 113: Loss = 15.59902286529541\n",
      "step = 114: Loss = 14.71852970123291\n",
      "step = 115: Loss = 14.243549346923828\n",
      "step = 116: Loss = 14.501129150390625\n",
      "step = 117: Loss = 13.51846694946289\n",
      "step = 118: Loss = 12.929915428161621\n",
      "step = 119: Loss = 11.970564842224121\n",
      "step = 120: Loss = 12.52213191986084\n",
      "step = 121: Loss = 12.737702369689941\n",
      "step = 122: Loss = 13.076581954956055\n",
      "step = 123: Loss = 12.025171279907227\n",
      "step = 124: Loss = 11.95053768157959\n",
      "step = 125: Loss = 11.76377010345459\n",
      "step = 126: Loss = 11.618474006652832\n",
      "step = 127: Loss = 11.542473793029785\n",
      "step = 128: Loss = 12.036458015441895\n",
      "step = 129: Loss = 12.50616455078125\n",
      "step = 130: Loss = 12.888364791870117\n",
      "step = 131: Loss = 12.734607696533203\n",
      "step = 132: Loss = 12.987992286682129\n",
      "step = 133: Loss = 13.00579833984375\n",
      "step = 134: Loss = 12.959488868713379\n",
      "step = 135: Loss = 13.066817283630371\n",
      "step = 136: Loss = 13.393207550048828\n",
      "step = 137: Loss = 14.818219184875488\n",
      "step = 138: Loss = 14.81079387664795\n",
      "step = 139: Loss = 14.909637451171875\n",
      "step = 140: Loss = 14.50704574584961\n",
      "step = 141: Loss = 15.50762939453125\n",
      "step = 142: Loss = 15.424178123474121\n",
      "step = 143: Loss = 15.426450729370117\n",
      "step = 144: Loss = 16.76934242248535\n",
      "step = 145: Loss = 16.53850555419922\n",
      "step = 146: Loss = 16.937400817871094\n",
      "step = 147: Loss = 17.568214416503906\n",
      "step = 148: Loss = 16.14866828918457\n",
      "step = 149: Loss = 16.074848175048828\n",
      "step = 150: Loss = 16.747343063354492\n",
      "step = 151: Loss = 17.788728713989258\n",
      "step = 152: Loss = 17.70475196838379\n",
      "step = 153: Loss = 17.682506561279297\n",
      "step = 154: Loss = 17.42228126525879\n",
      "step = 155: Loss = 17.878475189208984\n",
      "step = 156: Loss = 17.2457218170166\n",
      "step = 157: Loss = 17.635013580322266\n",
      "step = 158: Loss = 18.652896881103516\n",
      "step = 159: Loss = 19.183853149414062\n",
      "step = 160: Loss = 19.79039764404297\n",
      "step = 161: Loss = 20.629207611083984\n",
      "step = 162: Loss = 19.927183151245117\n",
      "step = 163: Loss = 18.857290267944336\n",
      "step = 164: Loss = 19.25767707824707\n",
      "step = 165: Loss = 19.397045135498047\n",
      "step = 166: Loss = 20.402523040771484\n",
      "step = 167: Loss = 21.00277328491211\n",
      "step = 168: Loss = 22.040306091308594\n",
      "step = 169: Loss = 21.10680389404297\n",
      "step = 170: Loss = 21.171611785888672\n",
      "step = 171: Loss = 21.35171890258789\n",
      "step = 172: Loss = 21.641286849975586\n",
      "step = 173: Loss = 21.735076904296875\n",
      "step = 174: Loss = 22.4791259765625\n",
      "step = 175: Loss = 22.238121032714844\n",
      "step = 176: Loss = 24.986652374267578\n",
      "step = 177: Loss = 24.44894790649414\n",
      "step = 178: Loss = 23.382465362548828\n",
      "step = 179: Loss = 24.058509826660156\n",
      "step = 180: Loss = 23.97885513305664\n",
      "step = 181: Loss = 24.96773910522461\n",
      "step = 182: Loss = 24.722965240478516\n",
      "step = 183: Loss = 25.323169708251953\n",
      "step = 184: Loss = 26.184560775756836\n",
      "step = 185: Loss = 26.916244506835938\n",
      "step = 186: Loss = 27.115787506103516\n",
      "step = 187: Loss = 26.92589569091797\n",
      "step = 188: Loss = 27.23806381225586\n",
      "step = 189: Loss = 25.78707504272461\n",
      "step = 190: Loss = 26.788724899291992\n",
      "step = 191: Loss = 27.94771385192871\n",
      "step = 192: Loss = 28.66219139099121\n",
      "step = 193: Loss = 28.77376365661621\n",
      "step = 194: Loss = 30.452713012695312\n",
      "step = 195: Loss = 29.868022918701172\n",
      "step = 196: Loss = 29.269786834716797\n",
      "step = 197: Loss = 29.642635345458984\n",
      "step = 198: Loss = 29.82002830505371\n",
      "step = 199: Loss = 31.026790618896484\n",
      "step = 200: Loss = 30.98503875732422\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x0000017041F0C160>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x0000017041F0C160>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 201: Loss = 32.6592903137207\n",
      "step = 202: Loss = 33.236106872558594\n",
      "step = 203: Loss = 32.823402404785156\n",
      "step = 204: Loss = 33.61589050292969\n",
      "step = 205: Loss = 33.03923416137695\n",
      "step = 206: Loss = 32.742042541503906\n",
      "step = 207: Loss = 33.338600158691406\n",
      "step = 208: Loss = 32.356475830078125\n",
      "step = 209: Loss = 33.45854568481445\n",
      "step = 210: Loss = 33.40104293823242\n",
      "step = 211: Loss = 33.74993896484375\n",
      "step = 212: Loss = 33.389312744140625\n",
      "step = 213: Loss = 34.368812561035156\n",
      "step = 214: Loss = 33.97489929199219\n",
      "step = 215: Loss = 33.83698272705078\n",
      "step = 216: Loss = 34.15704345703125\n",
      "step = 217: Loss = 35.59551239013672\n",
      "step = 218: Loss = 35.81010818481445\n",
      "step = 219: Loss = 36.364017486572266\n",
      "step = 220: Loss = 35.60051727294922\n",
      "step = 221: Loss = 35.32924270629883\n",
      "step = 222: Loss = 33.490386962890625\n",
      "step = 223: Loss = 33.366668701171875\n",
      "step = 224: Loss = 32.3648796081543\n",
      "step = 225: Loss = 33.368080139160156\n",
      "step = 226: Loss = 32.65901565551758\n",
      "step = 227: Loss = 33.708152770996094\n",
      "step = 228: Loss = 32.71555709838867\n",
      "step = 229: Loss = 32.62482452392578\n",
      "step = 230: Loss = 31.03518295288086\n",
      "step = 231: Loss = 30.463802337646484\n",
      "step = 232: Loss = 30.266036987304688\n",
      "step = 233: Loss = 32.24790573120117\n",
      "step = 234: Loss = 31.566486358642578\n",
      "step = 235: Loss = 32.070213317871094\n",
      "step = 236: Loss = 30.57917022705078\n",
      "step = 237: Loss = 30.24102783203125\n",
      "step = 238: Loss = 29.840229034423828\n",
      "step = 239: Loss = 29.429733276367188\n",
      "step = 240: Loss = 28.4404296875\n",
      "step = 241: Loss = 28.874719619750977\n",
      "step = 242: Loss = 29.14666748046875\n",
      "step = 243: Loss = 29.33234405517578\n",
      "step = 244: Loss = 28.92315673828125\n",
      "step = 245: Loss = 28.73482322692871\n",
      "step = 246: Loss = 27.917980194091797\n",
      "step = 247: Loss = 27.824567794799805\n",
      "step = 248: Loss = 27.528717041015625\n",
      "step = 249: Loss = 26.97133445739746\n",
      "step = 250: Loss = 27.975852966308594\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x0000017042EC1810>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x0000017042EC1810>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 251: Loss = 27.627836227416992\n",
      "step = 252: Loss = 27.914352416992188\n",
      "step = 253: Loss = 27.166934967041016\n",
      "step = 254: Loss = 25.836753845214844\n",
      "step = 255: Loss = 25.427640914916992\n",
      "step = 256: Loss = 26.025718688964844\n",
      "step = 257: Loss = 26.56068229675293\n",
      "step = 258: Loss = 26.466760635375977\n",
      "step = 259: Loss = 26.876468658447266\n",
      "step = 260: Loss = 27.122901916503906\n",
      "step = 261: Loss = 25.55857276916504\n",
      "step = 262: Loss = 25.77020263671875\n",
      "step = 263: Loss = 25.200904846191406\n",
      "step = 264: Loss = 25.633378982543945\n",
      "step = 265: Loss = 26.43639373779297\n",
      "step = 266: Loss = 26.296428680419922\n",
      "step = 267: Loss = 26.81468963623047\n",
      "step = 268: Loss = 26.723182678222656\n",
      "step = 269: Loss = 26.30539894104004\n",
      "step = 270: Loss = 25.550765991210938\n",
      "step = 271: Loss = 25.461185455322266\n",
      "step = 272: Loss = 26.071453094482422\n",
      "step = 273: Loss = 26.552839279174805\n",
      "step = 274: Loss = 26.98684310913086\n",
      "step = 275: Loss = 26.337112426757812\n",
      "step = 276: Loss = 25.903121948242188\n",
      "step = 277: Loss = 24.46316146850586\n",
      "step = 278: Loss = 25.294437408447266\n",
      "step = 279: Loss = 25.312541961669922\n",
      "step = 280: Loss = 25.232942581176758\n",
      "step = 281: Loss = 25.06647300720215\n",
      "step = 282: Loss = 25.200468063354492\n",
      "step = 283: Loss = 25.92613983154297\n",
      "step = 284: Loss = 25.8520450592041\n",
      "step = 285: Loss = 25.072277069091797\n",
      "step = 286: Loss = 25.208255767822266\n",
      "step = 287: Loss = 25.403406143188477\n",
      "step = 288: Loss = 25.40171241760254\n",
      "step = 289: Loss = 24.822776794433594\n",
      "step = 290: Loss = 24.163619995117188\n",
      "step = 291: Loss = 24.562042236328125\n",
      "step = 292: Loss = 25.558773040771484\n",
      "step = 293: Loss = 25.143766403198242\n",
      "step = 294: Loss = 25.21261978149414\n",
      "step = 295: Loss = 24.402175903320312\n",
      "step = 296: Loss = 24.2910213470459\n",
      "step = 297: Loss = 24.906333923339844\n",
      "step = 298: Loss = 25.377120971679688\n",
      "step = 299: Loss = 25.219146728515625\n",
      "step = 300: Loss = 25.40168571472168\n",
      "step = 301: Loss = 23.86440658569336\n",
      "step = 302: Loss = 24.413217544555664\n",
      "step = 303: Loss = 25.32136344909668\n",
      "step = 304: Loss = 25.677921295166016\n",
      "step = 305: Loss = 24.223888397216797\n",
      "step = 306: Loss = 24.327289581298828\n",
      "step = 307: Loss = 24.577695846557617\n",
      "step = 308: Loss = 25.02584457397461\n",
      "step = 309: Loss = 24.944416046142578\n",
      "step = 310: Loss = 24.922332763671875\n",
      "step = 311: Loss = 24.842479705810547\n",
      "step = 312: Loss = 25.175811767578125\n",
      "step = 313: Loss = 25.088794708251953\n",
      "step = 314: Loss = 25.378732681274414\n",
      "step = 315: Loss = 24.746566772460938\n",
      "step = 316: Loss = 25.14931869506836\n",
      "step = 317: Loss = 25.444589614868164\n",
      "step = 318: Loss = 24.711496353149414\n",
      "step = 319: Loss = 24.850820541381836\n",
      "step = 320: Loss = 24.97801399230957\n",
      "step = 321: Loss = 24.652952194213867\n",
      "step = 322: Loss = 25.000896453857422\n",
      "step = 323: Loss = 25.286392211914062\n",
      "step = 324: Loss = 25.102998733520508\n",
      "step = 325: Loss = 25.383636474609375\n",
      "step = 326: Loss = 24.241668701171875\n",
      "step = 327: Loss = 23.767940521240234\n",
      "step = 328: Loss = 24.78692054748535\n",
      "step = 329: Loss = 24.82991600036621\n",
      "step = 330: Loss = 24.807680130004883\n",
      "step = 331: Loss = 24.828763961791992\n",
      "step = 332: Loss = 25.077800750732422\n",
      "step = 333: Loss = 25.69561004638672\n",
      "step = 334: Loss = 25.316715240478516\n",
      "step = 335: Loss = 25.15829086303711\n",
      "step = 336: Loss = 25.053237915039062\n",
      "step = 337: Loss = 25.922035217285156\n",
      "step = 338: Loss = 25.699914932250977\n",
      "step = 339: Loss = 24.867443084716797\n",
      "step = 340: Loss = 24.35472869873047\n",
      "step = 341: Loss = 25.13327980041504\n",
      "step = 342: Loss = 25.137794494628906\n",
      "step = 343: Loss = 25.987895965576172\n",
      "step = 344: Loss = 25.854408264160156\n",
      "step = 345: Loss = 25.402416229248047\n",
      "step = 346: Loss = 24.814294815063477\n",
      "step = 347: Loss = 25.055145263671875\n",
      "step = 348: Loss = 25.635391235351562\n",
      "step = 349: Loss = 26.00439453125\n",
      "step = 350: Loss = 26.199462890625\n",
      "step = 351: Loss = 25.423738479614258\n",
      "step = 352: Loss = 25.166675567626953\n",
      "step = 353: Loss = 25.041658401489258\n",
      "step = 354: Loss = 25.886560440063477\n",
      "step = 355: Loss = 25.827924728393555\n",
      "step = 356: Loss = 26.065536499023438\n",
      "step = 357: Loss = 26.610336303710938\n",
      "step = 358: Loss = 25.497112274169922\n",
      "step = 359: Loss = 25.584415435791016\n",
      "step = 360: Loss = 25.70647430419922\n",
      "step = 361: Loss = 25.38454246520996\n",
      "step = 362: Loss = 25.577064514160156\n",
      "step = 363: Loss = 25.40401840209961\n",
      "step = 364: Loss = 26.912343978881836\n",
      "step = 365: Loss = 26.127635955810547\n",
      "step = 366: Loss = 25.86705207824707\n",
      "step = 367: Loss = 25.925460815429688\n",
      "step = 368: Loss = 25.617286682128906\n",
      "step = 369: Loss = 26.650691986083984\n",
      "step = 370: Loss = 26.407535552978516\n",
      "step = 371: Loss = 24.66292953491211\n",
      "step = 372: Loss = 25.900135040283203\n",
      "step = 373: Loss = 25.814823150634766\n",
      "step = 374: Loss = 25.364084243774414\n",
      "step = 375: Loss = 25.42303466796875\n",
      "step = 376: Loss = 25.937870025634766\n",
      "step = 377: Loss = 25.195354461669922\n",
      "step = 378: Loss = 26.527870178222656\n",
      "step = 379: Loss = 26.246505737304688\n",
      "step = 380: Loss = 25.34256362915039\n",
      "step = 381: Loss = 25.71392250061035\n",
      "step = 382: Loss = 25.155872344970703\n",
      "step = 383: Loss = 25.4022159576416\n",
      "step = 384: Loss = 25.92228889465332\n",
      "step = 385: Loss = 26.841758728027344\n",
      "step = 386: Loss = 26.327362060546875\n",
      "step = 387: Loss = 27.086284637451172\n",
      "step = 388: Loss = 26.243322372436523\n",
      "step = 389: Loss = 26.441238403320312\n",
      "step = 390: Loss = 25.992427825927734\n",
      "step = 391: Loss = 26.8916072845459\n",
      "step = 392: Loss = 26.464750289916992\n",
      "step = 393: Loss = 26.327682495117188\n",
      "step = 394: Loss = 25.901695251464844\n",
      "step = 395: Loss = 26.218196868896484\n",
      "step = 396: Loss = 26.26203155517578\n",
      "step = 397: Loss = 26.427663803100586\n",
      "step = 398: Loss = 26.54735565185547\n",
      "step = 399: Loss = 26.045578002929688\n",
      "step = 400: Loss = 26.85738754272461\n",
      "step = 401: Loss = 26.175886154174805\n",
      "step = 402: Loss = 26.204071044921875\n",
      "step = 403: Loss = 26.33625602722168\n",
      "step = 404: Loss = 26.630828857421875\n",
      "step = 405: Loss = 26.64324378967285\n",
      "step = 406: Loss = 26.33685302734375\n",
      "step = 407: Loss = 26.529457092285156\n",
      "step = 408: Loss = 26.83056640625\n",
      "step = 409: Loss = 26.639440536499023\n",
      "step = 410: Loss = 26.749305725097656\n",
      "step = 411: Loss = 27.329952239990234\n",
      "step = 412: Loss = 26.350135803222656\n",
      "step = 413: Loss = 27.197860717773438\n",
      "step = 414: Loss = 26.213546752929688\n",
      "step = 415: Loss = 26.249900817871094\n",
      "step = 416: Loss = 26.471923828125\n",
      "step = 417: Loss = 26.78926658630371\n",
      "step = 418: Loss = 28.252044677734375\n",
      "step = 419: Loss = 27.004226684570312\n",
      "step = 420: Loss = 26.458484649658203\n",
      "step = 421: Loss = 26.027812957763672\n",
      "step = 422: Loss = 26.403377532958984\n",
      "step = 423: Loss = 26.621566772460938\n",
      "step = 424: Loss = 27.048248291015625\n",
      "step = 425: Loss = 26.410024642944336\n",
      "step = 426: Loss = 25.975059509277344\n",
      "step = 427: Loss = 26.26172637939453\n",
      "step = 428: Loss = 26.496097564697266\n",
      "step = 429: Loss = 27.226943969726562\n",
      "step = 430: Loss = 27.485960006713867\n",
      "step = 431: Loss = 28.295764923095703\n",
      "step = 432: Loss = 26.7762393951416\n",
      "step = 433: Loss = 25.87360954284668\n",
      "step = 434: Loss = 27.05349349975586\n",
      "step = 435: Loss = 26.787445068359375\n",
      "step = 436: Loss = 26.950424194335938\n",
      "step = 437: Loss = 27.60932159423828\n",
      "step = 438: Loss = 27.466205596923828\n",
      "step = 439: Loss = 27.597824096679688\n",
      "step = 440: Loss = 27.19144058227539\n",
      "step = 441: Loss = 27.491348266601562\n",
      "step = 442: Loss = 26.49860191345215\n",
      "step = 443: Loss = 28.29336929321289\n",
      "step = 444: Loss = 28.176834106445312\n",
      "step = 445: Loss = 27.07172966003418\n",
      "step = 446: Loss = 27.431278228759766\n",
      "step = 447: Loss = 27.87366485595703\n",
      "step = 448: Loss = 27.078792572021484\n",
      "step = 449: Loss = 27.698719024658203\n",
      "step = 450: Loss = 27.924823760986328\n",
      "step = 451: Loss = 27.529939651489258\n",
      "step = 452: Loss = 27.078529357910156\n",
      "step = 453: Loss = 27.049863815307617\n",
      "step = 454: Loss = 28.13831329345703\n",
      "step = 455: Loss = 28.446880340576172\n",
      "step = 456: Loss = 27.134502410888672\n",
      "step = 457: Loss = 26.928647994995117\n",
      "step = 458: Loss = 27.571369171142578\n",
      "step = 459: Loss = 26.949750900268555\n",
      "step = 460: Loss = 26.24974822998047\n",
      "step = 461: Loss = 27.215320587158203\n",
      "step = 462: Loss = 28.041868209838867\n",
      "step = 463: Loss = 28.305620193481445\n",
      "step = 464: Loss = 27.003963470458984\n",
      "step = 465: Loss = 25.748435974121094\n",
      "step = 466: Loss = 27.61205291748047\n",
      "step = 467: Loss = 28.376888275146484\n",
      "step = 468: Loss = 28.005298614501953\n",
      "step = 469: Loss = 26.739791870117188\n",
      "step = 470: Loss = 26.971118927001953\n",
      "step = 471: Loss = 27.272045135498047\n",
      "step = 472: Loss = 28.151626586914062\n",
      "step = 473: Loss = 28.232192993164062\n",
      "step = 474: Loss = 28.286972045898438\n",
      "step = 475: Loss = 27.875137329101562\n",
      "step = 476: Loss = 26.788043975830078\n",
      "step = 477: Loss = 26.579849243164062\n",
      "step = 478: Loss = 26.5675048828125\n",
      "step = 479: Loss = 28.869850158691406\n",
      "step = 480: Loss = 28.682125091552734\n",
      "step = 481: Loss = 27.557540893554688\n",
      "step = 482: Loss = 28.04505729675293\n",
      "step = 483: Loss = 26.765369415283203\n",
      "step = 484: Loss = 27.27855682373047\n",
      "step = 485: Loss = 28.407352447509766\n",
      "step = 486: Loss = 28.1812801361084\n",
      "step = 487: Loss = 28.470542907714844\n",
      "step = 488: Loss = 28.11688232421875\n",
      "step = 489: Loss = 27.42775535583496\n",
      "step = 490: Loss = 27.223724365234375\n",
      "step = 491: Loss = 27.580806732177734\n",
      "step = 492: Loss = 28.08786964416504\n",
      "step = 493: Loss = 27.141286849975586\n",
      "step = 494: Loss = 28.44732093811035\n",
      "step = 495: Loss = 27.040172576904297\n",
      "step = 496: Loss = 27.64898681640625\n",
      "step = 497: Loss = 28.249042510986328\n",
      "step = 498: Loss = 29.354198455810547\n",
      "step = 499: Loss = 28.491201400756836\n",
      "step = 500: Loss = 28.55255889892578\n",
      "step = 501: Loss = 27.442636489868164\n",
      "step = 502: Loss = 28.375465393066406\n",
      "step = 503: Loss = 28.098949432373047\n",
      "step = 504: Loss = 27.206932067871094\n",
      "step = 505: Loss = 28.58014678955078\n",
      "step = 506: Loss = 28.486587524414062\n",
      "step = 507: Loss = 28.63536262512207\n",
      "step = 508: Loss = 28.391565322875977\n",
      "step = 509: Loss = 28.19680404663086\n",
      "step = 510: Loss = 27.52958106994629\n",
      "step = 511: Loss = 27.64655303955078\n",
      "step = 512: Loss = 27.740428924560547\n",
      "step = 513: Loss = 28.4600887298584\n",
      "step = 514: Loss = 27.491077423095703\n",
      "step = 515: Loss = 29.44629669189453\n",
      "step = 516: Loss = 28.594680786132812\n",
      "step = 517: Loss = 27.670446395874023\n",
      "step = 518: Loss = 27.66596221923828\n",
      "step = 519: Loss = 27.617237091064453\n",
      "step = 520: Loss = 28.493837356567383\n",
      "step = 521: Loss = 27.867271423339844\n",
      "step = 522: Loss = 29.290138244628906\n",
      "step = 523: Loss = 28.983104705810547\n",
      "step = 524: Loss = 28.814258575439453\n",
      "step = 525: Loss = 27.56051254272461\n",
      "step = 526: Loss = 28.91480827331543\n",
      "step = 527: Loss = 28.241594314575195\n",
      "step = 528: Loss = 28.698406219482422\n",
      "step = 529: Loss = 28.302181243896484\n",
      "step = 530: Loss = 28.611282348632812\n",
      "step = 531: Loss = 28.30306625366211\n",
      "step = 532: Loss = 28.38967514038086\n",
      "step = 533: Loss = 29.11418914794922\n",
      "step = 534: Loss = 28.603496551513672\n",
      "step = 535: Loss = 28.056354522705078\n",
      "step = 536: Loss = 29.763427734375\n",
      "step = 537: Loss = 27.390216827392578\n",
      "step = 538: Loss = 28.132091522216797\n",
      "step = 539: Loss = 27.634031295776367\n",
      "step = 540: Loss = 28.30557632446289\n",
      "step = 541: Loss = 30.376956939697266\n",
      "step = 542: Loss = 28.629789352416992\n",
      "step = 543: Loss = 27.786319732666016\n",
      "step = 544: Loss = 29.377696990966797\n",
      "step = 545: Loss = 28.706310272216797\n",
      "step = 546: Loss = 28.793621063232422\n",
      "step = 547: Loss = 29.083337783813477\n",
      "step = 548: Loss = 29.429738998413086\n",
      "step = 549: Loss = 28.57073211669922\n",
      "step = 550: Loss = 28.09903907775879\n",
      "step = 551: Loss = 28.284133911132812\n",
      "step = 552: Loss = 28.25591278076172\n",
      "step = 553: Loss = 28.7567195892334\n",
      "step = 554: Loss = 28.910551071166992\n",
      "step = 555: Loss = 29.15009307861328\n",
      "step = 556: Loss = 29.3530330657959\n",
      "step = 557: Loss = 28.06218719482422\n",
      "step = 558: Loss = 28.286529541015625\n",
      "step = 559: Loss = 28.209484100341797\n",
      "step = 560: Loss = 28.451492309570312\n",
      "step = 561: Loss = 29.180870056152344\n",
      "step = 562: Loss = 28.076704025268555\n",
      "step = 563: Loss = 28.01583480834961\n",
      "step = 564: Loss = 29.78500747680664\n",
      "step = 565: Loss = 28.495513916015625\n",
      "step = 566: Loss = 28.549495697021484\n",
      "step = 567: Loss = 28.687570571899414\n",
      "step = 568: Loss = 28.97197723388672\n",
      "step = 569: Loss = 29.145591735839844\n",
      "step = 570: Loss = 28.60634422302246\n",
      "step = 571: Loss = 29.570098876953125\n",
      "step = 572: Loss = 28.493961334228516\n",
      "step = 573: Loss = 28.725255966186523\n",
      "step = 574: Loss = 29.9445858001709\n",
      "step = 575: Loss = 30.219783782958984\n",
      "step = 576: Loss = 28.227453231811523\n",
      "step = 577: Loss = 29.32846450805664\n",
      "step = 578: Loss = 28.89984130859375\n",
      "step = 579: Loss = 29.211105346679688\n",
      "step = 580: Loss = 29.516517639160156\n",
      "step = 581: Loss = 27.71979331970215\n",
      "step = 582: Loss = 29.71622085571289\n",
      "step = 583: Loss = 28.647645950317383\n",
      "step = 584: Loss = 28.316442489624023\n",
      "step = 585: Loss = 29.08172607421875\n",
      "step = 586: Loss = 28.492027282714844\n",
      "step = 587: Loss = 30.293487548828125\n",
      "step = 588: Loss = 29.83761978149414\n",
      "step = 589: Loss = 29.87919044494629\n",
      "step = 590: Loss = 29.406837463378906\n",
      "step = 591: Loss = 29.93012237548828\n",
      "step = 592: Loss = 29.908893585205078\n",
      "step = 593: Loss = 29.692935943603516\n",
      "step = 594: Loss = 29.74163055419922\n",
      "step = 595: Loss = 30.008092880249023\n",
      "step = 596: Loss = 30.421382904052734\n",
      "step = 597: Loss = 29.137779235839844\n",
      "step = 598: Loss = 29.65515899658203\n",
      "step = 599: Loss = 29.624773025512695\n",
      "step = 600: Loss = 29.316810607910156\n",
      "step = 601: Loss = 29.408885955810547\n",
      "step = 602: Loss = 28.286521911621094\n",
      "step = 603: Loss = 28.7650146484375\n",
      "step = 604: Loss = 28.747953414916992\n",
      "step = 605: Loss = 29.950958251953125\n",
      "step = 606: Loss = 29.79125213623047\n",
      "step = 607: Loss = 29.441226959228516\n",
      "step = 608: Loss = 30.30359649658203\n",
      "step = 609: Loss = 29.02898406982422\n",
      "step = 610: Loss = 30.058731079101562\n",
      "step = 611: Loss = 28.115833282470703\n",
      "step = 612: Loss = 30.348655700683594\n",
      "step = 613: Loss = 30.01968765258789\n",
      "step = 614: Loss = 30.2994441986084\n",
      "step = 615: Loss = 29.414093017578125\n",
      "step = 616: Loss = 28.66458511352539\n",
      "step = 617: Loss = 31.164907455444336\n",
      "step = 618: Loss = 29.258119583129883\n",
      "step = 619: Loss = 29.461585998535156\n",
      "step = 620: Loss = 31.415382385253906\n",
      "step = 621: Loss = 29.135440826416016\n",
      "step = 622: Loss = 30.08306121826172\n",
      "step = 623: Loss = 29.10497283935547\n",
      "step = 624: Loss = 30.507619857788086\n",
      "step = 625: Loss = 31.072452545166016\n",
      "step = 626: Loss = 30.099895477294922\n",
      "step = 627: Loss = 30.48874282836914\n",
      "step = 628: Loss = 30.18077850341797\n",
      "step = 629: Loss = 30.158321380615234\n",
      "step = 630: Loss = 30.22353744506836\n",
      "step = 631: Loss = 29.55694580078125\n",
      "step = 632: Loss = 29.724506378173828\n",
      "step = 633: Loss = 30.07907485961914\n",
      "step = 634: Loss = 29.143285751342773\n",
      "step = 635: Loss = 29.28329849243164\n",
      "step = 636: Loss = 28.874267578125\n",
      "step = 637: Loss = 30.949790954589844\n",
      "step = 638: Loss = 31.028831481933594\n",
      "step = 639: Loss = 29.202423095703125\n",
      "step = 640: Loss = 29.063358306884766\n",
      "step = 641: Loss = 29.48357391357422\n",
      "step = 642: Loss = 30.76775550842285\n",
      "step = 643: Loss = 30.810611724853516\n",
      "step = 644: Loss = 30.464683532714844\n",
      "step = 645: Loss = 29.525083541870117\n",
      "step = 646: Loss = 29.48848533630371\n",
      "step = 647: Loss = 30.32276153564453\n",
      "step = 648: Loss = 30.18787956237793\n",
      "step = 649: Loss = 30.081707000732422\n",
      "step = 650: Loss = 30.13709259033203\n",
      "step = 651: Loss = 29.167068481445312\n",
      "step = 652: Loss = 29.21063995361328\n",
      "step = 653: Loss = 30.838939666748047\n",
      "step = 654: Loss = 30.836807250976562\n",
      "step = 655: Loss = 31.19563865661621\n",
      "step = 656: Loss = 29.87601089477539\n",
      "step = 657: Loss = 30.19580841064453\n",
      "step = 658: Loss = 29.40614128112793\n",
      "step = 659: Loss = 31.43570899963379\n",
      "step = 660: Loss = 29.6854248046875\n",
      "step = 661: Loss = 30.971742630004883\n",
      "step = 662: Loss = 31.583528518676758\n",
      "step = 663: Loss = 29.036102294921875\n",
      "step = 664: Loss = 30.499155044555664\n",
      "step = 665: Loss = 31.124982833862305\n",
      "step = 666: Loss = 30.14582061767578\n",
      "step = 667: Loss = 30.283145904541016\n",
      "step = 668: Loss = 29.879989624023438\n",
      "step = 669: Loss = 30.709951400756836\n",
      "step = 670: Loss = 29.3695068359375\n",
      "step = 671: Loss = 31.105445861816406\n",
      "step = 672: Loss = 31.15350914001465\n",
      "step = 673: Loss = 30.68124008178711\n",
      "step = 674: Loss = 31.04960060119629\n",
      "step = 675: Loss = 30.34229278564453\n",
      "step = 676: Loss = 31.33414649963379\n",
      "step = 677: Loss = 31.443504333496094\n",
      "step = 678: Loss = 30.202857971191406\n",
      "step = 679: Loss = 29.075965881347656\n",
      "step = 680: Loss = 29.672290802001953\n",
      "step = 681: Loss = 30.39025115966797\n",
      "step = 682: Loss = 29.601608276367188\n",
      "step = 683: Loss = 32.097198486328125\n",
      "step = 684: Loss = 30.62668800354004\n",
      "step = 685: Loss = 30.41232681274414\n",
      "step = 686: Loss = 30.222835540771484\n",
      "step = 687: Loss = 29.73628044128418\n",
      "step = 688: Loss = 31.045677185058594\n",
      "step = 689: Loss = 30.09896469116211\n",
      "step = 690: Loss = 29.667903900146484\n",
      "step = 691: Loss = 29.625972747802734\n",
      "step = 692: Loss = 29.414466857910156\n",
      "step = 693: Loss = 30.985855102539062\n",
      "step = 694: Loss = 31.32377815246582\n",
      "step = 695: Loss = 30.45946502685547\n",
      "step = 696: Loss = 31.04163360595703\n",
      "step = 697: Loss = 30.905067443847656\n",
      "step = 698: Loss = 31.402978897094727\n",
      "step = 699: Loss = 31.227066040039062\n",
      "step = 700: Loss = 30.22226333618164\n",
      "step = 701: Loss = 29.346864700317383\n",
      "step = 702: Loss = 31.099327087402344\n",
      "step = 703: Loss = 31.107585906982422\n",
      "step = 704: Loss = 30.80605697631836\n",
      "step = 705: Loss = 31.055091857910156\n",
      "step = 706: Loss = 29.851329803466797\n",
      "step = 707: Loss = 30.326889038085938\n",
      "step = 708: Loss = 30.54419708251953\n",
      "step = 709: Loss = 31.296293258666992\n",
      "step = 710: Loss = 31.206998825073242\n",
      "step = 711: Loss = 32.4874382019043\n",
      "step = 712: Loss = 30.923519134521484\n",
      "step = 713: Loss = 29.604137420654297\n",
      "step = 714: Loss = 31.613815307617188\n",
      "step = 715: Loss = 30.098674774169922\n",
      "step = 716: Loss = 32.1343994140625\n",
      "step = 717: Loss = 31.009206771850586\n",
      "step = 718: Loss = 31.649330139160156\n",
      "step = 719: Loss = 30.614370346069336\n",
      "step = 720: Loss = 30.03447151184082\n",
      "step = 721: Loss = 31.402502059936523\n",
      "step = 722: Loss = 31.655738830566406\n",
      "step = 723: Loss = 32.02764892578125\n",
      "step = 724: Loss = 30.033241271972656\n",
      "step = 725: Loss = 31.28118896484375\n",
      "step = 726: Loss = 30.550199508666992\n",
      "step = 727: Loss = 31.39032745361328\n",
      "step = 728: Loss = 31.694442749023438\n",
      "step = 729: Loss = 31.45929718017578\n",
      "step = 730: Loss = 31.07373809814453\n",
      "step = 731: Loss = 30.789993286132812\n",
      "step = 732: Loss = 30.096282958984375\n",
      "step = 733: Loss = 30.988819122314453\n",
      "step = 734: Loss = 31.036800384521484\n",
      "step = 735: Loss = 29.898664474487305\n",
      "step = 736: Loss = 30.37801742553711\n",
      "step = 737: Loss = 30.58576202392578\n",
      "step = 738: Loss = 30.279003143310547\n",
      "step = 739: Loss = 31.06160545349121\n",
      "step = 740: Loss = 30.69378089904785\n",
      "step = 741: Loss = 30.13520050048828\n",
      "step = 742: Loss = 32.19426727294922\n",
      "step = 743: Loss = 31.92595863342285\n",
      "step = 744: Loss = 31.642101287841797\n",
      "step = 745: Loss = 30.847187042236328\n",
      "step = 746: Loss = 31.245323181152344\n",
      "step = 747: Loss = 31.212570190429688\n",
      "step = 748: Loss = 31.9383544921875\n",
      "step = 749: Loss = 30.667308807373047\n",
      "step = 750: Loss = 31.261138916015625\n",
      "step = 751: Loss = 32.13715362548828\n",
      "step = 752: Loss = 32.33831787109375\n",
      "step = 753: Loss = 32.99200439453125\n",
      "step = 754: Loss = 30.916950225830078\n",
      "step = 755: Loss = 31.538610458374023\n",
      "step = 756: Loss = 32.27061080932617\n",
      "step = 757: Loss = 31.81595802307129\n",
      "step = 758: Loss = 31.859285354614258\n",
      "step = 759: Loss = 31.94964599609375\n",
      "step = 760: Loss = 31.763309478759766\n",
      "step = 761: Loss = 30.288570404052734\n",
      "step = 762: Loss = 30.739280700683594\n",
      "step = 763: Loss = 33.28424835205078\n",
      "step = 764: Loss = 32.61453628540039\n",
      "step = 765: Loss = 31.349716186523438\n",
      "step = 766: Loss = 32.599327087402344\n",
      "step = 767: Loss = 31.696590423583984\n",
      "step = 768: Loss = 32.813140869140625\n",
      "step = 769: Loss = 32.204734802246094\n",
      "step = 770: Loss = 31.545455932617188\n",
      "step = 771: Loss = 31.69841194152832\n",
      "step = 772: Loss = 32.58915328979492\n",
      "step = 773: Loss = 32.48270034790039\n",
      "step = 774: Loss = 32.11016082763672\n",
      "step = 775: Loss = 32.057830810546875\n",
      "step = 776: Loss = 31.42902946472168\n",
      "step = 777: Loss = 31.824539184570312\n",
      "step = 778: Loss = 32.567569732666016\n",
      "step = 779: Loss = 32.17610168457031\n",
      "step = 780: Loss = 31.824783325195312\n",
      "step = 781: Loss = 32.66010284423828\n",
      "step = 782: Loss = 32.47540283203125\n",
      "step = 783: Loss = 32.24758529663086\n",
      "step = 784: Loss = 31.06430435180664\n",
      "step = 785: Loss = 33.6350212097168\n",
      "step = 786: Loss = 31.683589935302734\n",
      "step = 787: Loss = 32.190460205078125\n",
      "step = 788: Loss = 32.26581573486328\n",
      "step = 789: Loss = 31.67807388305664\n",
      "step = 790: Loss = 31.81312370300293\n",
      "step = 791: Loss = 32.826690673828125\n",
      "step = 792: Loss = 31.50478744506836\n",
      "step = 793: Loss = 32.37254333496094\n",
      "step = 794: Loss = 32.692718505859375\n",
      "step = 795: Loss = 34.58613967895508\n",
      "step = 796: Loss = 30.301528930664062\n",
      "step = 797: Loss = 33.02831268310547\n",
      "step = 798: Loss = 32.6580810546875\n",
      "step = 799: Loss = 32.56290817260742\n",
      "step = 800: Loss = 31.29343032836914\n",
      "step = 801: Loss = 32.355010986328125\n",
      "step = 802: Loss = 34.0006217956543\n",
      "step = 803: Loss = 33.234214782714844\n",
      "step = 804: Loss = 32.88153839111328\n",
      "step = 805: Loss = 32.90885543823242\n",
      "step = 806: Loss = 32.01791763305664\n",
      "step = 807: Loss = 32.37515640258789\n",
      "step = 808: Loss = 32.465965270996094\n",
      "step = 809: Loss = 32.39849853515625\n",
      "step = 810: Loss = 32.68300247192383\n",
      "step = 811: Loss = 33.08470153808594\n",
      "step = 812: Loss = 33.595664978027344\n",
      "step = 813: Loss = 33.130836486816406\n",
      "step = 814: Loss = 34.49893569946289\n",
      "step = 815: Loss = 31.74161148071289\n",
      "step = 816: Loss = 32.57205581665039\n",
      "step = 817: Loss = 33.30181884765625\n",
      "step = 818: Loss = 32.19380187988281\n",
      "step = 819: Loss = 33.423797607421875\n",
      "step = 820: Loss = 34.102081298828125\n",
      "step = 821: Loss = 32.12809371948242\n",
      "step = 822: Loss = 32.288856506347656\n",
      "step = 823: Loss = 33.47603225708008\n",
      "step = 824: Loss = 32.958377838134766\n",
      "step = 825: Loss = 33.25843811035156\n",
      "step = 826: Loss = 33.08933639526367\n",
      "step = 827: Loss = 33.952091217041016\n",
      "step = 828: Loss = 33.238365173339844\n",
      "step = 829: Loss = 33.24955368041992\n",
      "step = 830: Loss = 32.87455749511719\n",
      "step = 831: Loss = 31.50194549560547\n",
      "step = 832: Loss = 33.26784896850586\n",
      "step = 833: Loss = 33.857269287109375\n",
      "step = 834: Loss = 32.03827667236328\n",
      "step = 835: Loss = 32.6163215637207\n",
      "step = 836: Loss = 33.074039459228516\n",
      "step = 837: Loss = 32.465049743652344\n",
      "step = 838: Loss = 32.84445571899414\n",
      "step = 839: Loss = 33.859004974365234\n",
      "step = 840: Loss = 32.98802185058594\n",
      "step = 841: Loss = 33.117950439453125\n",
      "step = 842: Loss = 33.64015579223633\n",
      "step = 843: Loss = 31.33936309814453\n",
      "step = 844: Loss = 34.39060974121094\n",
      "step = 845: Loss = 33.677486419677734\n",
      "step = 846: Loss = 32.68084716796875\n",
      "step = 847: Loss = 32.495765686035156\n",
      "step = 848: Loss = 32.60247039794922\n",
      "step = 849: Loss = 33.00904083251953\n",
      "step = 850: Loss = 32.85612869262695\n",
      "step = 851: Loss = 32.93277359008789\n",
      "step = 852: Loss = 33.656959533691406\n",
      "step = 853: Loss = 34.26487731933594\n",
      "step = 854: Loss = 34.33024597167969\n",
      "step = 855: Loss = 33.042076110839844\n",
      "step = 856: Loss = 33.62150573730469\n",
      "step = 857: Loss = 34.099273681640625\n",
      "step = 858: Loss = 34.154727935791016\n",
      "step = 859: Loss = 32.610450744628906\n",
      "step = 860: Loss = 35.344757080078125\n",
      "step = 861: Loss = 34.461185455322266\n",
      "step = 862: Loss = 35.45375442504883\n",
      "step = 863: Loss = 34.81767654418945\n",
      "step = 864: Loss = 34.35593795776367\n",
      "step = 865: Loss = 33.15153503417969\n",
      "step = 866: Loss = 33.81208801269531\n",
      "step = 867: Loss = 34.632164001464844\n",
      "step = 868: Loss = 33.281166076660156\n",
      "step = 869: Loss = 32.84333419799805\n",
      "step = 870: Loss = 33.797119140625\n",
      "step = 871: Loss = 33.54985809326172\n",
      "step = 872: Loss = 34.32124328613281\n",
      "step = 873: Loss = 33.86095428466797\n",
      "step = 874: Loss = 33.62295913696289\n",
      "step = 875: Loss = 33.37255096435547\n",
      "step = 876: Loss = 34.36894607543945\n",
      "step = 877: Loss = 33.928306579589844\n",
      "step = 878: Loss = 36.1199951171875\n",
      "step = 879: Loss = 35.42094039916992\n",
      "step = 880: Loss = 33.056331634521484\n",
      "step = 881: Loss = 35.059417724609375\n",
      "step = 882: Loss = 33.62847137451172\n",
      "step = 883: Loss = 35.66559600830078\n",
      "step = 884: Loss = 33.41509246826172\n",
      "step = 885: Loss = 35.9051513671875\n",
      "step = 886: Loss = 34.632816314697266\n",
      "step = 887: Loss = 33.784149169921875\n",
      "step = 888: Loss = 36.948768615722656\n",
      "step = 889: Loss = 34.720191955566406\n",
      "step = 890: Loss = 33.28838348388672\n",
      "step = 891: Loss = 35.255821228027344\n",
      "step = 892: Loss = 33.63016128540039\n",
      "step = 893: Loss = 33.60728454589844\n",
      "step = 894: Loss = 33.71745300292969\n",
      "step = 895: Loss = 34.315185546875\n",
      "step = 896: Loss = 34.76728057861328\n",
      "step = 897: Loss = 33.72144317626953\n",
      "step = 898: Loss = 34.028282165527344\n",
      "step = 899: Loss = 33.97099685668945\n",
      "step = 900: Loss = 34.8356819152832\n",
      "step = 901: Loss = 33.09544372558594\n",
      "step = 902: Loss = 34.763824462890625\n",
      "step = 903: Loss = 35.93490982055664\n",
      "step = 904: Loss = 35.278465270996094\n",
      "step = 905: Loss = 34.287471771240234\n",
      "step = 906: Loss = 34.712345123291016\n",
      "step = 907: Loss = 35.85894012451172\n",
      "step = 908: Loss = 35.626060485839844\n",
      "step = 909: Loss = 35.200923919677734\n",
      "step = 910: Loss = 33.25883483886719\n",
      "step = 911: Loss = 32.538394927978516\n",
      "step = 912: Loss = 33.244205474853516\n",
      "step = 913: Loss = 34.90959930419922\n",
      "step = 914: Loss = 32.592464447021484\n",
      "step = 915: Loss = 34.02632141113281\n",
      "step = 916: Loss = 34.97123718261719\n",
      "step = 917: Loss = 33.632572174072266\n",
      "step = 918: Loss = 35.59675216674805\n",
      "step = 919: Loss = 34.25960159301758\n",
      "step = 920: Loss = 34.198143005371094\n",
      "step = 921: Loss = 34.2260856628418\n",
      "step = 922: Loss = 35.12375259399414\n",
      "step = 923: Loss = 34.76939392089844\n",
      "step = 924: Loss = 34.73917007446289\n",
      "step = 925: Loss = 34.13914489746094\n",
      "step = 926: Loss = 34.176422119140625\n",
      "step = 927: Loss = 35.98191833496094\n",
      "step = 928: Loss = 35.079063415527344\n",
      "step = 929: Loss = 33.506473541259766\n",
      "step = 930: Loss = 35.22343826293945\n",
      "step = 931: Loss = 35.90727233886719\n",
      "step = 932: Loss = 36.013946533203125\n",
      "step = 933: Loss = 38.0858154296875\n",
      "step = 934: Loss = 34.79883575439453\n",
      "step = 935: Loss = 33.518470764160156\n",
      "step = 936: Loss = 35.205055236816406\n",
      "step = 937: Loss = 36.724300384521484\n",
      "step = 938: Loss = 35.53740310668945\n",
      "step = 939: Loss = 34.999122619628906\n",
      "step = 940: Loss = 36.661903381347656\n",
      "step = 941: Loss = 36.14464569091797\n",
      "step = 942: Loss = 33.479068756103516\n",
      "step = 943: Loss = 34.238616943359375\n",
      "step = 944: Loss = 35.5185546875\n",
      "step = 945: Loss = 36.022342681884766\n",
      "step = 946: Loss = 36.13938522338867\n",
      "step = 947: Loss = 34.62967300415039\n",
      "step = 948: Loss = 36.57063674926758\n",
      "step = 949: Loss = 35.460201263427734\n",
      "step = 950: Loss = 35.94471740722656\n",
      "step = 951: Loss = 37.25252151489258\n",
      "step = 952: Loss = 36.74883270263672\n",
      "step = 953: Loss = 33.43117904663086\n",
      "step = 954: Loss = 34.195167541503906\n",
      "step = 955: Loss = 36.319454193115234\n",
      "step = 956: Loss = 35.72132110595703\n",
      "step = 957: Loss = 35.101436614990234\n",
      "step = 958: Loss = 35.530616760253906\n",
      "step = 959: Loss = 35.38343048095703\n",
      "step = 960: Loss = 35.75449752807617\n",
      "step = 961: Loss = 34.81950378417969\n",
      "step = 962: Loss = 34.9466667175293\n",
      "step = 963: Loss = 34.217445373535156\n",
      "step = 964: Loss = 34.980308532714844\n",
      "step = 965: Loss = 34.59393310546875\n",
      "step = 966: Loss = 35.441566467285156\n",
      "step = 967: Loss = 35.773529052734375\n",
      "step = 968: Loss = 35.33114242553711\n",
      "step = 969: Loss = 34.66419982910156\n",
      "step = 970: Loss = 36.252532958984375\n",
      "step = 971: Loss = 36.07138442993164\n",
      "step = 972: Loss = 34.17904281616211\n",
      "step = 973: Loss = 36.3280029296875\n",
      "step = 974: Loss = 34.500572204589844\n",
      "step = 975: Loss = 35.07639694213867\n",
      "step = 976: Loss = 37.95924377441406\n",
      "step = 977: Loss = 35.77923583984375\n",
      "step = 978: Loss = 34.457550048828125\n",
      "step = 979: Loss = 34.156429290771484\n",
      "step = 980: Loss = 36.786216735839844\n",
      "step = 981: Loss = 36.169395446777344\n",
      "step = 982: Loss = 34.47269821166992\n",
      "step = 983: Loss = 34.41815185546875\n",
      "step = 984: Loss = 35.2990608215332\n",
      "step = 985: Loss = 36.77300262451172\n",
      "step = 986: Loss = 37.647926330566406\n",
      "step = 987: Loss = 36.39201354980469\n",
      "step = 988: Loss = 37.22509765625\n",
      "step = 989: Loss = 33.91278839111328\n",
      "step = 990: Loss = 37.150020599365234\n",
      "step = 991: Loss = 35.056419372558594\n",
      "step = 992: Loss = 36.850006103515625\n",
      "step = 993: Loss = 35.13364028930664\n",
      "step = 994: Loss = 37.34974670410156\n",
      "step = 995: Loss = 37.66773223876953\n",
      "step = 996: Loss = 37.756282806396484\n",
      "step = 997: Loss = 36.493125915527344\n",
      "step = 998: Loss = 35.1634635925293\n",
      "step = 999: Loss = 36.63482666015625\n",
      "step = 1000: Loss = 37.509521484375\n",
      "step = 1001: Loss = 35.57587814331055\n",
      "step = 1002: Loss = 35.888450622558594\n",
      "step = 1003: Loss = 35.07319641113281\n",
      "step = 1004: Loss = 36.130027770996094\n",
      "step = 1005: Loss = 36.802955627441406\n",
      "step = 1006: Loss = 35.66286087036133\n",
      "step = 1007: Loss = 36.96259307861328\n",
      "step = 1008: Loss = 36.74189758300781\n",
      "step = 1009: Loss = 34.61097717285156\n",
      "step = 1010: Loss = 37.199806213378906\n",
      "step = 1011: Loss = 36.559993743896484\n",
      "step = 1012: Loss = 34.62984848022461\n",
      "step = 1013: Loss = 34.26285934448242\n",
      "step = 1014: Loss = 36.06486129760742\n",
      "step = 1015: Loss = 35.9224967956543\n",
      "step = 1016: Loss = 39.218299865722656\n",
      "step = 1017: Loss = 36.02406692504883\n",
      "step = 1018: Loss = 35.6537971496582\n",
      "step = 1019: Loss = 36.97069549560547\n",
      "step = 1020: Loss = 35.74306106567383\n",
      "step = 1021: Loss = 34.39144515991211\n",
      "step = 1022: Loss = 34.90863800048828\n",
      "step = 1023: Loss = 34.6884765625\n",
      "step = 1024: Loss = 35.55591583251953\n",
      "step = 1025: Loss = 34.78416442871094\n",
      "step = 1026: Loss = 34.36928939819336\n",
      "step = 1027: Loss = 35.26754379272461\n",
      "step = 1028: Loss = 37.025596618652344\n",
      "step = 1029: Loss = 36.530296325683594\n",
      "step = 1030: Loss = 35.87804412841797\n",
      "step = 1031: Loss = 36.0340576171875\n",
      "step = 1032: Loss = 35.341243743896484\n",
      "step = 1033: Loss = 35.896446228027344\n",
      "step = 1034: Loss = 35.56494903564453\n",
      "step = 1035: Loss = 36.192378997802734\n",
      "step = 1036: Loss = 35.141353607177734\n",
      "step = 1037: Loss = 37.15262222290039\n",
      "step = 1038: Loss = 34.535640716552734\n",
      "step = 1039: Loss = 35.52206039428711\n",
      "step = 1040: Loss = 31.92630386352539\n",
      "step = 1041: Loss = 36.61927795410156\n",
      "step = 1042: Loss = 35.88979721069336\n",
      "step = 1043: Loss = 35.33309555053711\n",
      "step = 1044: Loss = 35.10728454589844\n",
      "step = 1045: Loss = 35.87764358520508\n",
      "step = 1046: Loss = 34.73700714111328\n",
      "step = 1047: Loss = 34.867454528808594\n",
      "step = 1048: Loss = 35.259334564208984\n",
      "step = 1049: Loss = 35.79103469848633\n",
      "step = 1050: Loss = 35.505680084228516\n",
      "step = 1051: Loss = 35.07944107055664\n",
      "step = 1052: Loss = 34.69999694824219\n",
      "step = 1053: Loss = 33.72577667236328\n",
      "step = 1054: Loss = 33.96730041503906\n",
      "step = 1055: Loss = 34.37852096557617\n",
      "step = 1056: Loss = 36.118194580078125\n",
      "step = 1057: Loss = 37.3632698059082\n",
      "step = 1058: Loss = 35.27008819580078\n",
      "step = 1059: Loss = 36.990875244140625\n",
      "step = 1060: Loss = 35.842227935791016\n",
      "step = 1061: Loss = 34.27143096923828\n",
      "step = 1062: Loss = 35.93954849243164\n",
      "step = 1063: Loss = 32.81325149536133\n",
      "step = 1064: Loss = 35.65475845336914\n",
      "step = 1065: Loss = 34.72425079345703\n",
      "step = 1066: Loss = 35.66566467285156\n",
      "step = 1067: Loss = 35.597633361816406\n",
      "step = 1068: Loss = 36.91532897949219\n",
      "step = 1069: Loss = 33.73585510253906\n",
      "step = 1070: Loss = 34.77044677734375\n",
      "step = 1071: Loss = 35.51549530029297\n",
      "step = 1072: Loss = 35.63689422607422\n",
      "step = 1073: Loss = 37.37901306152344\n",
      "step = 1074: Loss = 36.00042724609375\n",
      "step = 1075: Loss = 36.79948043823242\n",
      "step = 1076: Loss = 35.58264923095703\n",
      "step = 1077: Loss = 37.314697265625\n",
      "step = 1078: Loss = 36.67842102050781\n",
      "step = 1079: Loss = 34.62749099731445\n",
      "step = 1080: Loss = 35.560699462890625\n",
      "step = 1081: Loss = 36.6410026550293\n",
      "step = 1082: Loss = 34.660743713378906\n",
      "step = 1083: Loss = 35.97172546386719\n",
      "step = 1084: Loss = 34.69364929199219\n",
      "step = 1085: Loss = 34.95370101928711\n",
      "step = 1086: Loss = 33.94462966918945\n",
      "step = 1087: Loss = 37.044498443603516\n",
      "step = 1088: Loss = 35.921165466308594\n",
      "step = 1089: Loss = 34.211761474609375\n",
      "step = 1090: Loss = 34.19921112060547\n",
      "step = 1091: Loss = 34.984840393066406\n",
      "step = 1092: Loss = 35.53094482421875\n",
      "step = 1093: Loss = 34.66017532348633\n",
      "step = 1094: Loss = 34.362327575683594\n",
      "step = 1095: Loss = 38.01705551147461\n",
      "step = 1096: Loss = 36.12286376953125\n",
      "step = 1097: Loss = 35.585445404052734\n",
      "step = 1098: Loss = 36.265167236328125\n",
      "step = 1099: Loss = 35.46009826660156\n",
      "step = 1100: Loss = 36.88335418701172\n",
      "step = 1101: Loss = 34.37735366821289\n",
      "step = 1102: Loss = 36.295570373535156\n",
      "step = 1103: Loss = 35.74531936645508\n",
      "step = 1104: Loss = 34.161373138427734\n",
      "step = 1105: Loss = 34.45367431640625\n",
      "step = 1106: Loss = 37.318965911865234\n",
      "step = 1107: Loss = 35.02780532836914\n",
      "step = 1108: Loss = 36.185150146484375\n",
      "step = 1109: Loss = 34.682151794433594\n",
      "step = 1110: Loss = 36.595951080322266\n",
      "step = 1111: Loss = 36.04900360107422\n",
      "step = 1112: Loss = 35.71048355102539\n",
      "step = 1113: Loss = 36.60586166381836\n",
      "step = 1114: Loss = 37.111759185791016\n",
      "step = 1115: Loss = 36.9274787902832\n",
      "step = 1116: Loss = 36.131160736083984\n",
      "step = 1117: Loss = 35.479042053222656\n",
      "step = 1118: Loss = 36.60652160644531\n",
      "step = 1119: Loss = 37.38468933105469\n",
      "step = 1120: Loss = 35.15503692626953\n",
      "step = 1121: Loss = 36.29362869262695\n",
      "step = 1122: Loss = 35.24104309082031\n",
      "step = 1123: Loss = 35.88054656982422\n",
      "step = 1124: Loss = 35.65956497192383\n",
      "step = 1125: Loss = 36.08591842651367\n",
      "step = 1126: Loss = 35.69794464111328\n",
      "step = 1127: Loss = 35.119346618652344\n",
      "step = 1128: Loss = 37.707481384277344\n",
      "step = 1129: Loss = 36.793087005615234\n",
      "step = 1130: Loss = 36.65747833251953\n",
      "step = 1131: Loss = 36.64479064941406\n",
      "step = 1132: Loss = 36.44232177734375\n",
      "step = 1133: Loss = 36.977317810058594\n",
      "step = 1134: Loss = 36.087459564208984\n",
      "step = 1135: Loss = 37.890403747558594\n",
      "step = 1136: Loss = 39.71920394897461\n",
      "step = 1137: Loss = 35.0621337890625\n",
      "step = 1138: Loss = 36.031028747558594\n",
      "step = 1139: Loss = 36.503387451171875\n",
      "step = 1140: Loss = 36.805545806884766\n",
      "step = 1141: Loss = 37.49171829223633\n",
      "step = 1142: Loss = 35.327049255371094\n",
      "step = 1143: Loss = 36.82051467895508\n",
      "step = 1144: Loss = 36.700923919677734\n",
      "step = 1145: Loss = 38.1280403137207\n",
      "step = 1146: Loss = 38.10468673706055\n",
      "step = 1147: Loss = 38.27692794799805\n",
      "step = 1148: Loss = 37.870361328125\n",
      "step = 1149: Loss = 34.309688568115234\n",
      "step = 1150: Loss = 36.49024963378906\n",
      "step = 1151: Loss = 36.35718536376953\n",
      "step = 1152: Loss = 36.23100280761719\n",
      "step = 1153: Loss = 36.2502326965332\n",
      "step = 1154: Loss = 37.826812744140625\n",
      "step = 1155: Loss = 36.235958099365234\n",
      "step = 1156: Loss = 37.085960388183594\n",
      "step = 1157: Loss = 37.342166900634766\n",
      "step = 1158: Loss = 35.12976837158203\n",
      "step = 1159: Loss = 37.93803405761719\n",
      "step = 1160: Loss = 36.87299346923828\n",
      "step = 1161: Loss = 37.8703498840332\n",
      "step = 1162: Loss = 36.951332092285156\n",
      "step = 1163: Loss = 37.94446563720703\n",
      "step = 1164: Loss = 37.91453552246094\n",
      "step = 1165: Loss = 37.87286376953125\n",
      "step = 1166: Loss = 36.875370025634766\n",
      "step = 1167: Loss = 36.640472412109375\n",
      "step = 1168: Loss = 36.603580474853516\n",
      "step = 1169: Loss = 37.42278289794922\n",
      "step = 1170: Loss = 36.97440719604492\n",
      "step = 1171: Loss = 37.78440856933594\n",
      "step = 1172: Loss = 36.60819625854492\n",
      "step = 1173: Loss = 38.204681396484375\n",
      "step = 1174: Loss = 35.873146057128906\n",
      "step = 1175: Loss = 36.12982177734375\n",
      "step = 1176: Loss = 35.546043395996094\n",
      "step = 1177: Loss = 38.64341735839844\n",
      "step = 1178: Loss = 33.61857223510742\n",
      "step = 1179: Loss = 37.58745574951172\n",
      "step = 1180: Loss = 35.72422790527344\n",
      "step = 1181: Loss = 35.52397155761719\n",
      "step = 1182: Loss = 38.13433837890625\n",
      "step = 1183: Loss = 36.145851135253906\n",
      "step = 1184: Loss = 36.419349670410156\n",
      "step = 1185: Loss = 37.15492248535156\n",
      "step = 1186: Loss = 39.857627868652344\n",
      "step = 1187: Loss = 38.74132537841797\n",
      "step = 1188: Loss = 36.69041442871094\n",
      "step = 1189: Loss = 35.09514236450195\n",
      "step = 1190: Loss = 37.79247283935547\n",
      "step = 1191: Loss = 39.543678283691406\n",
      "step = 1192: Loss = 38.92787551879883\n",
      "step = 1193: Loss = 36.586700439453125\n",
      "step = 1194: Loss = 38.607078552246094\n",
      "step = 1195: Loss = 38.49534606933594\n",
      "step = 1196: Loss = 37.63387680053711\n",
      "step = 1197: Loss = 37.3721809387207\n",
      "step = 1198: Loss = 38.1513557434082\n",
      "step = 1199: Loss = 38.43267059326172\n",
      "step = 1200: Loss = 39.28197479248047\n",
      "step = 1201: Loss = 38.778045654296875\n",
      "step = 1202: Loss = 35.53913879394531\n",
      "step = 1203: Loss = 36.578128814697266\n",
      "step = 1204: Loss = 39.61867904663086\n",
      "step = 1205: Loss = 38.42872619628906\n",
      "step = 1206: Loss = 39.79034423828125\n",
      "step = 1207: Loss = 39.50511932373047\n",
      "step = 1208: Loss = 40.01404571533203\n",
      "step = 1209: Loss = 39.15949249267578\n",
      "step = 1210: Loss = 39.134117126464844\n",
      "step = 1211: Loss = 37.711856842041016\n",
      "step = 1212: Loss = 37.68750762939453\n",
      "step = 1213: Loss = 38.3431396484375\n",
      "step = 1214: Loss = 38.941749572753906\n",
      "step = 1215: Loss = 39.65003967285156\n",
      "step = 1216: Loss = 36.5726318359375\n",
      "step = 1217: Loss = 38.177494049072266\n",
      "step = 1218: Loss = 40.04918670654297\n",
      "step = 1219: Loss = 39.32243347167969\n",
      "step = 1220: Loss = 38.71210861206055\n",
      "step = 1221: Loss = 37.05490493774414\n",
      "step = 1222: Loss = 37.07288360595703\n",
      "step = 1223: Loss = 41.56981658935547\n",
      "step = 1224: Loss = 39.18465042114258\n",
      "step = 1225: Loss = 38.43609619140625\n",
      "step = 1226: Loss = 38.43998336791992\n",
      "step = 1227: Loss = 41.63618087768555\n",
      "step = 1228: Loss = 38.79172134399414\n",
      "step = 1229: Loss = 39.91606521606445\n",
      "step = 1230: Loss = 38.13768005371094\n",
      "step = 1231: Loss = 38.74978256225586\n",
      "step = 1232: Loss = 36.20977783203125\n",
      "step = 1233: Loss = 38.126251220703125\n",
      "step = 1234: Loss = 40.51984786987305\n",
      "step = 1235: Loss = 41.75145721435547\n",
      "step = 1236: Loss = 37.75697708129883\n",
      "step = 1237: Loss = 40.32356262207031\n",
      "step = 1238: Loss = 38.235694885253906\n",
      "step = 1239: Loss = 36.466609954833984\n",
      "step = 1240: Loss = 39.69572830200195\n",
      "step = 1241: Loss = 39.142738342285156\n",
      "step = 1242: Loss = 37.87586975097656\n",
      "step = 1243: Loss = 39.94862365722656\n",
      "step = 1244: Loss = 38.145050048828125\n",
      "step = 1245: Loss = 40.55062484741211\n",
      "step = 1246: Loss = 36.957618713378906\n",
      "step = 1247: Loss = 38.583106994628906\n",
      "step = 1248: Loss = 40.367103576660156\n",
      "step = 1249: Loss = 39.78174591064453\n",
      "step = 1250: Loss = 41.32857131958008\n",
      "step = 1251: Loss = 39.175418853759766\n",
      "step = 1252: Loss = 39.41579055786133\n",
      "step = 1253: Loss = 39.35956573486328\n",
      "step = 1254: Loss = 38.74346923828125\n",
      "step = 1255: Loss = 37.67374801635742\n",
      "step = 1256: Loss = 41.752071380615234\n",
      "step = 1257: Loss = 40.45792770385742\n",
      "step = 1258: Loss = 39.26396942138672\n",
      "step = 1259: Loss = 37.01604461669922\n",
      "step = 1260: Loss = 37.071617126464844\n",
      "step = 1261: Loss = 39.309326171875\n",
      "step = 1262: Loss = 38.967674255371094\n",
      "step = 1263: Loss = 38.47296142578125\n",
      "step = 1264: Loss = 39.4512939453125\n",
      "step = 1265: Loss = 39.62510299682617\n",
      "step = 1266: Loss = 43.13224792480469\n",
      "step = 1267: Loss = 39.0553092956543\n",
      "step = 1268: Loss = 37.887046813964844\n",
      "step = 1269: Loss = 38.62555694580078\n",
      "step = 1270: Loss = 38.3649787902832\n",
      "step = 1271: Loss = 40.29817199707031\n",
      "step = 1272: Loss = 39.74998092651367\n",
      "step = 1273: Loss = 39.782386779785156\n",
      "step = 1274: Loss = 42.16194152832031\n",
      "step = 1275: Loss = 43.827205657958984\n",
      "step = 1276: Loss = 42.73665237426758\n",
      "step = 1277: Loss = 39.38262176513672\n",
      "step = 1278: Loss = 38.83765411376953\n",
      "step = 1279: Loss = 41.039703369140625\n",
      "step = 1280: Loss = 38.324127197265625\n",
      "step = 1281: Loss = 39.983821868896484\n",
      "step = 1282: Loss = 42.48883819580078\n",
      "step = 1283: Loss = 40.3038215637207\n",
      "step = 1284: Loss = 40.41090774536133\n",
      "step = 1285: Loss = 39.35448455810547\n",
      "step = 1286: Loss = 39.7328987121582\n",
      "step = 1287: Loss = 39.92026138305664\n",
      "step = 1288: Loss = 39.4661979675293\n",
      "step = 1289: Loss = 39.81974411010742\n",
      "step = 1290: Loss = 41.08531188964844\n",
      "step = 1291: Loss = 41.916015625\n",
      "step = 1292: Loss = 39.592891693115234\n",
      "step = 1293: Loss = 39.193058013916016\n",
      "step = 1294: Loss = 43.083534240722656\n",
      "step = 1295: Loss = 38.53690719604492\n",
      "step = 1296: Loss = 39.33539581298828\n",
      "step = 1297: Loss = 38.420570373535156\n",
      "step = 1298: Loss = 40.67328643798828\n",
      "step = 1299: Loss = 39.965675354003906\n",
      "step = 1300: Loss = 40.84555435180664\n",
      "step = 1301: Loss = 40.19915008544922\n",
      "step = 1302: Loss = 40.54948425292969\n",
      "step = 1303: Loss = 40.236351013183594\n",
      "step = 1304: Loss = 40.344085693359375\n",
      "step = 1305: Loss = 40.34881591796875\n",
      "step = 1306: Loss = 39.858924865722656\n",
      "step = 1307: Loss = 38.282310485839844\n",
      "step = 1308: Loss = 41.84568786621094\n",
      "step = 1309: Loss = 43.48902130126953\n",
      "step = 1310: Loss = 41.588958740234375\n",
      "step = 1311: Loss = 40.57793426513672\n",
      "step = 1312: Loss = 39.39690399169922\n",
      "step = 1313: Loss = 42.452213287353516\n",
      "step = 1314: Loss = 44.502784729003906\n",
      "step = 1315: Loss = 39.760536193847656\n",
      "step = 1316: Loss = 42.1185302734375\n",
      "step = 1317: Loss = 41.04502868652344\n",
      "step = 1318: Loss = 40.4386100769043\n",
      "step = 1319: Loss = 40.339698791503906\n",
      "step = 1320: Loss = 40.44908142089844\n",
      "step = 1321: Loss = 40.1591911315918\n",
      "step = 1322: Loss = 39.509891510009766\n",
      "step = 1323: Loss = 43.7276611328125\n",
      "step = 1324: Loss = 39.94148254394531\n",
      "step = 1325: Loss = 42.378360748291016\n",
      "step = 1326: Loss = 42.40196990966797\n",
      "step = 1327: Loss = 39.268558502197266\n",
      "step = 1328: Loss = 40.58038330078125\n",
      "step = 1329: Loss = 42.70054244995117\n",
      "step = 1330: Loss = 39.83656311035156\n",
      "step = 1331: Loss = 41.305397033691406\n",
      "step = 1332: Loss = 41.35747146606445\n",
      "step = 1333: Loss = 40.522682189941406\n",
      "step = 1334: Loss = 42.03936004638672\n",
      "step = 1335: Loss = 41.492408752441406\n",
      "step = 1336: Loss = 40.264190673828125\n",
      "step = 1337: Loss = 42.44152069091797\n",
      "step = 1338: Loss = 41.13670349121094\n",
      "step = 1339: Loss = 42.00404357910156\n",
      "step = 1340: Loss = 39.58179473876953\n",
      "step = 1341: Loss = 41.14643478393555\n",
      "step = 1342: Loss = 41.314064025878906\n",
      "step = 1343: Loss = 40.81324768066406\n",
      "step = 1344: Loss = 41.51963806152344\n",
      "step = 1345: Loss = 41.27843475341797\n",
      "step = 1346: Loss = 41.201087951660156\n",
      "step = 1347: Loss = 40.755035400390625\n",
      "step = 1348: Loss = 42.011592864990234\n",
      "step = 1349: Loss = 41.02922821044922\n",
      "step = 1350: Loss = 43.29998779296875\n",
      "step = 1351: Loss = 41.952667236328125\n",
      "step = 1352: Loss = 40.85029602050781\n",
      "step = 1353: Loss = 41.751373291015625\n",
      "step = 1354: Loss = 43.39104461669922\n",
      "step = 1355: Loss = 42.37016296386719\n",
      "step = 1356: Loss = 40.132713317871094\n",
      "step = 1357: Loss = 42.80992126464844\n",
      "step = 1358: Loss = 43.19298553466797\n",
      "step = 1359: Loss = 43.91490173339844\n",
      "step = 1360: Loss = 43.660125732421875\n",
      "step = 1361: Loss = 44.91781234741211\n",
      "step = 1362: Loss = 43.9125862121582\n",
      "step = 1363: Loss = 43.327754974365234\n",
      "step = 1364: Loss = 42.306644439697266\n",
      "step = 1365: Loss = 42.4783821105957\n",
      "step = 1366: Loss = 45.382686614990234\n",
      "step = 1367: Loss = 42.47731018066406\n",
      "step = 1368: Loss = 42.7175178527832\n",
      "step = 1369: Loss = 41.18094253540039\n",
      "step = 1370: Loss = 40.94378662109375\n",
      "step = 1371: Loss = 44.238365173339844\n",
      "step = 1372: Loss = 42.2121696472168\n",
      "step = 1373: Loss = 42.33027267456055\n",
      "step = 1374: Loss = 42.567752838134766\n",
      "step = 1375: Loss = 41.38036346435547\n",
      "step = 1376: Loss = 39.890281677246094\n",
      "step = 1377: Loss = 41.58064270019531\n",
      "step = 1378: Loss = 42.430198669433594\n",
      "step = 1379: Loss = 43.092811584472656\n",
      "step = 1380: Loss = 42.269832611083984\n",
      "step = 1381: Loss = 42.76438903808594\n",
      "step = 1382: Loss = 43.36473083496094\n",
      "step = 1383: Loss = 44.011741638183594\n",
      "step = 1384: Loss = 43.03754425048828\n",
      "step = 1385: Loss = 42.57992935180664\n",
      "step = 1386: Loss = 41.991458892822266\n",
      "step = 1387: Loss = 42.81861877441406\n",
      "step = 1388: Loss = 41.283180236816406\n",
      "step = 1389: Loss = 43.69345474243164\n",
      "step = 1390: Loss = 44.125587463378906\n",
      "step = 1391: Loss = 41.58850860595703\n",
      "step = 1392: Loss = 45.29026794433594\n",
      "step = 1393: Loss = 43.13386535644531\n",
      "step = 1394: Loss = 40.816349029541016\n",
      "step = 1395: Loss = 42.282840728759766\n",
      "step = 1396: Loss = 40.937705993652344\n",
      "step = 1397: Loss = 42.42675018310547\n",
      "step = 1398: Loss = 39.382965087890625\n",
      "step = 1399: Loss = 43.12837219238281\n",
      "step = 1400: Loss = 43.686546325683594\n",
      "step = 1401: Loss = 41.970619201660156\n",
      "step = 1402: Loss = 42.401817321777344\n",
      "step = 1403: Loss = 41.21015930175781\n",
      "step = 1404: Loss = 43.24494934082031\n",
      "step = 1405: Loss = 43.92510986328125\n",
      "step = 1406: Loss = 40.62395477294922\n",
      "step = 1407: Loss = 41.21977996826172\n",
      "step = 1408: Loss = 43.56928253173828\n",
      "step = 1409: Loss = 43.22869873046875\n",
      "step = 1410: Loss = 42.14704895019531\n",
      "step = 1411: Loss = 39.82597732543945\n",
      "step = 1412: Loss = 45.51979064941406\n",
      "step = 1413: Loss = 44.602783203125\n",
      "step = 1414: Loss = 42.40948486328125\n",
      "step = 1415: Loss = 43.96369934082031\n",
      "step = 1416: Loss = 44.55180358886719\n",
      "step = 1417: Loss = 43.870574951171875\n",
      "step = 1418: Loss = 42.821136474609375\n",
      "step = 1419: Loss = 43.593055725097656\n",
      "step = 1420: Loss = 43.09851837158203\n",
      "step = 1421: Loss = 44.086219787597656\n",
      "step = 1422: Loss = 41.077388763427734\n",
      "step = 1423: Loss = 41.961029052734375\n",
      "step = 1424: Loss = 40.83013916015625\n",
      "step = 1425: Loss = 44.90546417236328\n",
      "step = 1426: Loss = 44.61735534667969\n",
      "step = 1427: Loss = 43.309532165527344\n",
      "step = 1428: Loss = 44.918190002441406\n",
      "step = 1429: Loss = 43.55765914916992\n",
      "step = 1430: Loss = 43.39800262451172\n",
      "step = 1431: Loss = 45.029541015625\n",
      "step = 1432: Loss = 44.44582748413086\n",
      "step = 1433: Loss = 43.96232223510742\n",
      "step = 1434: Loss = 48.208030700683594\n",
      "step = 1435: Loss = 42.609283447265625\n",
      "step = 1436: Loss = 44.72420883178711\n",
      "step = 1437: Loss = 42.89457702636719\n",
      "step = 1438: Loss = 45.10710144042969\n",
      "step = 1439: Loss = 45.2492561340332\n",
      "step = 1440: Loss = 44.018310546875\n",
      "step = 1441: Loss = 44.03434753417969\n",
      "step = 1442: Loss = 44.94353485107422\n",
      "step = 1443: Loss = 43.12377166748047\n",
      "step = 1444: Loss = 45.494537353515625\n",
      "step = 1445: Loss = 39.532798767089844\n",
      "step = 1446: Loss = 41.788658142089844\n",
      "step = 1447: Loss = 44.560203552246094\n",
      "step = 1448: Loss = 45.79298782348633\n",
      "step = 1449: Loss = 40.761112213134766\n",
      "step = 1450: Loss = 46.01527404785156\n",
      "step = 1451: Loss = 43.49919891357422\n",
      "step = 1452: Loss = 43.48686599731445\n",
      "step = 1453: Loss = 42.612831115722656\n",
      "step = 1454: Loss = 44.68147277832031\n",
      "step = 1455: Loss = 44.353431701660156\n",
      "step = 1456: Loss = 46.01539611816406\n",
      "step = 1457: Loss = 45.527183532714844\n",
      "step = 1458: Loss = 46.088539123535156\n",
      "step = 1459: Loss = 47.07420349121094\n",
      "step = 1460: Loss = 42.92828369140625\n",
      "step = 1461: Loss = 45.18421936035156\n",
      "step = 1462: Loss = 45.097381591796875\n",
      "step = 1463: Loss = 44.214805603027344\n",
      "step = 1464: Loss = 46.93492126464844\n",
      "step = 1465: Loss = 44.0097541809082\n",
      "step = 1466: Loss = 45.841339111328125\n",
      "step = 1467: Loss = 44.631591796875\n",
      "step = 1468: Loss = 45.523216247558594\n",
      "step = 1469: Loss = 43.68105697631836\n",
      "step = 1470: Loss = 40.7402229309082\n",
      "step = 1471: Loss = 46.26569747924805\n",
      "step = 1472: Loss = 42.84657669067383\n",
      "step = 1473: Loss = 44.747215270996094\n",
      "step = 1474: Loss = 47.01604080200195\n",
      "step = 1475: Loss = 48.8448600769043\n",
      "step = 1476: Loss = 45.502105712890625\n",
      "step = 1477: Loss = 44.29767990112305\n",
      "step = 1478: Loss = 42.73728561401367\n",
      "step = 1479: Loss = 46.79383087158203\n",
      "step = 1480: Loss = 42.78091049194336\n",
      "step = 1481: Loss = 46.29662322998047\n",
      "step = 1482: Loss = 45.623817443847656\n",
      "step = 1483: Loss = 46.991783142089844\n",
      "step = 1484: Loss = 44.7495002746582\n",
      "step = 1485: Loss = 42.53684997558594\n",
      "step = 1486: Loss = 44.17594909667969\n",
      "step = 1487: Loss = 43.71086502075195\n",
      "step = 1488: Loss = 41.78270721435547\n",
      "step = 1489: Loss = 45.387325286865234\n",
      "step = 1490: Loss = 44.10796356201172\n",
      "step = 1491: Loss = 44.269622802734375\n",
      "step = 1492: Loss = 44.21490478515625\n",
      "step = 1493: Loss = 46.17681121826172\n",
      "step = 1494: Loss = 44.32783126831055\n",
      "step = 1495: Loss = 44.832584381103516\n",
      "step = 1496: Loss = 46.887596130371094\n",
      "step = 1497: Loss = 45.790794372558594\n",
      "step = 1498: Loss = 44.55393981933594\n",
      "step = 1499: Loss = 45.54133605957031\n",
      "step = 1500: Loss = 43.403106689453125\n",
      "step = 1501: Loss = 44.43524169921875\n",
      "step = 1502: Loss = 44.03288650512695\n",
      "step = 1503: Loss = 45.51250457763672\n",
      "step = 1504: Loss = 46.177345275878906\n",
      "step = 1505: Loss = 46.864742279052734\n",
      "step = 1506: Loss = 44.42634582519531\n",
      "step = 1507: Loss = 44.670570373535156\n",
      "step = 1508: Loss = 45.67814636230469\n",
      "step = 1509: Loss = 46.04833984375\n",
      "step = 1510: Loss = 45.76390838623047\n",
      "step = 1511: Loss = 45.67213821411133\n",
      "step = 1512: Loss = 44.85003662109375\n",
      "step = 1513: Loss = 46.322776794433594\n",
      "step = 1514: Loss = 45.33924865722656\n",
      "step = 1515: Loss = 45.926788330078125\n",
      "step = 1516: Loss = 43.82143783569336\n",
      "step = 1517: Loss = 46.33344268798828\n",
      "step = 1518: Loss = 45.679080963134766\n",
      "step = 1519: Loss = 46.118988037109375\n",
      "step = 1520: Loss = 44.83448791503906\n",
      "step = 1521: Loss = 48.23552703857422\n",
      "step = 1522: Loss = 46.8121337890625\n",
      "step = 1523: Loss = 43.77427291870117\n",
      "step = 1524: Loss = 46.08250045776367\n",
      "step = 1525: Loss = 44.45277404785156\n",
      "step = 1526: Loss = 47.856201171875\n",
      "step = 1527: Loss = 43.995948791503906\n",
      "step = 1528: Loss = 47.10152053833008\n",
      "step = 1529: Loss = 46.966033935546875\n",
      "step = 1530: Loss = 44.4478759765625\n",
      "step = 1531: Loss = 44.718345642089844\n",
      "step = 1532: Loss = 43.59268569946289\n",
      "step = 1533: Loss = 50.5970458984375\n",
      "step = 1534: Loss = 43.46195602416992\n",
      "step = 1535: Loss = 48.579994201660156\n",
      "step = 1536: Loss = 44.75028991699219\n",
      "step = 1537: Loss = 46.129966735839844\n",
      "step = 1538: Loss = 47.8269157409668\n",
      "step = 1539: Loss = 46.58076095581055\n",
      "step = 1540: Loss = 46.12602233886719\n",
      "step = 1541: Loss = 46.64915466308594\n",
      "step = 1542: Loss = 47.14888000488281\n",
      "step = 1543: Loss = 47.388641357421875\n",
      "step = 1544: Loss = 48.06557846069336\n",
      "step = 1545: Loss = 46.803466796875\n",
      "step = 1546: Loss = 45.53713607788086\n",
      "step = 1547: Loss = 47.5462646484375\n",
      "step = 1548: Loss = 45.83296203613281\n",
      "step = 1549: Loss = 42.358150482177734\n",
      "step = 1550: Loss = 47.679649353027344\n",
      "step = 1551: Loss = 44.949119567871094\n",
      "step = 1552: Loss = 45.52766799926758\n",
      "step = 1553: Loss = 48.63121032714844\n",
      "step = 1554: Loss = 46.327606201171875\n",
      "step = 1555: Loss = 47.41878128051758\n",
      "step = 1556: Loss = 46.83340835571289\n",
      "step = 1557: Loss = 47.81456756591797\n",
      "step = 1558: Loss = 45.57620620727539\n",
      "step = 1559: Loss = 45.17073059082031\n",
      "step = 1560: Loss = 47.33624267578125\n",
      "step = 1561: Loss = 48.824920654296875\n",
      "step = 1562: Loss = 48.00495147705078\n",
      "step = 1563: Loss = 47.683921813964844\n",
      "step = 1564: Loss = 47.769371032714844\n",
      "step = 1565: Loss = 49.6881217956543\n",
      "step = 1566: Loss = 46.49231719970703\n",
      "step = 1567: Loss = 46.26121520996094\n",
      "step = 1568: Loss = 48.44917297363281\n",
      "step = 1569: Loss = 46.10179901123047\n",
      "step = 1570: Loss = 46.616065979003906\n",
      "step = 1571: Loss = 47.06353759765625\n",
      "step = 1572: Loss = 47.7899169921875\n",
      "step = 1573: Loss = 45.524696350097656\n",
      "step = 1574: Loss = 48.35870361328125\n",
      "step = 1575: Loss = 45.23257827758789\n",
      "step = 1576: Loss = 46.72063064575195\n",
      "step = 1577: Loss = 46.55870056152344\n",
      "step = 1578: Loss = 49.182159423828125\n",
      "step = 1579: Loss = 46.93337631225586\n",
      "step = 1580: Loss = 48.02770233154297\n",
      "step = 1581: Loss = 47.96210479736328\n",
      "step = 1582: Loss = 45.49945831298828\n",
      "step = 1583: Loss = 50.71712875366211\n",
      "step = 1584: Loss = 47.012237548828125\n",
      "step = 1585: Loss = 46.00359344482422\n",
      "step = 1586: Loss = 46.81483840942383\n",
      "step = 1587: Loss = 48.07305145263672\n",
      "step = 1588: Loss = 46.67420196533203\n",
      "step = 1589: Loss = 48.20143508911133\n",
      "step = 1590: Loss = 49.54756164550781\n",
      "step = 1591: Loss = 48.029090881347656\n",
      "step = 1592: Loss = 50.959476470947266\n",
      "step = 1593: Loss = 48.33082962036133\n",
      "step = 1594: Loss = 47.437828063964844\n",
      "step = 1595: Loss = 47.92510223388672\n",
      "step = 1596: Loss = 47.898170471191406\n",
      "step = 1597: Loss = 47.436248779296875\n",
      "step = 1598: Loss = 49.95857238769531\n",
      "step = 1599: Loss = 48.86970520019531\n",
      "step = 1600: Loss = 48.45577621459961\n",
      "step = 1601: Loss = 48.182716369628906\n",
      "step = 1602: Loss = 48.0361328125\n",
      "step = 1603: Loss = 47.2083854675293\n",
      "step = 1604: Loss = 47.69601058959961\n",
      "step = 1605: Loss = 47.643890380859375\n",
      "step = 1606: Loss = 48.000823974609375\n",
      "step = 1607: Loss = 47.17950439453125\n",
      "step = 1608: Loss = 48.491539001464844\n",
      "step = 1609: Loss = 48.421810150146484\n",
      "step = 1610: Loss = 50.61955261230469\n",
      "step = 1611: Loss = 48.024497985839844\n",
      "step = 1612: Loss = 48.86686706542969\n",
      "step = 1613: Loss = 45.446720123291016\n",
      "step = 1614: Loss = 47.9605712890625\n",
      "step = 1615: Loss = 48.985504150390625\n",
      "step = 1616: Loss = 49.608943939208984\n",
      "step = 1617: Loss = 46.21164321899414\n",
      "step = 1618: Loss = 48.90204620361328\n",
      "step = 1619: Loss = 46.448570251464844\n",
      "step = 1620: Loss = 49.30305480957031\n",
      "step = 1621: Loss = 48.99430847167969\n",
      "step = 1622: Loss = 46.26873016357422\n",
      "step = 1623: Loss = 45.32548522949219\n",
      "step = 1624: Loss = 47.843994140625\n",
      "step = 1625: Loss = 48.780235290527344\n",
      "step = 1626: Loss = 47.15985107421875\n",
      "step = 1627: Loss = 51.548805236816406\n",
      "step = 1628: Loss = 49.0850944519043\n",
      "step = 1629: Loss = 47.9947624206543\n",
      "step = 1630: Loss = 51.41438293457031\n",
      "step = 1631: Loss = 50.25074005126953\n",
      "step = 1632: Loss = 51.512001037597656\n",
      "step = 1633: Loss = 47.964019775390625\n",
      "step = 1634: Loss = 48.243858337402344\n",
      "step = 1635: Loss = 51.00043487548828\n",
      "step = 1636: Loss = 47.77376174926758\n",
      "step = 1637: Loss = 51.20549011230469\n",
      "step = 1638: Loss = 48.11585998535156\n",
      "step = 1639: Loss = 50.75008773803711\n",
      "step = 1640: Loss = 48.351280212402344\n",
      "step = 1641: Loss = 48.56756591796875\n",
      "step = 1642: Loss = 49.81697082519531\n",
      "step = 1643: Loss = 48.65076446533203\n",
      "step = 1644: Loss = 47.456825256347656\n",
      "step = 1645: Loss = 47.92057418823242\n",
      "step = 1646: Loss = 51.82359313964844\n",
      "step = 1647: Loss = 48.17518615722656\n",
      "step = 1648: Loss = 48.88168716430664\n",
      "step = 1649: Loss = 49.7703742980957\n",
      "step = 1650: Loss = 49.36296844482422\n",
      "step = 1651: Loss = 49.40889358520508\n",
      "step = 1652: Loss = 49.38068771362305\n",
      "step = 1653: Loss = 52.828399658203125\n",
      "step = 1654: Loss = 47.79811096191406\n",
      "step = 1655: Loss = 48.55805206298828\n",
      "step = 1656: Loss = 50.98164367675781\n",
      "step = 1657: Loss = 46.822635650634766\n",
      "step = 1658: Loss = 49.01508712768555\n",
      "step = 1659: Loss = 51.737281799316406\n",
      "step = 1660: Loss = 46.69806671142578\n",
      "step = 1661: Loss = 49.70817565917969\n",
      "step = 1662: Loss = 47.975242614746094\n",
      "step = 1663: Loss = 47.2403564453125\n",
      "step = 1664: Loss = 47.38721466064453\n",
      "step = 1665: Loss = 50.222049713134766\n",
      "step = 1666: Loss = 50.02690887451172\n",
      "step = 1667: Loss = 50.86628723144531\n",
      "step = 1668: Loss = 49.010536193847656\n",
      "step = 1669: Loss = 48.37629699707031\n",
      "step = 1670: Loss = 47.11407470703125\n",
      "step = 1671: Loss = 50.22217559814453\n",
      "step = 1672: Loss = 51.13056182861328\n",
      "step = 1673: Loss = 50.89778137207031\n",
      "step = 1674: Loss = 50.512210845947266\n",
      "step = 1675: Loss = 46.994346618652344\n",
      "step = 1676: Loss = 47.710384368896484\n",
      "step = 1677: Loss = 50.40366744995117\n",
      "step = 1678: Loss = 49.93317413330078\n",
      "step = 1679: Loss = 50.0361328125\n",
      "step = 1680: Loss = 52.4117546081543\n",
      "step = 1681: Loss = 49.501739501953125\n",
      "step = 1682: Loss = 50.042545318603516\n",
      "step = 1683: Loss = 50.93311309814453\n",
      "step = 1684: Loss = 50.75756072998047\n",
      "step = 1685: Loss = 50.56209182739258\n",
      "step = 1686: Loss = 50.95872497558594\n",
      "step = 1687: Loss = 49.47688293457031\n",
      "step = 1688: Loss = 47.68091583251953\n",
      "step = 1689: Loss = 50.05589294433594\n",
      "step = 1690: Loss = 50.0050163269043\n",
      "step = 1691: Loss = 48.43381118774414\n",
      "step = 1692: Loss = 52.60096740722656\n",
      "step = 1693: Loss = 51.505882263183594\n",
      "step = 1694: Loss = 49.470436096191406\n",
      "step = 1695: Loss = 49.84267044067383\n",
      "step = 1696: Loss = 52.43663787841797\n",
      "step = 1697: Loss = 50.17530822753906\n",
      "step = 1698: Loss = 51.429805755615234\n",
      "step = 1699: Loss = 51.11497116088867\n",
      "step = 1700: Loss = 49.66455841064453\n",
      "step = 1701: Loss = 51.55840301513672\n",
      "step = 1702: Loss = 52.91510009765625\n",
      "step = 1703: Loss = 51.247528076171875\n",
      "step = 1704: Loss = 49.23171615600586\n",
      "step = 1705: Loss = 51.43838119506836\n",
      "step = 1706: Loss = 51.732177734375\n",
      "step = 1707: Loss = 48.09520721435547\n",
      "step = 1708: Loss = 53.3028450012207\n",
      "step = 1709: Loss = 49.24183654785156\n",
      "step = 1710: Loss = 49.30250549316406\n",
      "step = 1711: Loss = 51.32133865356445\n",
      "step = 1712: Loss = 49.499427795410156\n",
      "step = 1713: Loss = 49.309814453125\n",
      "step = 1714: Loss = 50.032920837402344\n",
      "step = 1715: Loss = 48.94586181640625\n",
      "step = 1716: Loss = 51.829654693603516\n",
      "step = 1717: Loss = 50.117347717285156\n",
      "step = 1718: Loss = 52.35814666748047\n",
      "step = 1719: Loss = 52.156578063964844\n",
      "step = 1720: Loss = 52.150123596191406\n",
      "step = 1721: Loss = 50.10052490234375\n",
      "step = 1722: Loss = 49.03700637817383\n",
      "step = 1723: Loss = 53.38349914550781\n",
      "step = 1724: Loss = 50.78215026855469\n",
      "step = 1725: Loss = 53.94488525390625\n",
      "step = 1726: Loss = 50.30475997924805\n",
      "step = 1727: Loss = 51.2835807800293\n",
      "step = 1728: Loss = 54.00061798095703\n",
      "step = 1729: Loss = 49.232757568359375\n",
      "step = 1730: Loss = 53.86573028564453\n",
      "step = 1731: Loss = 52.439422607421875\n",
      "step = 1732: Loss = 51.77610778808594\n",
      "step = 1733: Loss = 52.724517822265625\n",
      "step = 1734: Loss = 49.56172561645508\n",
      "step = 1735: Loss = 50.400611877441406\n",
      "step = 1736: Loss = 51.461456298828125\n",
      "step = 1737: Loss = 50.96416473388672\n",
      "step = 1738: Loss = 49.34474182128906\n",
      "step = 1739: Loss = 51.138954162597656\n",
      "step = 1740: Loss = 52.34271240234375\n",
      "step = 1741: Loss = 50.74370193481445\n",
      "step = 1742: Loss = 54.028995513916016\n",
      "step = 1743: Loss = 49.82573699951172\n",
      "step = 1744: Loss = 51.33069610595703\n",
      "step = 1745: Loss = 50.3386344909668\n",
      "step = 1746: Loss = 50.4853515625\n",
      "step = 1747: Loss = 49.81440734863281\n",
      "step = 1748: Loss = 51.936683654785156\n",
      "step = 1749: Loss = 51.427391052246094\n",
      "step = 1750: Loss = 52.77341079711914\n",
      "step = 1751: Loss = 49.87709045410156\n",
      "step = 1752: Loss = 49.80332946777344\n",
      "step = 1753: Loss = 47.89136505126953\n",
      "step = 1754: Loss = 52.6226692199707\n",
      "step = 1755: Loss = 51.699344635009766\n",
      "step = 1756: Loss = 50.18586349487305\n",
      "step = 1757: Loss = 50.108158111572266\n",
      "step = 1758: Loss = 50.64623260498047\n",
      "step = 1759: Loss = 52.14389419555664\n",
      "step = 1760: Loss = 52.42496871948242\n",
      "step = 1761: Loss = 48.86277770996094\n",
      "step = 1762: Loss = 52.24702453613281\n",
      "step = 1763: Loss = 51.41790771484375\n",
      "step = 1764: Loss = 50.583011627197266\n",
      "step = 1765: Loss = 53.6329345703125\n",
      "step = 1766: Loss = 53.21355056762695\n",
      "step = 1767: Loss = 51.015296936035156\n",
      "step = 1768: Loss = 48.601749420166016\n",
      "step = 1769: Loss = 52.17201614379883\n",
      "step = 1770: Loss = 51.46586608886719\n",
      "step = 1771: Loss = 51.8781623840332\n",
      "step = 1772: Loss = 51.98550796508789\n",
      "step = 1773: Loss = 52.15663146972656\n",
      "step = 1774: Loss = 53.25355529785156\n",
      "step = 1775: Loss = 51.83643341064453\n",
      "step = 1776: Loss = 52.697784423828125\n",
      "step = 1777: Loss = 54.16099548339844\n",
      "step = 1778: Loss = 51.634639739990234\n",
      "step = 1779: Loss = 53.71554183959961\n",
      "step = 1780: Loss = 50.949928283691406\n",
      "step = 1781: Loss = 51.48754119873047\n",
      "step = 1782: Loss = 50.84967803955078\n",
      "step = 1783: Loss = 54.02166748046875\n",
      "step = 1784: Loss = 52.370811462402344\n",
      "step = 1785: Loss = 51.36555099487305\n",
      "step = 1786: Loss = 49.93286895751953\n",
      "step = 1787: Loss = 51.726531982421875\n",
      "step = 1788: Loss = 51.35350799560547\n",
      "step = 1789: Loss = 50.777740478515625\n",
      "step = 1790: Loss = 53.639320373535156\n",
      "step = 1791: Loss = 51.88353729248047\n",
      "step = 1792: Loss = 52.8665771484375\n",
      "step = 1793: Loss = 52.192283630371094\n",
      "step = 1794: Loss = 54.8286247253418\n",
      "step = 1795: Loss = 55.83127975463867\n",
      "step = 1796: Loss = 53.2071533203125\n",
      "step = 1797: Loss = 51.60370635986328\n",
      "step = 1798: Loss = 54.001617431640625\n",
      "step = 1799: Loss = 55.86819839477539\n",
      "step = 1800: Loss = 51.85885238647461\n",
      "step = 1801: Loss = 52.12712097167969\n",
      "step = 1802: Loss = 48.858917236328125\n",
      "step = 1803: Loss = 53.10249710083008\n",
      "step = 1804: Loss = 54.949954986572266\n",
      "step = 1805: Loss = 51.652610778808594\n",
      "step = 1806: Loss = 52.761878967285156\n",
      "step = 1807: Loss = 52.08479690551758\n",
      "step = 1808: Loss = 49.91289520263672\n",
      "step = 1809: Loss = 52.972381591796875\n",
      "step = 1810: Loss = 53.41394805908203\n",
      "step = 1811: Loss = 51.20149612426758\n",
      "step = 1812: Loss = 51.681114196777344\n",
      "step = 1813: Loss = 52.90751647949219\n",
      "step = 1814: Loss = 50.51092529296875\n",
      "step = 1815: Loss = 51.41230010986328\n",
      "step = 1816: Loss = 53.006858825683594\n",
      "step = 1817: Loss = 54.94599151611328\n",
      "step = 1818: Loss = 55.18152618408203\n",
      "step = 1819: Loss = 55.25146484375\n",
      "step = 1820: Loss = 53.331966400146484\n",
      "step = 1821: Loss = 49.44526290893555\n",
      "step = 1822: Loss = 52.77375793457031\n",
      "step = 1823: Loss = 52.14442443847656\n",
      "step = 1824: Loss = 55.15397644042969\n",
      "step = 1825: Loss = 52.224830627441406\n",
      "step = 1826: Loss = 52.28807067871094\n",
      "step = 1827: Loss = 51.404327392578125\n",
      "step = 1828: Loss = 55.49451446533203\n",
      "step = 1829: Loss = 52.74748611450195\n",
      "step = 1830: Loss = 52.14771270751953\n",
      "step = 1831: Loss = 52.87799072265625\n",
      "step = 1832: Loss = 52.67856979370117\n",
      "step = 1833: Loss = 53.94328308105469\n",
      "step = 1834: Loss = 50.80039978027344\n",
      "step = 1835: Loss = 55.26681137084961\n",
      "step = 1836: Loss = 54.59478759765625\n",
      "step = 1837: Loss = 53.17079162597656\n",
      "step = 1838: Loss = 52.77909851074219\n",
      "step = 1839: Loss = 52.313228607177734\n",
      "step = 1840: Loss = 52.31403350830078\n",
      "step = 1841: Loss = 52.59522247314453\n",
      "step = 1842: Loss = 52.574501037597656\n",
      "step = 1843: Loss = 52.102447509765625\n",
      "step = 1844: Loss = 53.4942741394043\n",
      "step = 1845: Loss = 53.634361267089844\n",
      "step = 1846: Loss = 50.668853759765625\n",
      "step = 1847: Loss = 53.98080062866211\n",
      "step = 1848: Loss = 52.23347854614258\n",
      "step = 1849: Loss = 54.76287841796875\n",
      "step = 1850: Loss = 49.760406494140625\n",
      "step = 1851: Loss = 53.447139739990234\n",
      "step = 1852: Loss = 53.83342742919922\n",
      "step = 1853: Loss = 50.574554443359375\n",
      "step = 1854: Loss = 52.047454833984375\n",
      "step = 1855: Loss = 52.39398193359375\n",
      "step = 1856: Loss = 52.790618896484375\n",
      "step = 1857: Loss = 52.609222412109375\n",
      "step = 1858: Loss = 53.54854202270508\n",
      "step = 1859: Loss = 53.25929641723633\n",
      "step = 1860: Loss = 55.03913497924805\n",
      "step = 1861: Loss = 56.41535186767578\n",
      "step = 1862: Loss = 54.95077896118164\n",
      "step = 1863: Loss = 55.039188385009766\n",
      "step = 1864: Loss = 52.71339416503906\n",
      "step = 1865: Loss = 55.21087646484375\n",
      "step = 1866: Loss = 55.28339385986328\n",
      "step = 1867: Loss = 53.525428771972656\n",
      "step = 1868: Loss = 52.42167282104492\n",
      "step = 1869: Loss = 54.41047286987305\n",
      "step = 1870: Loss = 51.437381744384766\n",
      "step = 1871: Loss = 55.58293914794922\n",
      "step = 1872: Loss = 54.458038330078125\n",
      "step = 1873: Loss = 52.545928955078125\n",
      "step = 1874: Loss = 49.89436340332031\n",
      "step = 1875: Loss = 54.68592834472656\n",
      "step = 1876: Loss = 53.502174377441406\n",
      "step = 1877: Loss = 53.589622497558594\n",
      "step = 1878: Loss = 58.310462951660156\n",
      "step = 1879: Loss = 49.9766731262207\n",
      "step = 1880: Loss = 52.10192108154297\n",
      "step = 1881: Loss = 52.99199295043945\n",
      "step = 1882: Loss = 56.36998748779297\n",
      "step = 1883: Loss = 54.65434265136719\n",
      "step = 1884: Loss = 51.523746490478516\n",
      "step = 1885: Loss = 54.81095886230469\n",
      "step = 1886: Loss = 54.23662185668945\n",
      "step = 1887: Loss = 56.079742431640625\n",
      "step = 1888: Loss = 51.882904052734375\n",
      "step = 1889: Loss = 55.56689453125\n",
      "step = 1890: Loss = 55.81755065917969\n",
      "step = 1891: Loss = 55.76436233520508\n",
      "step = 1892: Loss = 49.900089263916016\n",
      "step = 1893: Loss = 55.42700958251953\n",
      "step = 1894: Loss = 51.7013053894043\n",
      "step = 1895: Loss = 56.12763214111328\n",
      "step = 1896: Loss = 53.63481521606445\n",
      "step = 1897: Loss = 54.36037063598633\n",
      "step = 1898: Loss = 53.301185607910156\n",
      "step = 1899: Loss = 54.09084701538086\n",
      "step = 1900: Loss = 54.46205139160156\n",
      "step = 1901: Loss = 55.00434112548828\n",
      "step = 1902: Loss = 53.80851364135742\n",
      "step = 1903: Loss = 57.70037078857422\n",
      "step = 1904: Loss = 54.280784606933594\n",
      "step = 1905: Loss = 56.36280059814453\n",
      "step = 1906: Loss = 55.114036560058594\n",
      "step = 1907: Loss = 52.64999771118164\n",
      "step = 1908: Loss = 54.06634521484375\n",
      "step = 1909: Loss = 51.70856475830078\n",
      "step = 1910: Loss = 55.13624954223633\n",
      "step = 1911: Loss = 55.04088592529297\n",
      "step = 1912: Loss = 53.327720642089844\n",
      "step = 1913: Loss = 56.622802734375\n",
      "step = 1914: Loss = 53.27439498901367\n",
      "step = 1915: Loss = 54.31527328491211\n",
      "step = 1916: Loss = 52.873863220214844\n",
      "step = 1917: Loss = 52.50711441040039\n",
      "step = 1918: Loss = 54.29420471191406\n",
      "step = 1919: Loss = 56.73896026611328\n",
      "step = 1920: Loss = 57.89392852783203\n",
      "step = 1921: Loss = 56.665992736816406\n",
      "step = 1922: Loss = 52.64704132080078\n",
      "step = 1923: Loss = 58.32332992553711\n",
      "step = 1924: Loss = 55.36747741699219\n",
      "step = 1925: Loss = 56.12912368774414\n",
      "step = 1926: Loss = 54.27762222290039\n",
      "step = 1927: Loss = 58.56425476074219\n",
      "step = 1928: Loss = 54.73130416870117\n",
      "step = 1929: Loss = 55.036598205566406\n",
      "step = 1930: Loss = 53.52467346191406\n",
      "step = 1931: Loss = 54.49313735961914\n",
      "step = 1932: Loss = 56.69584655761719\n",
      "step = 1933: Loss = 53.74320983886719\n",
      "step = 1934: Loss = 56.51966857910156\n",
      "step = 1935: Loss = 54.23041534423828\n",
      "step = 1936: Loss = 56.06181716918945\n",
      "step = 1937: Loss = 55.873130798339844\n",
      "step = 1938: Loss = 54.51423645019531\n",
      "step = 1939: Loss = 56.15249252319336\n",
      "step = 1940: Loss = 54.37847900390625\n",
      "step = 1941: Loss = 53.74197769165039\n",
      "step = 1942: Loss = 55.057621002197266\n",
      "step = 1943: Loss = 54.30668258666992\n",
      "step = 1944: Loss = 51.59547424316406\n",
      "step = 1945: Loss = 56.91022872924805\n",
      "step = 1946: Loss = 57.2955322265625\n",
      "step = 1947: Loss = 54.36948013305664\n",
      "step = 1948: Loss = 56.96839141845703\n",
      "step = 1949: Loss = 54.4568977355957\n",
      "step = 1950: Loss = 56.93866729736328\n",
      "step = 1951: Loss = 54.070831298828125\n",
      "step = 1952: Loss = 53.61009979248047\n",
      "step = 1953: Loss = 55.06760787963867\n",
      "step = 1954: Loss = 55.241905212402344\n",
      "step = 1955: Loss = 55.35088348388672\n",
      "step = 1956: Loss = 56.28172302246094\n",
      "step = 1957: Loss = 54.37651824951172\n",
      "step = 1958: Loss = 55.20161437988281\n",
      "step = 1959: Loss = 54.84879684448242\n",
      "step = 1960: Loss = 54.07195281982422\n",
      "step = 1961: Loss = 58.62861633300781\n",
      "step = 1962: Loss = 58.565635681152344\n",
      "step = 1963: Loss = 56.411155700683594\n",
      "step = 1964: Loss = 58.476219177246094\n",
      "step = 1965: Loss = 57.611976623535156\n",
      "step = 1966: Loss = 55.1974983215332\n",
      "step = 1967: Loss = 55.08396911621094\n",
      "step = 1968: Loss = 54.019737243652344\n",
      "step = 1969: Loss = 53.890499114990234\n",
      "step = 1970: Loss = 58.85591125488281\n",
      "step = 1971: Loss = 55.29325866699219\n",
      "step = 1972: Loss = 57.39556884765625\n",
      "step = 1973: Loss = 55.76907730102539\n",
      "step = 1974: Loss = 52.391902923583984\n",
      "step = 1975: Loss = 57.57384490966797\n",
      "step = 1976: Loss = 54.07938003540039\n",
      "step = 1977: Loss = 54.15206527709961\n",
      "step = 1978: Loss = 54.939239501953125\n",
      "step = 1979: Loss = 57.3535041809082\n",
      "step = 1980: Loss = 58.111427307128906\n",
      "step = 1981: Loss = 57.39612579345703\n",
      "step = 1982: Loss = 55.41966247558594\n",
      "step = 1983: Loss = 58.397789001464844\n",
      "step = 1984: Loss = 56.93808364868164\n",
      "step = 1985: Loss = 59.69178771972656\n",
      "step = 1986: Loss = 56.70368194580078\n",
      "step = 1987: Loss = 51.53863525390625\n",
      "step = 1988: Loss = 56.36585235595703\n",
      "step = 1989: Loss = 57.29605484008789\n",
      "step = 1990: Loss = 57.879615783691406\n",
      "step = 1991: Loss = 55.91062545776367\n",
      "step = 1992: Loss = 57.97615432739258\n",
      "step = 1993: Loss = 57.05210494995117\n",
      "step = 1994: Loss = 54.295448303222656\n",
      "step = 1995: Loss = 54.811920166015625\n",
      "step = 1996: Loss = 57.46833038330078\n",
      "step = 1997: Loss = 57.96995162963867\n",
      "step = 1998: Loss = 52.958961486816406\n",
      "step = 1999: Loss = 58.581321716308594\n",
      "step = 2000: Loss = 57.926300048828125\n",
      "step = 2001: Loss = 55.73002243041992\n",
      "step = 2002: Loss = 55.714542388916016\n",
      "step = 2003: Loss = 54.55339050292969\n",
      "step = 2004: Loss = 56.6329231262207\n",
      "step = 2005: Loss = 54.86381530761719\n",
      "step = 2006: Loss = 58.30307388305664\n",
      "step = 2007: Loss = 56.810935974121094\n",
      "step = 2008: Loss = 55.09992980957031\n",
      "step = 2009: Loss = 56.76725769042969\n",
      "step = 2010: Loss = 52.019691467285156\n",
      "step = 2011: Loss = 53.9041748046875\n",
      "step = 2012: Loss = 57.71405792236328\n",
      "step = 2013: Loss = 59.78923416137695\n",
      "step = 2014: Loss = 53.5570068359375\n",
      "step = 2015: Loss = 59.83330154418945\n",
      "step = 2016: Loss = 54.30126190185547\n",
      "step = 2017: Loss = 56.1064453125\n",
      "step = 2018: Loss = 59.034515380859375\n",
      "step = 2019: Loss = 56.46441650390625\n",
      "step = 2020: Loss = 57.634742736816406\n",
      "step = 2021: Loss = 59.5345344543457\n",
      "step = 2022: Loss = 59.036346435546875\n",
      "step = 2023: Loss = 53.948341369628906\n",
      "step = 2024: Loss = 58.63890838623047\n",
      "step = 2025: Loss = 61.61847686767578\n",
      "step = 2026: Loss = 62.08876037597656\n",
      "step = 2027: Loss = 59.808345794677734\n",
      "step = 2028: Loss = 57.79534912109375\n",
      "step = 2029: Loss = 55.362548828125\n",
      "step = 2030: Loss = 58.0587272644043\n",
      "step = 2031: Loss = 56.06196212768555\n",
      "step = 2032: Loss = 57.497615814208984\n",
      "step = 2033: Loss = 56.53142547607422\n",
      "step = 2034: Loss = 58.10071563720703\n",
      "step = 2035: Loss = 57.3370246887207\n",
      "step = 2036: Loss = 61.11899185180664\n",
      "step = 2037: Loss = 56.59147262573242\n",
      "step = 2038: Loss = 53.48308563232422\n",
      "step = 2039: Loss = 58.108367919921875\n",
      "step = 2040: Loss = 59.09051513671875\n",
      "step = 2041: Loss = 59.16569519042969\n",
      "step = 2042: Loss = 56.843170166015625\n",
      "step = 2043: Loss = 55.43033981323242\n",
      "step = 2044: Loss = 57.71544647216797\n",
      "step = 2045: Loss = 57.85865783691406\n",
      "step = 2046: Loss = 58.46851348876953\n",
      "step = 2047: Loss = 59.171852111816406\n",
      "step = 2048: Loss = 55.26839065551758\n",
      "step = 2049: Loss = 59.38692855834961\n",
      "step = 2050: Loss = 58.214412689208984\n",
      "step = 2051: Loss = 58.314903259277344\n",
      "step = 2052: Loss = 56.489540100097656\n",
      "step = 2053: Loss = 57.82362747192383\n",
      "step = 2054: Loss = 58.728355407714844\n",
      "step = 2055: Loss = 53.25397491455078\n",
      "step = 2056: Loss = 59.796104431152344\n",
      "step = 2057: Loss = 59.933685302734375\n",
      "step = 2058: Loss = 56.92533874511719\n",
      "step = 2059: Loss = 54.469390869140625\n",
      "step = 2060: Loss = 59.661231994628906\n",
      "step = 2061: Loss = 58.59023666381836\n",
      "step = 2062: Loss = 52.94291305541992\n",
      "step = 2063: Loss = 58.6175651550293\n",
      "step = 2064: Loss = 58.85871887207031\n",
      "step = 2065: Loss = 58.73589324951172\n",
      "step = 2066: Loss = 60.26959991455078\n",
      "step = 2067: Loss = 56.942440032958984\n",
      "step = 2068: Loss = 58.55108642578125\n",
      "step = 2069: Loss = 59.148399353027344\n",
      "step = 2070: Loss = 58.507568359375\n",
      "step = 2071: Loss = 55.65633773803711\n",
      "step = 2072: Loss = 54.569183349609375\n",
      "step = 2073: Loss = 55.945106506347656\n",
      "step = 2074: Loss = 57.70133972167969\n",
      "step = 2075: Loss = 60.99788284301758\n",
      "step = 2076: Loss = 60.31208801269531\n",
      "step = 2077: Loss = 57.25927734375\n",
      "step = 2078: Loss = 59.24184799194336\n",
      "step = 2079: Loss = 57.55322265625\n",
      "step = 2080: Loss = 62.866188049316406\n",
      "step = 2081: Loss = 59.55280303955078\n",
      "step = 2082: Loss = 59.97477340698242\n",
      "step = 2083: Loss = 57.07213592529297\n",
      "step = 2084: Loss = 61.60817337036133\n",
      "step = 2085: Loss = 57.308380126953125\n",
      "step = 2086: Loss = 57.81734848022461\n",
      "step = 2087: Loss = 59.19969940185547\n",
      "step = 2088: Loss = 55.3096809387207\n",
      "step = 2089: Loss = 59.053314208984375\n",
      "step = 2090: Loss = 58.462799072265625\n",
      "step = 2091: Loss = 55.32774353027344\n",
      "step = 2092: Loss = 55.59001922607422\n",
      "step = 2093: Loss = 56.01390075683594\n",
      "step = 2094: Loss = 58.40753173828125\n",
      "step = 2095: Loss = 57.31539535522461\n",
      "step = 2096: Loss = 60.667747497558594\n",
      "step = 2097: Loss = 55.846336364746094\n",
      "step = 2098: Loss = 57.998046875\n",
      "step = 2099: Loss = 60.831756591796875\n",
      "step = 2100: Loss = 61.43647766113281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\TD3.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m global_step\u001b[39m.\u001b[39mnumpy() \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     train_checkpointer\u001b[39m.\u001b[39msave(global_step)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     metric_utils\u001b[39m.\u001b[39;49meager_compute(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         eval_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         tf_env_eval,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         eval_policy,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         num_episodes\u001b[39m=\u001b[39;49mnum_eval_episodes,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         train_step\u001b[39m=\u001b[39;49mglobal_step,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         summary_writer\u001b[39m=\u001b[39;49meval_summary_writer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Federated-Reinforcement-Learning-for-Battery-Charging/src/notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         summary_prefix\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMetrics\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\gin\\config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39mnew_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\eval\\metric_utils.py:164\u001b[0m, in \u001b[0;36meager_compute\u001b[1;34m(metrics, environment, policy, num_episodes, train_step, summary_writer, summary_prefix, use_function)\u001b[0m\n\u001b[0;32m    160\u001b[0m driver \u001b[39m=\u001b[39m dynamic_episode_driver\u001b[39m.\u001b[39mDynamicEpisodeDriver(\n\u001b[0;32m    161\u001b[0m     environment, policy, observers\u001b[39m=\u001b[39mmetrics, num_episodes\u001b[39m=\u001b[39mnum_episodes\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m use_function:\n\u001b[1;32m--> 164\u001b[0m   common\u001b[39m.\u001b[39;49mfunction(driver\u001b[39m.\u001b[39;49mrun)(time_step, policy_state)\n\u001b[0;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m   driver\u001b[39m.\u001b[39mrun(time_step, policy_state)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:918\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m   \u001b[39m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m   filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    914\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(\n\u001b[0;32m    915\u001b[0m           bound_args\n\u001b[0;32m    916\u001b[0m       )\n\u001b[0;32m    917\u001b[0m   )\n\u001b[1;32m--> 918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concrete_variable_creation_fn\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    919\u001b[0m       filtered_flat_args,\n\u001b[0;32m    920\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concrete_variable_creation_fn\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[0;32m    921\u001b[0m   )\n\u001b[0;32m    923\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[0;32m    924\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.compat.v2.summary.record_if(True):\n",
    "    metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        tf_env_eval,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics')\n",
    "\n",
    "    # Train and evaluate\n",
    "    while global_step.numpy() <= num_iterations:\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "        experience, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience)\n",
    "        print('step = {0}: Loss = {1}'.format(global_step.numpy(), train_loss.loss))\n",
    "        with eval_summary_writer.as_default():\n",
    "            tf.summary.scalar(name='loss', data=train_loss.loss, step=global_step)\n",
    "        if global_step.numpy() % eval_interval == 0:\n",
    "            train_checkpointer.save(global_step)\n",
    "            metric_utils.eager_compute(\n",
    "                eval_metrics,\n",
    "                tf_env_eval,\n",
    "                eval_policy,\n",
    "                num_episodes=num_eval_episodes,\n",
    "                train_step=global_step,\n",
    "                summary_writer=eval_summary_writer,\n",
    "                summary_prefix='Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "data_test = Dataloader.get_customer_data(Dataloader.loadData('./data/load1213.csv'),\n",
    "                                         Dataloader.loadPrice('./data/price.csv'), 1)\n",
    "tf_env_test = tf_py_environment.TFPyEnvironment(Environment.Environment(init_charge=10.0, data=data_test, test=True))\n",
    "time_step_test = tf_env_test.reset()\n",
    "\n",
    "while not time_step_test.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step_test)\n",
    "    time_step_test = tf_env_test.step(action_step.action)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
