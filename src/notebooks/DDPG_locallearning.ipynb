{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment, py_environment, batched_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from environments.EnergyManagementEnv import EnergyManagementEnv\n",
    "from utils.agentNetworks import ActorNetwork, CriticNetwork, CustomLayers\n",
    "import utils.dataloader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "State Space: 6, Action Space: 1\n",
      "Upper bound: 2.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_1</th>\n",
       "      <th>pv_1</th>\n",
       "      <th>price</th>\n",
       "      <th>fuelmix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05704</td>\n",
       "      <td>0.530991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   load_1  pv_1    price   fuelmix\n",
       "0   1.149   0.0  0.05704  0.530991"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and setup environments\n",
    "\n",
    "num_buildings = 30\n",
    "energy_data = pd.read_csv(\"../../data/3final_data/Final_Energy_dataset.csv\", header=0)\n",
    "energy_data.set_index('Date', inplace=True)\n",
    "energy_data.fillna(0, inplace=True)\n",
    "\n",
    "dataset = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "environments = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for idx in range(num_buildings):\n",
    "    user_data = energy_data[[f'load_{idx+1}', f'pv_{idx+1}', 'price', 'fuelmix']]\n",
    "    \n",
    "    dataset[\"train\"][f\"building_{idx+1}\"] = user_data[0:17520].set_index(pd.RangeIndex(0,17520))\n",
    "    dataset[\"eval\"][f\"building_{idx+1}\"] = user_data[17520:35088].set_index(pd.RangeIndex(0,17568))\n",
    "    dataset[\"test\"][f\"building_{idx+1}\"] = user_data[35088:52608].set_index(pd.RangeIndex(0,17520))\n",
    "\n",
    "    environments[\"train\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"train\"][f\"building_{idx+1}\"]))\n",
    "    environments[\"eval\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"eval\"][f\"building_{idx+1}\"]))\n",
    "    environments[\"test\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"test\"][f\"building_{idx+1}\"]))\n",
    "\n",
    "print(\"Batch size: \", environments[\"train\"][f\"building_1\"].batch_size)\n",
    "print(\"State Space: {}, Action Space: {}\".format(environments[\"train\"][f\"building_1\"].observation_spec().shape[0], environments[\"train\"][f\"building_1\"].action_spec().shape[0])) #SoE, price, price forecast 1-6\n",
    "print(\"Upper bound: {}\".format(round(environments[\"train\"][f\"building_1\"].action_spec().maximum.item(), 3)))\n",
    "dataset[\"test\"][f\"building_1\"].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "Start training building 1\n",
      "0 /  50\n",
      "1 /  50\n",
      "2 /  50\n",
      "3 /  50\n",
      "4 /  50\n",
      "5 /  50\n",
      "6 /  50\n",
      "7 /  50\n",
      "8 /  50\n",
      "9 /  50\n",
      "10 /  50\n",
      "11 /  50\n",
      "12 /  50\n",
      "13 /  50\n",
      "14 /  50\n",
      "15 /  50\n",
      "16 /  50\n",
      "17 /  50\n",
      "18 /  50\n",
      "19 /  50\n",
      "20 /  50\n",
      "21 /  50\n",
      "22 /  50\n",
      "23 /  50\n",
      "24 /  50\n",
      "25 /  50\n",
      "26 /  50\n",
      "27 /  50\n",
      "28 /  50\n",
      "29 /  50\n",
      "30 /  50\n",
      "31 /  50\n",
      "32 /  50\n",
      "33 /  50\n",
      "34 /  50\n",
      "35 /  50\n",
      "36 /  50\n",
      "37 /  50\n",
      "38 /  50\n",
      "39 /  50\n",
      "40 /  50\n",
      "41 /  50\n",
      "42 /  50\n",
      "43 /  50\n",
      "44 /  50\n",
      "45 /  50\n",
      "46 /  50\n",
      "47 /  50\n",
      "48 /  50\n",
      "49 /  50\n",
      "Start testing ...\n",
      "Start training building 2\n",
      "0 /  50\n",
      "1 /  50\n",
      "2 /  50\n",
      "3 /  50\n",
      "4 /  50\n",
      "5 /  50\n",
      "6 /  50\n",
      "7 /  50\n",
      "8 /  50\n",
      "9 /  50\n",
      "10 /  50\n",
      "11 /  50\n",
      "12 /  50\n",
      "13 /  50\n",
      "14 /  50\n",
      "15 /  50\n",
      "16 /  50\n",
      "17 /  50\n",
      "18 /  50\n",
      "19 /  50\n",
      "20 /  50\n",
      "21 /  50\n",
      "22 /  50\n",
      "23 /  50\n",
      "24 /  50\n",
      "25 /  50\n",
      "26 /  50\n",
      "27 /  50\n",
      "28 /  50\n",
      "29 /  50\n",
      "30 /  50\n",
      "31 /  50\n",
      "32 /  50\n",
      "33 /  50\n",
      "34 /  50\n",
      "35 /  50\n",
      "36 /  50\n",
      "37 /  50\n",
      "38 /  50\n",
      "39 /  50\n",
      "40 /  50\n",
      "41 /  50\n",
      "42 /  50\n",
      "43 /  50\n",
      "44 /  50\n",
      "45 /  50\n",
      "46 /  50\n",
      "47 /  50\n",
      "48 /  50\n",
      "49 /  50\n",
      "Start testing ...\n"
     ]
    }
   ],
   "source": [
    "# Setup Agent networks\n",
    "batch_size = 1\n",
    "replay_buffer_capacity = 100000\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1000 #2000\n",
    "num_iterations = 50 #1500\n",
    "\n",
    "for idx in range(num_buildings):\n",
    "\n",
    "    global_step = tf.compat.v1.train.create_global_step()\n",
    "    \n",
    "    actor_net = ActorNetwork(\n",
    "        observation_spec=environments[\"train\"][f\"building_{idx+1}\"].observation_spec(),\n",
    "        action_spec=environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "        custom_layers=[CustomLayers.get_dense_layers(layers=1, units=4)],\n",
    "        )\n",
    "\n",
    "    critic_net = CriticNetwork(\n",
    "        observation_spec=environments[\"train\"][f\"building_{idx+1}\"].observation_spec(),\n",
    "        action_spec=environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "        custom_layers=[CustomLayers.get_dense_layers(layers=1, units=4)],\n",
    "        )\n",
    "\n",
    "    target_actor_network = ActorNetwork(\n",
    "        observation_spec=environments[\"train\"][f\"building_{idx+1}\"].observation_spec(),\n",
    "        action_spec=environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "        custom_layers=[CustomLayers.get_dense_layers(layers=1, units=4)],\n",
    "        )\n",
    "\n",
    "    target_critic_network = CriticNetwork(\n",
    "        observation_spec=environments[\"train\"][f\"building_{idx+1}\"].observation_spec(),\n",
    "        action_spec=environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "        custom_layers=[CustomLayers.get_dense_layers(layers=1, units=4)],\n",
    "        )\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        environments[\"train\"][f\"building_{idx+1}\"].time_step_spec(),\n",
    "        environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4), #-2 bis -4\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3), #-1 bis -3\n",
    "        ou_stddev=0.2, #0.3, # 0.2 , 0.3,\n",
    "        ou_damping=0.15, #0.15, #0.15,\n",
    "        target_actor_network=target_actor_network,\n",
    "        target_critic_network=target_critic_network,\n",
    "        target_update_tau=0.05, # 0.005, 0.01, 0.05,\n",
    "        target_update_period=10, # 5, 20, 50\n",
    "        dqda_clipping=1,\n",
    "        td_errors_loss_fn= tf.compat.v1.losses.huber_loss, #tf.keras.losses.MeanSquaredError(),\n",
    "        gamma=0.99, # 0.9, 0.99\n",
    "        reward_scale_factor=10, # 1.0,\n",
    "        train_step_counter=global_step,\n",
    "    )\n",
    "\n",
    "    tf_agent.initialize()\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size= environments[\"train\"][f\"building_{idx+1}\"].batch_size,\n",
    "        max_length=replay_buffer_capacity,\n",
    "    )\n",
    "\n",
    "    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        environments[\"train\"][f\"building_{idx+1}\"],\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=initial_collect_steps,\n",
    "    )\n",
    "\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        environments[\"train\"][f\"building_{idx+1}\"],\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=collect_steps_per_iteration,\n",
    "    )\n",
    "\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=\"DDPG_battery_testing\",\n",
    "        job_type=\"train_eval_test\",\n",
    "        name=f\"Exp_building{idx+1}\",\n",
    "        config={\n",
    "            \"train_steps\": num_iterations,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"actor_learning_rate\": 1e-4,\n",
    "            \"critic_learning_rate\": 1e-3}\n",
    "    )\n",
    "    artifact = wandb.Artifact(name='save', type=\"checkpoint\")\n",
    "\n",
    "    eval_metrics = [tf_metrics.AverageReturnMetric(batch_size=batch_size)]\n",
    "    test_metrics = [tf_metrics.AverageReturnMetric(batch_size=batch_size)]\n",
    "\n",
    "    \"\"\"train_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir='checkpoints/ddpg/',\n",
    "        max_to_keep=1,\n",
    "        agent=tf_agent,\n",
    "        policy=tf_agent.policy,\n",
    "        replay_buffer=replay_buffer,\n",
    "        global_step=global_step\n",
    "    )\n",
    "    train_checkpointer.initialize_or_restore()\"\"\"\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    # For better performance\n",
    "    initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "    collect_driver.run = common.function(collect_driver.run)\n",
    "    tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data\n",
    "    initial_collect_driver.run()\n",
    "    time_step = environments[\"train\"][f\"building_{idx+1}\"].reset()\n",
    "    policy_state = collect_policy.get_initial_state(environments[\"train\"][f\"building_{idx+1}\"].batch_size)\n",
    "\n",
    "    # pipeline which will feed data to the agent\n",
    "    dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(f\"Start training building {idx+1}\")\n",
    "    while global_step.numpy() < num_iterations:\n",
    "        print(global_step.numpy(), \"/ \", num_iterations)\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "        experience, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience)\n",
    "\n",
    "        if global_step.numpy() % 2 == 0:\n",
    "            metrics = {}    \n",
    "            metrics[\"Loss\"] = train_loss.loss\n",
    "            wandb.log(metrics)\n",
    "    \n",
    "    print(\"Start testing ...\")\n",
    "    metrics = metric_utils.eager_compute(\n",
    "        test_metrics,\n",
    "        environments[\"test\"][f\"building_{idx+1}\"],\n",
    "        tf_agent.policy,\n",
    "        num_episodes=batch_size)\n",
    "    logging = {}    \n",
    "    logging[\"AverageReturn\"] = metrics['AverageReturn'].numpy()\n",
    "    wandb.log(logging)\n",
    "    #artifact.add_dir(local_path='checkpoints/ddpg/')\n",
    "    wandb.log_artifact(artifact)\n",
    "    wandb.finish()\n",
    "    tf.compat.v1.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
