{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "Building a Deep Deterministic Policy Gradient (DDPG) algorithm for battery scheduling involves several key components. Here's a brief overview of the main components and a sample TensorFlow implementation.\n",
    "\n",
    "Main Components of DDPG for Battery Scheduling:\n",
    "State Space: This includes the State of Energy (SoE) of the battery, Load demand, Electricity Price, and Photovoltaic (PV) output. These factors collectively represent the current situation and are input to the neural network.\n",
    "\n",
    "- **Action Space**: This is the (dis)charging current of the battery, which is the output of the policy network. The action will be a continuous value, determining how much to charge or discharge the battery.\n",
    "\n",
    "- **Actor Network**: This neural network approximates the policy. It takes the state as input and outputs the best believed action to maximize future rewards.\n",
    "\n",
    "- **Critic Network**: This network estimates the value function. It takes both the state and action as input and outputs a Q-value, representing the expected future rewards of taking that action in that state.\n",
    "\n",
    "- **Replay Buffer**: This is a data structure used to store and recall experience tuples (state, action, reward, next state). It helps in breaking the correlation between consecutive training samples.\n",
    "\n",
    "- **Target Networks**: These are copies of the actor and critic networks, used to stabilize training. They are slowly updated to track the learned networks.\n",
    "\n",
    "- **Exploration Strategy**: Since DDPG is an off-policy algorithm, an exploration strategy like adding noise to the action is often used to explore the action space effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_dim = 4  # [SoE, Load, Price, PV]\n",
    "action_dim = 1  # (dis)charging current\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.002\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "buffer_size = 100000\n",
    "minibatch_size = 64\n",
    "num_episodes = 10\n",
    "iterations = 17520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_customer_data() missing 1 required positional argument: 'dfmix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mDL\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mDL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_customer_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/load1011.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadPrice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/price.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m data_eval \u001b[38;5;241m=\u001b[39m DL\u001b[38;5;241m.\u001b[39mget_customer_data(DL\u001b[38;5;241m.\u001b[39mloadData(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/load1112.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), DL\u001b[38;5;241m.\u001b[39mloadPrice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/price.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), customer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape_data\u001b[39m(df):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Reshape the DataFrame from a day-wise format to a continuous series format\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_customer_data() missing 1 required positional argument: 'dfmix'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import utils.dataloader as DL\n",
    "\n",
    "data_train = DL.get_customer_data(DL.loadData('../../data/load1011.csv'), DL.loadPrice('../../data/price.csv'), customer=1)\n",
    "data_eval = DL.get_customer_data(DL.loadData('../../data/load1112.csv'), DL.loadPrice('../../data/price.csv'), customer=1)\n",
    "\n",
    "def reshape_data(df):\n",
    "    # Reshape the DataFrame from a day-wise format to a continuous series format\n",
    "    reshaped_data = df.values.flatten()  # Flatten the day-wise data into a single array\n",
    "    return pd.DataFrame(reshaped_data)  # Convert back to DataFrame for consistency\n",
    "\n",
    "# Assuming data_train[0], data_train[1], and data_train[2] are your load, PV, and price data respectively\n",
    "load_data_reshaped = reshape_data(data_train[0])\n",
    "pv_data_reshaped = reshape_data(data_train[1])\n",
    "price_data_reshaped = pd.DataFrame(data_train[2].values)\n",
    "\n",
    "print(\"Load data values {}, PV values {}, Price values {}\".format(len(load_data_reshaped), len(pv_data_reshaped), len(price_data_reshaped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatteryEnvironment:\n",
    "    def __init__(self, load_data, pv_data, price_data, max_battery, eta=0.95):\n",
    "        self.load_data = load_data\n",
    "        self.pv_data = pv_data\n",
    "        self.price_data = price_data\n",
    "        self.max_battery = max_battery\n",
    "        self.eta = eta  # Efficiency\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_battery = self.max_battery / 2  # Start with half charge\n",
    "        return self._next_observation()\n",
    "\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        # Access the current state's load, PV, and price\n",
    "        load = self.load_data[self.current_step][0]\n",
    "        pv = self.pv_data[self.current_step][0]\n",
    "        price = self.price_data[self.current_step][0]\n",
    "\n",
    "        # Normalize the state\n",
    "        state = np.array([\n",
    "            self.current_battery / self.max_battery,\n",
    "            load / np.max(self.load_data),\n",
    "            price / np.max(self.price_data),\n",
    "            pv / np.max(self.pv_data)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Implement the battery dynamics\n",
    "        battery_change = action * self.max_battery\n",
    "        self.current_battery += battery_change * self.eta\n",
    "        self.current_battery = np.clip(self.current_battery, 0, self.max_battery)\n",
    "\n",
    "        # Ensure current_battery is a scalar\n",
    "        self.current_battery = self.current_battery.item() if isinstance(self.current_battery, np.ndarray) else self.current_battery\n",
    "        \n",
    "        # Calculate the reward\n",
    "        reward = -abs(self.load_data[self.current_step] - battery_change) * self.price_data[self.current_step]\n",
    "\n",
    "        # Ensure reward is a scalar (convert it to a single value)\n",
    "        reward = reward.item() if isinstance(reward, np.ndarray) else reward\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.load_data)\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "    \n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Actor Network\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(action_dim, activation='tanh')  # Scaled to action space\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Critic Network\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = self.fc1(tf.concat([state, action], axis=1))\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Networks\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "target_actor = Actor()\n",
    "target_critic = Critic()\n",
    "target_actor.set_weights(actor.get_weights())\n",
    "target_critic.set_weights(critic.get_weights())\n",
    "\n",
    "# Initialize the environment\n",
    "max_battery_capacity = 100  # Example capacity, adjust as needed\n",
    "env = BatteryEnvironment(load_data_reshaped.values, pv_data_reshaped.values, price_data_reshaped.values, max_battery_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "# Replay Buffer\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the networks\n",
    "def update_networks(tau):\n",
    "    new_actor_weights = []\n",
    "    actor_weights = actor.get_weights()\n",
    "    target_actor_weights = target_actor.get_weights()\n",
    "    for aw, taw in zip(actor_weights, target_actor_weights):\n",
    "        new_actor_weights.append(aw * tau + taw * (1 - tau))\n",
    "    target_actor.set_weights(new_actor_weights)\n",
    "\n",
    "    new_critic_weights = []\n",
    "    critic_weights = critic.get_weights()\n",
    "    target_critic_weights = target_critic.get_weights()\n",
    "    for cw, tcw in zip(critic_weights, target_critic_weights):\n",
    "        new_critic_weights.append(cw * tau + tcw * (1 - tau))\n",
    "    target_critic.set_weights(new_critic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Episode 1, Reward: -13705.569631635079, Time taken: 528.8077805042267 seconds\n",
      "Episode:  1\n",
      "Episode 2, Reward: -15403.587732426215, Time taken: 522.5618104934692 seconds\n",
      "Episode:  2\n",
      "Episode 3, Reward: -18799.297346016396, Time taken: 510.07124161720276 seconds\n",
      "Episode:  3\n",
      "Episode 4, Reward: -16970.971115304466, Time taken: 504.63184452056885 seconds\n",
      "Episode:  4\n",
      "Episode 5, Reward: -25333.424582508494, Time taken: 506.2705578804016 seconds\n",
      "Episode:  5\n",
      "Episode 6, Reward: -26437.93516378536, Time taken: 511.16979479789734 seconds\n",
      "Episode:  6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Time taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     actor_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(actor_grad, actor\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[43mupdate_networks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mupdate_networks\u001b[1;34m(tau)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_networks\u001b[39m(tau):\n\u001b[0;32m      3\u001b[0m     new_actor_weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m     actor_weights \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     target_actor_weights \u001b[38;5;241m=\u001b[39m target_actor\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m aw, taw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(actor_weights, target_actor_weights):\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3047\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[0;32m   3042\u001b[0m \n\u001b[0;32m   3043\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;124;03m    A flat list of Numpy arrays.\u001b[39;00m\n\u001b[0;32m   3045\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3046\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:1878\u001b[0m, in \u001b[0;36mLayer.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1877\u001b[0m         output_weights\u001b[38;5;241m.\u001b[39mappend(weight)\n\u001b[1;32m-> 1878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:4248\u001b[0m, in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   4236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   4237\u001b[0m \n\u001b[0;32m   4238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4245\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   4246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 4248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m   4249\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m   4250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:4248\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   4237\u001b[0m \n\u001b[0;32m   4238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4245\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   4246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 4248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m   4249\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m   4250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:689\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    688\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    691\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:395\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m    394\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numpy()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmaybe_arr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    for episode in range(num_episodes):\n",
    "        print(\"Episode: \", episode)  \n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        start_time = time.time()  # To measure the time taken for each episode\n",
    "\n",
    "        for t in range(iterations-1):  # 48 time steps per day * 365 days\n",
    "            action = actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "            noise = np.random.normal(0, 0.1, size=action_dim)\n",
    "            action = np.clip(action.numpy() + noise, -1, 1)\n",
    "            action = np.squeeze(action, axis=-1)\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if replay_buffer.size() > minibatch_size:\n",
    "                minibatch = replay_buffer.sample(minibatch_size)\n",
    "                states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))  # Convert to arrays\n",
    "\n",
    "                # Critic Update\n",
    "                with tf.GradientTape() as tape:\n",
    "                    target_actions = target_actor(next_states)\n",
    "                    target_critic_value = target_critic(next_states, target_actions, training=True)\n",
    "                    \n",
    "                    y = rewards + gamma * target_critic_value * (1 - dones)\n",
    "                    critic_value = critic(states, actions, training=True)\n",
    "                    critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "                critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "                critic_optimizer.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "\n",
    "                # Actor Update\n",
    "                with tf.GradientTape() as tape:\n",
    "                    actions = actor(states)\n",
    "                    critic_value = critic(states, actions, training=True)\n",
    "                    # We want to maximize the critic value, hence minimize the negative of critic_value\n",
    "                    actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "                actor_grad = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grad, actor.trainable_variables))\n",
    "\n",
    "                # Update target networks\n",
    "                update_networks(tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Episode {episode + 1}, Reward: {episode_reward}, Time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
