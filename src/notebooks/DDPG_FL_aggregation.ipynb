{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rs1044\\AppData\\Local\\Temp\\ipykernel_16816\\2142354254.py:18: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import logging\n",
    "import os\n",
    "logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment, py_environment, batched_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from environments.EnergyManagementEnv import EnergyManagementEnv\n",
    "from utils.agentNetworks import ActorNetwork, CriticNetwork, CustomLayers\n",
    "import utils.dataloader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "State Space: 6, Action Space: 1\n",
      "Upper bound: 2.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_1</th>\n",
       "      <th>pv_1</th>\n",
       "      <th>price</th>\n",
       "      <th>fuelmix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05704</td>\n",
       "      <td>0.530991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   load_1  pv_1    price   fuelmix\n",
       "0   1.149   0.0  0.05704  0.530991"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buildings = 30\n",
    "energy_data = pd.read_csv(\"../../data/3final_data/Final_Energy_dataset.csv\", header=0)\n",
    "energy_data.set_index('Date', inplace=True)\n",
    "energy_data.fillna(0, inplace=True)\n",
    "\n",
    "dataset = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "environments = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for idx in range(num_buildings):\n",
    "    user_data = energy_data[[f'load_{idx+1}', f'pv_{idx+1}', 'price', 'fuelmix']]\n",
    "    \n",
    "    dataset[\"train\"][f\"building_{idx+1}\"] = user_data[0:17520].set_index(pd.RangeIndex(0,17520))\n",
    "    dataset[\"eval\"][f\"building_{idx+1}\"] = user_data[17520:35088].set_index(pd.RangeIndex(0,17568))\n",
    "    dataset[\"test\"][f\"building_{idx+1}\"] = user_data[35088:52608].set_index(pd.RangeIndex(0,17520))\n",
    "\n",
    "    environments[\"train\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"train\"][f\"building_{idx+1}\"]))\n",
    "    environments[\"eval\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"eval\"][f\"building_{idx+1}\"]))\n",
    "    environments[\"test\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"test\"][f\"building_{idx+1}\"], logging=True))\n",
    "\n",
    "print(\"Batch size: \", environments[\"train\"][f\"building_1\"].batch_size)\n",
    "print(\"State Space: {}, Action Space: {}\".format(environments[\"train\"][f\"building_1\"].observation_spec().shape[0], environments[\"train\"][f\"building_1\"].action_spec().shape[0])) #SoE, price, price forecast 1-6\n",
    "print(\"Upper bound: {}\".format(round(environments[\"train\"][f\"building_1\"].action_spec().maximum.item(), 3)))\n",
    "dataset[\"test\"][f\"building_1\"].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_rounds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ 7, 14, 18, 22, 23, 25, 29], dtype=int64),\n",
       " 1: array([6], dtype=int64),\n",
       " 2: array([ 3,  4,  9, 13, 15, 19, 20, 30], dtype=int64),\n",
       " 3: array([1], dtype=int64),\n",
       " 4: array([21], dtype=int64),\n",
       " 5: array([ 2, 28], dtype=int64),\n",
       " 6: array([ 5, 10, 11, 12, 24, 26, 27], dtype=int64),\n",
       " 7: array([8], dtype=int64),\n",
       " 8: array([17], dtype=int64),\n",
       " 9: array([16], dtype=int64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.loadtxt(f'../../data/3final_data/Clusters_KMeans10_dtw.csv', delimiter=',').astype(int)\n",
    "num_clusters = 10\n",
    "cluster_buildings = {i: [] for i in range(num_clusters)}\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_number in range(num_clusters):\n",
    "    buildings_in_cluster = np.where(y == cluster_number)[0] +1\n",
    "    cluster_buildings[cluster_number] = buildings_in_cluster\n",
    "cluster_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import ddpg\n",
    "\n",
    "def get_ddpg_agent(observation_spec, action_spec, custom_layers, global_step): \n",
    "    \n",
    "    \"\"\"actor_net = ActorNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "\n",
    "    critic_net = CriticNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "    \n",
    "    target_actor_network = ActorNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "\n",
    "    target_critic_network = CriticNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "    \"\"\"\n",
    "\n",
    "    actor_net = ddpg.actor_network.ActorNetwork(\n",
    "        input_tensor_spec=observation_spec,\n",
    "        output_tensor_spec=action_spec, \n",
    "        fc_layer_params=(256, 256),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "     \n",
    "    critic_net = ddpg.critic_network.CriticNetwork(\n",
    "        input_tensor_spec=(observation_spec, action_spec),\n",
    "        joint_fc_layer_params=(256, 256),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "    target_actor_network = ddpg.actor_network.ActorNetwork(\n",
    "        input_tensor_spec=observation_spec,\n",
    "        output_tensor_spec=action_spec, fc_layer_params=(256, 256),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "    target_critic_network = ddpg.critic_network.CriticNetwork(\n",
    "        input_tensor_spec=(observation_spec, action_spec),\n",
    "        joint_fc_layer_params=(256, 256),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "    \n",
    "\n",
    "    agent_params = {\n",
    "        \"time_step_spec\": environments[\"train\"][f\"building_{1}\"].time_step_spec(),\n",
    "        \"action_spec\": environments[\"train\"][f\"building_{1}\"].action_spec(),\n",
    "        \"actor_network\": actor_net,\n",
    "        \"critic_network\": critic_net,\n",
    "        \"actor_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3), #1e-3\n",
    "        \"critic_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4), #1e-2\n",
    "        \"ou_stddev\": 0.9, #0.9,\n",
    "        \"ou_damping\": 0.15,\n",
    "        \"target_actor_network\": target_actor_network,\n",
    "        \"target_critic_network\": target_critic_network,\n",
    "        \"target_update_tau\": 0.05,\n",
    "        \"target_update_period\": 100, #5,\n",
    "        \"dqda_clipping\": 0.5,\n",
    "        \"td_errors_loss_fn\": tf.compat.v1.losses.huber_loss,\n",
    "        \"gamma\": 1, #0.99,\n",
    "        \"reward_scale_factor\": 1,\n",
    "        \"train_step_counter\": global_step,\n",
    "    }\n",
    "\n",
    "    # Create the DdpgAgent with unpacked parameters\n",
    "    tf_agent = ddpg_agent.DdpgAgent(**agent_params)\n",
    "\n",
    "    tf_agent.initialize()\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    return tf_agent, eval_policy, collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wandb_logging(project=\"DDPG_battery_testing\", name=\"Exp\", num_iterations=1500, batch_size=1, a_lr=\"1e-4\", c_lr=\"1e-3\"):\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=\"DDPG_battery_testing\",\n",
    "        job_type=\"train_eval_test\",\n",
    "        name=name,\n",
    "        config={\n",
    "            \"train_steps\": num_iterations,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"actor_learning_rate\": 1e-3,\n",
    "            \"critic_learning_rate\": 1e-2}\n",
    "    )\n",
    "    artifact = wandb.Artifact(name='save', type=\"checkpoint\")\n",
    "\n",
    "    \"\"\"train_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir='checkpoints/ddpg/',\n",
    "            max_to_keep=1,\n",
    "            agent=tf_agent,\n",
    "            policy=tf_agent.policy,\n",
    "            replay_buffer=replay_buffer,\n",
    "            global_step=global_step\n",
    "        )\n",
    "        train_checkpointer.initialize_or_restore()\"\"\"\n",
    "\n",
    "    return artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rl_training_pipeline(tf_agent, env_train, replay_buffer_capacity,collect_policy, initial_collect_steps, collect_steps_per_iteration, batch_size):\n",
    "    \n",
    "    #Setup replay buffer -> TFUniform to give each sample an equal selection chance\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            data_spec=tf_agent.collect_data_spec,\n",
    "            batch_size= env_train.batch_size,\n",
    "            max_length=replay_buffer_capacity,\n",
    "        )\n",
    "\n",
    "    # Populate replay buffer with inital experience before actual training (for num_steps times)\n",
    "    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=env_train,\n",
    "        policy=collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=initial_collect_steps,\n",
    "    )\n",
    "\n",
    "    # After the initial collection phase, the collect driver takes over for the continuous collection of data during the training process\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=env_train,\n",
    "        policy=collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=collect_steps_per_iteration,\n",
    "    )\n",
    "\n",
    "    # For better performance\n",
    "    initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "    collect_driver.run = common.function(collect_driver.run)\n",
    "    tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data\n",
    "    initial_collect_driver.run()\n",
    "    time_step = env_train.reset()\n",
    "    policy_state = collect_policy.get_initial_state(env_train.batch_size)\n",
    "\n",
    "    # The dataset is created from the replay buffer in a more structured and efficient way to provide mini-batches\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "        sample_batch_size=batch_size, num_steps=2).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    #Feed batches of experience to the agent for training\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    return iterator, collect_driver, time_step, policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Setup Agent networks\n",
    "batch_size = 256\n",
    "replay_buffer_capacity = 20000 #-> only <18.000 samples per dataset\n",
    "initial_collect_steps = 2000\n",
    "collect_steps_per_iteration = 20 \n",
    "num_iterations = 10000 #10000\n",
    "eval_interval = 9000 #3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also possible: Attention score: Rank based, soft-max ohne normalization \n",
    "def calculate_mean_weights_with_noise(weights_list, noise_scale=0):\n",
    "    # Assuming weights_list is a list containing two elements, where each element\n",
    "    # is a list of numpy arrays representing the weights of an actor network\n",
    "    mean_weights = []\n",
    "\n",
    "    for weight_pair in zip(*weights_list):\n",
    "        print(weight_pair)\n",
    "        #1. Averaging\n",
    "        mean_weight = tf.math.reduce_mean(tf.convert_to_tensor(weight_pair), axis=0)\n",
    "        \n",
    "        #2. Adding noise\n",
    "        noise = tf.random.normal(shape=mean_weight.shape, mean=0.0, stddev=noise_scale)\n",
    "        noisy_mean_weight = mean_weight + noise\n",
    "\n",
    "        mean_weights.append(noisy_mean_weight)\n",
    "    return mean_weights\n",
    "\n",
    "\n",
    "def calculate_mean_weights_with_softmax_attention(weights_list, performance_metrics, noise_scale=0.3):\n",
    "    \n",
    "    mean_weights = []\n",
    "\n",
    "    #Claulate standardized attention scores\n",
    "    performance_metrics_tensor = tf.convert_to_tensor(performance_metrics, dtype=tf.float32)\n",
    "    standardized_metrics = (performance_metrics_tensor - tf.reduce_mean(performance_metrics_tensor)) / tf.math.reduce_std(performance_metrics_tensor)\n",
    "    attention_scores = tf.nn.softmax(standardized_metrics)\n",
    "\n",
    "    for weight_pair in zip(*weights_list):\n",
    "        \n",
    "        stacked_weights = tf.stack(weight_pair, axis=0)\n",
    "        weighted_mean_weight = tf.zeros_like(stacked_weights[0])\n",
    "\n",
    "        # Iterate through each model's weights and the corresponding attention score\n",
    "        for model_weights, attention_score in zip(stacked_weights, attention_scores):\n",
    "            \n",
    "            weighted_model_weights = model_weights * attention_score\n",
    "            \n",
    "            # Add noise to the weighted model weights\n",
    "            noise = tf.random.normal(shape=weighted_model_weights.shape, mean=0.0, stddev=noise_scale)\n",
    "            noisy_weighted_model_weights = weighted_model_weights + noise\n",
    "            \n",
    "            # Accumulate the weighted (and noised) weights\n",
    "            weighted_mean_weight += noisy_weighted_model_weights\n",
    "        \n",
    "        mean_weights.append(weighted_mean_weight)\n",
    "    \n",
    "    return mean_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights = {\"actor_net\": {}, \"critic_net\": {}, \"target_actor_network\": {}, \"target_critic_network\": {}}\n",
    "\n",
    "#Initalize a global model for each Cluster of similar buildings\n",
    "for cluster in range(num_clusters):\n",
    "        # 1. Build global agent per cluster\n",
    "        global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "        first_building_in_cluster = cluster_buildings[cluster][0]\n",
    "\n",
    "        global_tf_agent, global_eval_policy, global_collect_policy = get_ddpg_agent(\n",
    "                observation_spec = environments[\"train\"][f\"building_{first_building_in_cluster}\"].observation_spec(),\n",
    "                action_spec = environments[\"train\"][f\"building_{first_building_in_cluster}\"].action_spec(),\n",
    "                custom_layers = [CustomLayers.get_dense_layers(layers=1, units=32)],\n",
    "                global_step = global_step\n",
    "                )\n",
    "\n",
    "        # 2. Initially store weights\n",
    "        global_weights[\"actor_net\"][cluster] = global_tf_agent._actor_network.get_weights()\n",
    "        global_weights[\"critic_net\"][cluster] = global_tf_agent._critic_network.get_weights()\n",
    "        global_weights[\"target_actor_network\"][cluster] = global_tf_agent._target_actor_network.get_weights()\n",
    "        global_weights[\"target_critic_network\"][cluster] = global_tf_agent._target_critic_network.get_weights()\n",
    "\n",
    "        model_dir = os.path.join(os.getcwd(), f\"models/cluster_{cluster}/FLround{0}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        np.savez(os.path.join(model_dir, \"actor_network_weights.npz\"), *global_tf_agent._actor_network.get_weights())\n",
    "        np.savez(os.path.join(model_dir, \"critic_weights.npz\"), *global_tf_agent._critic_network.get_weights())\n",
    "        np.savez(os.path.join(model_dir, \"target_actor_weights.npz\"), *global_tf_agent._target_actor_network.get_weights())\n",
    "        np.savez(os.path.join(model_dir, \"target_critic_weights.npz\"), *global_tf_agent._target_critic_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 1 / 10\n",
      "Return:  4955.4487\n",
      "Return:  4781.4746\n",
      "Return:  4235.6245\n",
      "Return:  4491.1934\n",
      "Return:  4478.134\n",
      "Return:  -192051.2\n",
      "Return:  -191878.56\n",
      "Performance List:  [4955.4487, 4781.4746, 4235.6245, 4491.1934, 4478.134, -192051.2, -191878.56]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 1 / 10\n",
      "Return:  4083.3286\n",
      "Performance List:  [4083.3286]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 1 / 10\n",
      "Return:  4648.6763\n",
      "Return:  4337.7705\n",
      "Return:  4456.5757\n",
      "Return:  -191450.55\n",
      "Return:  4600.381\n",
      "Return:  4810.3867\n",
      "Return:  4794.2197\n",
      "Return:  4618.5093\n",
      "Performance List:  [4648.6763, 4337.7705, 4456.5757, -191450.55, 4600.381, 4810.3867, 4794.2197, 4618.5093]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 1 / 10\n",
      "Return:  5425.6606\n",
      "Performance List:  [5425.6606]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 1 / 10\n",
      "Return:  3420.2805\n",
      "Performance List:  [3420.2805]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 1 / 10\n",
      "Return:  4226.6636\n",
      "Return:  -191882.53\n",
      "Performance List:  [4226.6636, -191882.53]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 1 / 10\n",
      "Return:  4469.471\n",
      "Return:  4069.3047\n",
      "Return:  4433.203\n",
      "Return:  4289.07\n",
      "Return:  6471.731\n",
      "Return:  4056.774\n",
      "Return:  4185.307\n",
      "Performance List:  [4469.471, 4069.3047, 4433.203, 4289.07, 6471.731, 4056.774, 4185.307]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 1 / 10\n",
      "Return:  3144.975\n",
      "Performance List:  [3144.975]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 1 / 10\n",
      "Return:  4183.9946\n",
      "Performance List:  [4183.9946]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 1 / 10\n",
      "Return:  4231.2603\n",
      "Performance List:  [4231.2603]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 2 / 10\n",
      "Return:  4822.638\n",
      "Return:  4871.233\n",
      "Return:  4220.4644\n",
      "Return:  -191923.94\n",
      "Return:  -191902.2\n",
      "Return:  4263.4136\n",
      "Return:  4477.835\n",
      "Performance List:  [4822.638, 4871.233, 4220.4644, -191923.94, -191902.2, 4263.4136, 4477.835]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 2 / 10\n",
      "Return:  -192240.12\n",
      "Performance List:  [-192240.12]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 2 / 10\n",
      "Return:  4647.2544\n",
      "Return:  4318.9043\n",
      "Return:  4432.195\n",
      "Return:  4962.415\n",
      "Return:  4326.5396\n",
      "Return:  4885.917\n",
      "Return:  -191628.88\n",
      "Return:  4667.957\n",
      "Performance List:  [4647.2544, 4318.9043, 4432.195, 4962.415, 4326.5396, 4885.917, -191628.88, 4667.957]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 2 / 10\n",
      "Return:  5396.878\n",
      "Performance List:  [5396.878]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 2 / 10\n",
      "Return:  3414.0989\n",
      "Performance List:  [3414.0989]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 2 / 10\n",
      "Return:  -192223.23\n",
      "Return:  4528.6714\n",
      "Performance List:  [-192223.23, 4528.6714]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 2 / 10\n",
      "Return:  4534.6943\n",
      "Return:  3425.3706\n",
      "Return:  4483.286\n",
      "Return:  4232.006\n",
      "Return:  6492.253\n",
      "Return:  4010.0889\n",
      "Return:  4235.2354\n",
      "Performance List:  [4534.6943, 3425.3706, 4483.286, 4232.006, 6492.253, 4010.0889, 4235.2354]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 2 / 10\n",
      "Return:  -193250.48\n",
      "Performance List:  [-193250.48]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 2 / 10\n",
      "Return:  4213.054\n",
      "Performance List:  [4213.054]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 2 / 10\n",
      "Return:  4315.656\n",
      "Performance List:  [4315.656]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 3 / 10\n",
      "Return:  4939.002\n",
      "Return:  4864.2446\n",
      "Return:  4224.0063\n",
      "Return:  -191923.94\n",
      "Return:  4534.8594\n",
      "Return:  4237.1616\n",
      "Return:  4477.8354\n",
      "Performance List:  [4939.002, 4864.2446, 4224.0063, -191923.94, 4534.8594, 4237.1616, 4477.8354]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 3 / 10\n",
      "Return:  -192367.38\n",
      "Performance List:  [-192367.38]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 3 / 10\n",
      "Return:  4642.4917\n",
      "Return:  4396.855\n",
      "Return:  4423.2573\n",
      "Return:  4929.8247\n",
      "Return:  4536.446\n",
      "Return:  4815.4907\n",
      "Return:  4802.196\n",
      "Return:  4650.675\n",
      "Performance List:  [4642.4917, 4396.855, 4423.2573, 4929.8247, 4536.446, 4815.4907, 4802.196, 4650.675]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 3 / 10\n",
      "Return:  5374.0186\n",
      "Performance List:  [5374.0186]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 3 / 10\n",
      "Return:  3044.42\n",
      "Performance List:  [3044.42]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 3 / 10\n",
      "Return:  4169.2715\n",
      "Return:  4505.842\n",
      "Performance List:  [4169.2715, 4505.842]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 3 / 10\n",
      "Return:  4469.4336\n",
      "Return:  4066.4473\n",
      "Return:  4250.9995\n",
      "Return:  4266.084\n",
      "Return:  6460.85\n",
      "Return:  4068.198\n",
      "Return:  4235.4893\n",
      "Performance List:  [4469.4336, 4066.4473, 4250.9995, 4266.084, 6460.85, 4068.198, 4235.4893]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 3 / 10\n",
      "Return:  3132.015\n",
      "Performance List:  [3132.015]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 3 / 10\n",
      "Return:  4196.257\n",
      "Performance List:  [4196.257]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 3 / 10\n",
      "Return:  4230.977\n",
      "Performance List:  [4230.977]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 4 / 10\n",
      "Return:  4957.7534\n",
      "Return:  -191559.08\n",
      "Return:  4178.4106\n",
      "Return:  4495.9556\n",
      "Return:  -191902.2\n",
      "Return:  -192159.4\n",
      "Return:  4552.7393\n",
      "Performance List:  [4957.7534, -191559.08, 4178.4106, 4495.9556, -191902.2, -192159.4, 4552.7393]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 4 / 10\n",
      "Return:  3990.725\n",
      "Performance List:  [3990.725]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 4 / 10\n",
      "Return:  4647.573\n",
      "Return:  4379.6475\n",
      "Return:  4398.7896\n",
      "Return:  4929.4697\n",
      "Return:  -191805.38\n",
      "Return:  4829.5654\n",
      "Return:  4727.444\n",
      "Return:  -191742.84\n",
      "Performance List:  [4647.573, 4379.6475, 4398.7896, 4929.4697, -191805.38, 4829.5654, 4727.444, -191742.84]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 4 / 10\n",
      "Return:  5404.5376\n",
      "Performance List:  [5404.5376]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 4 / 10\n",
      "Return:  3461.0913\n",
      "Performance List:  [3461.0913]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 4 / 10\n",
      "Return:  -192223.23\n",
      "Return:  -191900.92\n",
      "Performance List:  [-192223.23, -191900.92]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 4 / 10\n",
      "Return:  4519.629\n",
      "Return:  4064.966\n",
      "Return:  -191941.67\n",
      "Return:  4286.164\n",
      "Return:  -189920.0\n",
      "Return:  3901.2268\n",
      "Return:  -192180.08\n",
      "Performance List:  [4519.629, 4064.966, -191941.67, 4286.164, -189920.0, 3901.2268, -192180.08]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 4 / 10\n",
      "Return:  -193268.88\n",
      "Performance List:  [-193268.88]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 4 / 10\n",
      "Return:  4179.0005\n",
      "Performance List:  [4179.0005]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 4 / 10\n",
      "Return:  4315.2744\n",
      "Performance List:  [4315.2744]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 5 / 10\n",
      "Return:  -63766.402\n",
      "Return:  4827.4644\n",
      "Return:  4230.2285\n",
      "Return:  4476.8633\n",
      "Return:  4519.814\n",
      "Return:  3991.3525\n",
      "Return:  4521.5845\n",
      "Performance List:  [-63766.402, 4827.4644, 4230.2285, 4476.8633, 4519.814, 3991.3525, 4521.5845]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 5 / 10\n",
      "Return:  3990.9585\n",
      "Performance List:  [3990.9585]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 5 / 10\n",
      "Return:  -191716.61\n",
      "Return:  4396.8545\n",
      "Return:  4389.8525\n",
      "Return:  4981.4214\n",
      "Return:  4589.7993\n",
      "Return:  4878.216\n",
      "Return:  4784.5093\n",
      "Return:  -191742.84\n",
      "Performance List:  [-191716.61, 4396.8545, 4389.8525, 4981.4214, 4589.7993, 4878.216, 4784.5093, -191742.84]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 5 / 10\n",
      "Return:  5431.145\n",
      "Performance List:  [5431.145]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 5 / 10\n",
      "Return:  3474.312\n",
      "Performance List:  [3474.312]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 5 / 10\n",
      "Return:  4180.269\n",
      "Return:  -191900.92\n",
      "Performance List:  [4180.269, -191900.92]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 5 / 10\n",
      "Return:  4463.953\n",
      "Return:  -192294.2\n",
      "Return:  4482.579\n",
      "Return:  4289.07\n",
      "Return:  6471.73\n",
      "Return:  4076.7925\n",
      "Return:  4214.963\n",
      "Performance List:  [4463.953, -192294.2, 4482.579, 4289.07, 6471.73, 4076.7925, 4214.963]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 5 / 10\n",
      "Return:  3124.4468\n",
      "Performance List:  [3124.4468]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 5 / 10\n",
      "Return:  4213.873\n",
      "Performance List:  [4213.873]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 5 / 10\n",
      "Return:  4315.156\n",
      "Performance List:  [4315.156]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 6 / 10\n",
      "Return:  4911.4805\n",
      "Return:  4731.6255\n",
      "Return:  4160.4736\n",
      "Return:  -191923.94\n",
      "Return:  -191883.8\n",
      "Return:  4185.7036\n",
      "Return:  4553.312\n",
      "Performance List:  [4911.4805, 4731.6255, 4160.4736, -191923.94, -191883.8, 4185.7036, 4553.312]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 6 / 10\n",
      "Return:  -192367.38\n",
      "Performance List:  [-192367.38]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 6 / 10\n",
      "Return:  4707.336\n",
      "Return:  -192021.17\n",
      "Return:  4461.764\n",
      "Return:  4954.232\n",
      "Return:  4550.874\n",
      "Return:  4852.057\n",
      "Return:  4735.3096\n",
      "Return:  -191742.84\n",
      "Performance List:  [4707.336, -192021.17, 4461.764, 4954.232, 4550.874, 4852.057, 4735.3096, -191742.84]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 6 / 10\n",
      "Return:  5395.7183\n",
      "Performance List:  [5395.7183]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 6 / 10\n",
      "Return:  3376.1892\n",
      "Performance List:  [3376.1892]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 6 / 10\n",
      "Return:  -192223.23\n",
      "Return:  3788.4316\n",
      "Performance List:  [-192223.23, 3788.4316]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 6 / 10\n",
      "Return:  4070.7224\n",
      "Return:  4129.2524\n",
      "Return:  4474.614\n",
      "Return:  4279.584\n",
      "Return:  6510.467\n",
      "Return:  -192344.47\n",
      "Return:  -192180.08\n",
      "Performance List:  [4070.7224, 4129.2524, 4474.614, 4279.584, 6510.467, -192344.47, -192180.08]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 6 / 10\n",
      "Return:  3156.1746\n",
      "Performance List:  [3156.1746]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 6 / 10\n",
      "Return:  4180.225\n",
      "Performance List:  [4180.225]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 6 / 10\n",
      "Return:  -192127.5\n",
      "Performance List:  [-192127.5]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 7 / 10\n",
      "Return:  4950.653\n",
      "Return:  4849.5166\n",
      "Return:  3723.0684\n",
      "Return:  4452.8335\n",
      "Return:  4465.9585\n",
      "Return:  3790.099\n",
      "Return:  4561.946\n",
      "Performance List:  [4950.653, 4849.5166, 3723.0684, 4452.8335, 4465.9585, 3790.099, 4561.946]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 7 / 10\n",
      "Return:  -192367.38\n",
      "Performance List:  [-192367.38]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 7 / 10\n",
      "Return:  4029.3901\n",
      "Return:  4392.397\n",
      "Return:  4453.4014\n",
      "Return:  -191450.55\n",
      "Return:  4552.0884\n",
      "Return:  -191550.25\n",
      "Return:  4794.1562\n",
      "Return:  4682.031\n",
      "Performance List:  [4029.3901, 4392.397, 4453.4014, -191450.55, 4552.0884, -191550.25, 4794.1562, 4682.031]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 7 / 10\n",
      "Return:  5440.6943\n",
      "Performance List:  [5440.6943]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 7 / 10\n",
      "Return:  3485.667\n",
      "Performance List:  [3485.667]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 7 / 10\n",
      "Return:  4140.816\n",
      "Return:  4465.5596\n",
      "Performance List:  [4140.816, 4465.5596]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 7 / 10\n",
      "Return:  4462.1323\n",
      "Return:  4130.387\n",
      "Return:  4490.874\n",
      "Return:  4297.321\n",
      "Return:  -10558.19\n",
      "Return:  4064.9053\n",
      "Return:  4221.032\n",
      "Performance List:  [4462.1323, 4130.387, 4490.874, 4297.321, -10558.19, 4064.9053, 4221.032]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 7 / 10\n",
      "Return:  3142.131\n",
      "Performance List:  [3142.131]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 7 / 10\n",
      "Return:  4205.809\n",
      "Performance List:  [4205.809]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 7 / 10\n",
      "Return:  4311.3345\n",
      "Performance List:  [4311.3345]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 8 / 10\n",
      "Return:  4892.1978\n",
      "Return:  -191559.08\n",
      "Return:  4173.5825\n",
      "Return:  4471.367\n",
      "Return:  4521.6313\n",
      "Return:  4216.1855\n",
      "Return:  4556.0757\n",
      "Performance List:  [4892.1978, -191559.08, 4173.5825, 4471.367, 4521.6313, 4216.1855, 4556.0757]\n",
      "Cluster 1: Buildings [6] Federated round ---------- 8 / 10\n",
      "Return:  4027.435\n",
      "Performance List:  [4027.435]\n",
      "Cluster 2: Buildings [ 3  4  9 13 15 19 20 30] Federated round ---------- 8 / 10\n",
      "Return:  4651.467\n",
      "Return:  -191984.53\n",
      "Return:  4392.6333\n",
      "Return:  4973.529\n",
      "Return:  4595.9717\n",
      "Return:  4810.7266\n",
      "Return:  -191628.88\n",
      "Return:  -191742.84\n",
      "Performance List:  [4651.467, -191984.53, 4392.6333, 4973.529, 4595.9717, 4810.7266, -191628.88, -191742.84]\n",
      "Cluster 3: Buildings [1] Federated round ---------- 8 / 10\n",
      "Return:  5430.839\n",
      "Performance List:  [5430.839]\n",
      "Cluster 4: Buildings [21] Federated round ---------- 8 / 10\n",
      "Return:  3414.5981\n",
      "Performance List:  [3414.5981]\n",
      "Cluster 5: Buildings [ 2 28] Federated round ---------- 8 / 10\n",
      "Return:  4174.3145\n",
      "Return:  4524.121\n",
      "Performance List:  [4174.3145, 4524.121]\n",
      "Cluster 6: Buildings [ 5 10 11 12 24 26 27] Federated round ---------- 8 / 10\n",
      "Return:  4530.0127\n",
      "Return:  -192294.2\n",
      "Return:  4475.3457\n",
      "Return:  4303.747\n",
      "Return:  6466.5747\n",
      "Return:  4051.7905\n",
      "Return:  3886.6821\n",
      "Performance List:  [4530.0127, -192294.2, 4475.3457, 4303.747, 6466.5747, 4051.7905, 3886.6821]\n",
      "Cluster 7: Buildings [8] Federated round ---------- 8 / 10\n",
      "Return:  3098.0874\n",
      "Performance List:  [3098.0874]\n",
      "Cluster 8: Buildings [17] Federated round ---------- 8 / 10\n",
      "Return:  4196.316\n",
      "Performance List:  [4196.316]\n",
      "Cluster 9: Buildings [16] Federated round ---------- 8 / 10\n",
      "Return:  4229.981\n",
      "Performance List:  [4229.981]\n",
      "Cluster 0: Buildings [ 7 14 18 22 23 25 29] Federated round ---------- 9 / 10\n",
      "Return:  -154407.14\n",
      "Return:  4787.4727\n",
      "Return:  4200.402\n",
      "Return:  4274.2305\n",
      "Return:  -191914.27\n"
     ]
    }
   ],
   "source": [
    "#Start Federated Learning - For each federated round\n",
    "for federated_round  in range(federated_rounds):\n",
    "    \n",
    "    #Iterate through each cluster\n",
    "    for cluster_number, buildings_in_cluster in cluster_buildings.items():\n",
    "        print(f\"Cluster {cluster_number}: Buildings {buildings_in_cluster} Federated round ----------\", federated_round+1, f\"/ {federated_rounds}\")\n",
    "        \n",
    "        local_actor_weight_list = list()\n",
    "        local_critic_weight_list = list()\n",
    "        local_target_actor_weight_list = list()\n",
    "        local_target_critic_weight_list = list()\n",
    "\n",
    "        performance_metrics = list()\n",
    "\n",
    "        #Iterate through the buildings per cluster\n",
    "        for building_index in buildings_in_cluster:\n",
    "            \n",
    "            #0. Reset global step\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "            \n",
    "            #1. Initalize local agent and set global weights\n",
    "            local_tf_agent, local_eval_policy, local_collect_policy = get_ddpg_agent(\n",
    "                observation_spec = environments[\"train\"][f\"building_{building_index}\"].observation_spec(),\n",
    "                action_spec = environments[\"train\"][f\"building_{building_index}\"].action_spec(),\n",
    "                custom_layers = [CustomLayers.get_dense_layers(layers=1, units=32)],\n",
    "                global_step = global_step\n",
    "                )\n",
    "            \n",
    "            model_dir = os.path.join(os.getcwd(), f\"models/cluster_{cluster_number}/FLround{federated_round}\")\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"actor_network_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                global_tf_agent._actor_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"critic_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                global_tf_agent._critic_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"target_actor_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                global_tf_agent._target_actor_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"target_critic_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                global_tf_agent._target_critic_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "\n",
    "            local_tf_agent._actor_network.set_weights(global_weights[\"actor_net\"][cluster])\n",
    "            local_tf_agent._critic_network.set_weights(global_weights[\"critic_net\"][cluster])\n",
    "            local_tf_agent._target_actor_network.set_weights(global_weights[\"target_actor_network\"][cluster])\n",
    "            local_tf_agent._target_critic_network.set_weights(global_weights[\"target_critic_network\"][cluster])\n",
    "\n",
    "            #2. Prepare training pipeline: Setup iterator, replay buffer, driver\n",
    "            local_iterator, local_collect_driver, local_time_step, local_policy_state = setup_rl_training_pipeline(\n",
    "                local_tf_agent, environments[\"train\"][f\"building_{building_index}\"], replay_buffer_capacity, local_collect_policy, initial_collect_steps, collect_steps_per_iteration, batch_size\n",
    "                )\n",
    "\n",
    "            #3. Setup wandb logging\n",
    "            #artifact = initialize_wandb_logging(name=f\"Exp_building{building_index}_rd{federated_round+1}\", num_iterations=num_iterations)\n",
    "\n",
    "            #4. Start training\n",
    "            #print(f\"Start training building {building_index+1} - Round {federated_round+1}\")\n",
    "            \n",
    "            eval_metrics = [tf_metrics.AverageReturnMetric()]\n",
    "            test_metrics = [tf_metrics.AverageReturnMetric()]\n",
    "\n",
    "            while global_step.numpy() < num_iterations:\n",
    "\n",
    "                #if global_step.numpy() % 50 == 0:\n",
    "                #    print(global_step.numpy(), \"/ \", num_iterations)\n",
    "\n",
    "                local_time_step, local_policy_state = local_collect_driver.run(time_step=local_time_step, policy_state=local_policy_state)\n",
    "                local_experience, _ = next(local_iterator)\n",
    "                local_train_loss = local_tf_agent.train(local_experience)\n",
    "                \n",
    "                \"\"\"metrics = {}\n",
    "                if global_step.numpy() % eval_interval == 0:\n",
    "                    #train_checkpointer.save(global_step)\n",
    "                    metrics = metric_utils.eager_compute(eval_metrics,environments[\"eval\"][f\"building_{building_index}\"],\n",
    "                        local_eval_policy,num_episodes=1,train_step=global_step,summary_writer=None,summary_prefix='',use_function=True)\"\"\"\n",
    "                \n",
    "                \n",
    "                #performance_metrics.append()\n",
    "                #if global_step.numpy() % 2 == 0:\n",
    "                #    metrics[\"loss\"] = local_train_loss.loss\n",
    "                #    wandb.log(metrics)\n",
    "            \n",
    "            metrics = metric_utils.eager_compute(test_metrics,environments[\"eval\"][f\"building_{building_index}\"], local_eval_policy, num_episodes=1)\n",
    "            print(\"Return: \", metrics[\"AverageReturn\"].numpy())\n",
    "            performance_metrics.append(metrics[\"AverageReturn\"].numpy())\n",
    "            \n",
    "            #5. Add local agent weights to list\n",
    "            local_actor_weight_list.append(local_tf_agent._actor_network.get_weights())\n",
    "            local_critic_weight_list.append(local_tf_agent._critic_network.get_weights())\n",
    "            local_target_actor_weight_list.append(local_tf_agent._target_actor_network.get_weights())\n",
    "            local_target_critic_weight_list.append(local_tf_agent._target_critic_network.get_weights())\n",
    "\n",
    "        # Performe Federated aggregation\n",
    "        print(\"Performance List: \", performance_metrics)\n",
    "        average_actor_weights = calculate_mean_weights_with_softmax_attention(local_actor_weight_list, performance_metrics)\n",
    "        average_critic_weights = calculate_mean_weights_with_softmax_attention(local_critic_weight_list, performance_metrics) \n",
    "        average_target_actor_weights = calculate_mean_weights_with_softmax_attention(local_target_actor_weight_list, performance_metrics) \n",
    "        average_target_critic_weights = calculate_mean_weights_with_softmax_attention(local_target_critic_weight_list, performance_metrics)    \n",
    "        \n",
    "        global_tf_agent._actor_network.set_weights(average_actor_weights)\n",
    "        global_tf_agent._critic_network.set_weights(average_critic_weights)\n",
    "        global_tf_agent._target_actor_network.set_weights(average_target_actor_weights)\n",
    "        global_tf_agent._target_critic_network.set_weights(average_target_critic_weights)\n",
    "\n",
    "        model_dir = os.path.join(os.getcwd(), f\"models/cluster_{cluster_number}/FLround{federated_round+1}_WAAwN\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        np.savez(os.path.join(model_dir, \"actor_network_weights.npz\"), *global_tf_agent._actor_network.get_weights())\n",
    "        np.savez(os.path.join(model_dir, \"critic_weights.npz\"), *global_tf_agent._critic_network.get_weights())\n",
    "        np.savez(os.path.join(model_dir, \"target_actor_weights.npz\"), *global_tf_agent._target_actor_network.get_weights())\n",
    "        np.savez(os.path.join(model_dir, \"target_critic_weights.npz\"), *global_tf_agent._target_critic_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster:  0  - Building:  7  - round:  0\n",
      "Building:  7  - Total Profit:  394.6404406116863\n",
      "Cluster:  0  - Building:  14  - round:  0\n",
      "Building:  14  - Total Profit:  -130.95716455000016\n",
      "Cluster:  0  - Building:  18  - round:  0\n",
      "Building:  18  - Total Profit:  -311.0430112700002\n",
      "Cluster:  0  - Building:  22  - round:  0\n",
      "Building:  22  - Total Profit:  -156.31512790999983\n",
      "Cluster:  0  - Building:  23  - round:  0\n",
      "Building:  23  - Total Profit:  264.5340516986598\n",
      "Cluster:  0  - Building:  25  - round:  0\n",
      "Building:  25  - Total Profit:  -297.6910558299997\n",
      "Cluster:  0  - Building:  29  - round:  0\n",
      "Building:  29  - Total Profit:  -173.7948776\n",
      "Cluster:  1  - Building:  6  - round:  0\n",
      "Building:  6  - Total Profit:  -596.3276914678621\n",
      "Cluster:  2  - Building:  3  - round:  0\n",
      "Building:  3  - Total Profit:  413.99348610369844\n",
      "Cluster:  2  - Building:  4  - round:  0\n",
      "Building:  4  - Total Profit:  252.5607129845912\n",
      "Cluster:  2  - Building:  9  - round:  0\n",
      "Building:  9  - Total Profit:  297.6391300996246\n",
      "Cluster:  2  - Building:  13  - round:  0\n",
      "Building:  13  - Total Profit:  392.021286725807\n",
      "Cluster:  2  - Building:  15  - round:  0\n",
      "Building:  15  - Total Profit:  325.6929103277214\n",
      "Cluster:  2  - Building:  19  - round:  0\n",
      "Building:  19  - Total Profit:  402.17691573573495\n",
      "Cluster:  2  - Building:  20  - round:  0\n",
      "Building:  20  - Total Profit:  168.96990464148197\n",
      "Cluster:  2  - Building:  30  - round:  0\n",
      "Building:  30  - Total Profit:  328.4811647546786\n",
      "Cluster:  3  - Building:  1  - round:  0\n",
      "Building:  1  - Total Profit:  137.82241871865452\n",
      "Cluster:  4  - Building:  21  - round:  0\n",
      "Building:  21  - Total Profit:  14.336158015671593\n",
      "Cluster:  5  - Building:  2  - round:  0\n",
      "Building:  2  - Total Profit:  214.44223285613438\n",
      "Cluster:  5  - Building:  28  - round:  0\n",
      "Building:  28  - Total Profit:  -118.1104145300003\n",
      "Cluster:  6  - Building:  5  - round:  0\n",
      "Building:  5  - Total Profit:  215.36580456697217\n",
      "Cluster:  6  - Building:  10  - round:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m\n\u001b[0;32m     52\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m [tf_metrics\u001b[38;5;241m.\u001b[39mAverageReturnMetric()]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m global_step\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m<\u001b[39m num_iterations:\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m#if global_step.numpy() % 50 == 0:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m#    print(global_step.numpy(), \"/ \", num_iterations)\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     time_step, policy_state \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     experience, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m     61\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m federated_tf_agent\u001b[38;5;241m.\u001b[39mtrain(experience)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_rounds=1\n",
    "\n",
    "result_df = pd.DataFrame(columns=['Building', 'Total Profit'])\n",
    "\n",
    "for cluster_number, buildings_in_cluster in cluster_buildings.items():\n",
    "\n",
    "    for building_index in buildings_in_cluster:\n",
    "        \n",
    "        for round in range(num_rounds):\n",
    "            print(\"Cluster: \", cluster_number, \" - Building: \", building_index, \" - round: \", round)\n",
    "            \n",
    "            #0. Reset global step\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "            \n",
    "            #1. Initalize local agent and set trained global weights\n",
    "            federated_tf_agent, federated_eval_policy, federated_collect_policy = get_ddpg_agent(\n",
    "                observation_spec = environments[\"train\"][f\"building_{building_index}\"].observation_spec(),\n",
    "                action_spec = environments[\"train\"][f\"building_{building_index}\"].action_spec(),\n",
    "                custom_layers = [CustomLayers.get_dense_layers(layers=1, units=32)],\n",
    "                global_step = global_step\n",
    "                )\n",
    "            \n",
    "            model_dir = os.path.join(os.getcwd(), f\"models/cluster_{cluster_number}/FLround{3}\")\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"actor_network_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                federated_tf_agent._actor_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"critic_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                federated_tf_agent._critic_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"target_actor_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                federated_tf_agent._target_actor_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "            \n",
    "            with np.load(os.path.join(model_dir, \"target_critic_weights.npz\"), allow_pickle=True) as data:\n",
    "                # Extract the arrays using the keys corresponding to their order\n",
    "                federated_tf_agent._target_critic_network.set_weights([data[f'arr_{i}'] for i in range(len(data.files))])\n",
    "\n",
    "            #Setup iterator, replay buffer, driver\n",
    "            iterator, collect_driver, time_step, policy_state = setup_rl_training_pipeline(\n",
    "                federated_tf_agent, environments[\"train\"][f\"building_{building_index}\"], replay_buffer_capacity, federated_collect_policy, initial_collect_steps, collect_steps_per_iteration, batch_size\n",
    "                )\n",
    "\n",
    "            #Setup wandb logging\n",
    "            artifact = initialize_wandb_logging(name=f\"Exp_building{building_index}_rd{round}\", num_iterations=num_iterations)\n",
    "            \n",
    "            #2. Train and evaluate\n",
    "            eval_metrics = [tf_metrics.AverageReturnMetric()]\n",
    "            test_metrics = [tf_metrics.AverageReturnMetric()]\n",
    "\n",
    "            while global_step.numpy() < num_iterations:\n",
    "\n",
    "                #if global_step.numpy() % 50 == 0:\n",
    "                #    print(global_step.numpy(), \"/ \", num_iterations)\n",
    "\n",
    "                time_step, policy_state = collect_driver.run(time_step=time_step, policy_state=policy_state)\n",
    "                experience, _ = next(iterator)\n",
    "                train_loss = federated_tf_agent.train(experience)\n",
    "                \n",
    "                metrics = {}\n",
    "                if global_step.numpy() % eval_interval == 0:\n",
    "                    #train_checkpointer.save(global_step)\n",
    "                    metrics = metric_utils.eager_compute(eval_metrics,environments[\"eval\"][f\"building_{building_index}\"],\n",
    "                        federated_eval_policy,num_episodes=1,train_step=global_step,summary_writer=None,summary_prefix='',use_function=True)\n",
    "                \n",
    "                if global_step.numpy() % 2 == 0:\n",
    "                    metrics[\"loss\"] = train_loss.loss\n",
    "                    wandb.log(metrics)\n",
    "\n",
    "            #3. Start testing\n",
    "            metrics = metric_utils.eager_compute(test_metrics,environments[\"test\"][f\"building_{building_index}\"], federated_eval_policy, num_episodes=1)\n",
    "            print('Building: ', building_index, ' - Total Profit: ', wandb.summary[\"Total Profit\"])\n",
    "            result_df = pd.concat([result_df, pd.DataFrame({'Building': [building_index], 'Total Profit': [wandb.summary[\"Total Profit\"]]})], ignore_index=True)\n",
    "            wandb.log(metrics)\n",
    "            #artifact.add_dir(local_path='checkpoints/ddpg/')\n",
    "            wandb.log_artifact(artifact)\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     391.271085\n",
       "1    -130.957165\n",
       "2     114.282858\n",
       "3    -156.315128\n",
       "4    -158.256629\n",
       "5    -297.691056\n",
       "6    -173.794878\n",
       "7    -596.327691\n",
       "8     418.249709\n",
       "9     251.295036\n",
       "10    295.560140\n",
       "11    388.206348\n",
       "12    316.959420\n",
       "13    400.539787\n",
       "14    166.514324\n",
       "15    326.726452\n",
       "16    178.689987\n",
       "17     11.579345\n",
       "18    207.679142\n",
       "19    306.928433\n",
       "20    215.829310\n",
       "21    237.994325\n",
       "22    232.283552\n",
       "23    191.929315\n",
       "24    583.194516\n",
       "25     47.613313\n",
       "26    211.460812\n",
       "27   -183.380170\n",
       "28     57.571424\n",
       "29    267.557714\n",
       "Name: Total Profit, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"Total Profit\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
