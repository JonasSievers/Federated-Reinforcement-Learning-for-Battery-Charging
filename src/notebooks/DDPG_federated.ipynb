{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rs1044\\AppData\\Local\\Temp\\ipykernel_26500\\2142354254.py:18: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import logging\n",
    "import os\n",
    "logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment, py_environment, batched_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from environments.EnergyManagementEnv import EnergyManagementEnv\n",
    "from utils.agentNetworks import ActorNetwork, CriticNetwork, CustomLayers\n",
    "import utils.dataloader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "State Space: 6, Action Space: 1\n",
      "Upper bound: 2.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_1</th>\n",
       "      <th>pv_1</th>\n",
       "      <th>price</th>\n",
       "      <th>fuelmix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05704</td>\n",
       "      <td>0.530991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   load_1  pv_1    price   fuelmix\n",
       "0   1.149   0.0  0.05704  0.530991"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buildings = 30\n",
    "energy_data = pd.read_csv(\"../../data/3final_data/Final_Energy_dataset.csv\", header=0)\n",
    "energy_data.set_index('Date', inplace=True)\n",
    "energy_data.fillna(0, inplace=True)\n",
    "\n",
    "dataset = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "environments = {\"train\": {}, \"eval\": {}, \"test\": {}}\n",
    "for idx in range(num_buildings):\n",
    "    user_data = energy_data[[f'load_{idx+1}', f'pv_{idx+1}', 'price', 'fuelmix']]\n",
    "    \n",
    "    dataset[\"train\"][f\"building_{idx+1}\"] = user_data[0:17520].set_index(pd.RangeIndex(0,17520))\n",
    "    dataset[\"eval\"][f\"building_{idx+1}\"] = user_data[17520:35088].set_index(pd.RangeIndex(0,17568))\n",
    "    dataset[\"test\"][f\"building_{idx+1}\"] = user_data[35088:52608].set_index(pd.RangeIndex(0,17520))\n",
    "\n",
    "    environments[\"train\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"train\"][f\"building_{idx+1}\"]))\n",
    "    environments[\"eval\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"eval\"][f\"building_{idx+1}\"]))\n",
    "    environments[\"test\"][f\"building_{idx+1}\"] = tf_py_environment.TFPyEnvironment(EnergyManagementEnv(init_charge=0.0, data=dataset[\"test\"][f\"building_{idx+1}\"]))\n",
    "\n",
    "print(\"Batch size: \", environments[\"train\"][f\"building_1\"].batch_size)\n",
    "print(\"State Space: {}, Action Space: {}\".format(environments[\"train\"][f\"building_1\"].observation_spec().shape[0], environments[\"train\"][f\"building_1\"].action_spec().shape[0])) #SoE, price, price forecast 1-6\n",
    "print(\"Upper bound: {}\".format(round(environments[\"train\"][f\"building_1\"].action_spec().maximum.item(), 3)))\n",
    "dataset[\"test\"][f\"building_1\"].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_rounds = 3\n",
    "num_rounds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ 7, 14, 18, 22, 23, 25, 29], dtype=int64),\n",
       " 1: array([6], dtype=int64),\n",
       " 2: array([ 3,  4,  9, 13, 15, 19, 20, 30], dtype=int64),\n",
       " 3: array([1], dtype=int64),\n",
       " 4: array([21], dtype=int64),\n",
       " 5: array([ 2, 28], dtype=int64),\n",
       " 6: array([ 5, 10, 11, 12, 24, 26, 27], dtype=int64),\n",
       " 7: array([8], dtype=int64),\n",
       " 8: array([17], dtype=int64),\n",
       " 9: array([16], dtype=int64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.loadtxt(f'../../data/3final_data/Clusters_KMeans10_dtw.csv', delimiter=',').astype(int)\n",
    "num_clusters = 10\n",
    "cluster_buildings = {i: [] for i in range(num_clusters)}\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_number in range(num_clusters):\n",
    "    buildings_in_cluster = np.where(y == cluster_number)[0] +1\n",
    "    cluster_buildings[cluster_number] = buildings_in_cluster\n",
    "cluster_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_weights_with_noise_fedprox(weight_list, clip_threshold=None, noise_scale=0.001, proximal_term=0.1):\n",
    "    avg_grad = list()\n",
    "\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = tf.math.reduce_mean(grad_list_tuple, axis=0)\n",
    "\n",
    "        if clip_threshold is not None:\n",
    "            layer_mean = tf.clip_by_value(layer_mean, -clip_threshold, clip_threshold)\n",
    "\n",
    "        noise = tf.random.normal(shape=layer_mean.shape, mean=0.0, stddev=noise_scale)\n",
    "        noisy_layer_mean = layer_mean + noise\n",
    "\n",
    "        # Add FedProx proximal term\n",
    "        proximal_update = -proximal_term * noisy_layer_mean\n",
    "\n",
    "        avg_grad.append(noisy_layer_mean + proximal_update)\n",
    "\n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import ddpg\n",
    "\n",
    "def get_ddpg_agent(observation_spec, action_spec, custom_layers, global_step): \n",
    "    \n",
    "    \"\"\"actor_net = ActorNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "\n",
    "    critic_net = CriticNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "    \n",
    "    target_actor_network = ActorNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "\n",
    "    target_critic_network = CriticNetwork(observation_spec=observation_spec, action_spec=action_spec, custom_layers=custom_layers)\n",
    "    \"\"\"\n",
    "\n",
    "    actor_net = ddpg.actor_network.ActorNetwork(\n",
    "        input_tensor_spec=observation_spec,\n",
    "        output_tensor_spec=action_spec, \n",
    "        fc_layer_params=(32, 32),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "     \n",
    "    critic_net = ddpg.critic_network.CriticNetwork(\n",
    "        input_tensor_spec=(observation_spec, action_spec),\n",
    "        joint_fc_layer_params=(32, 32),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "    target_actor_network = ddpg.actor_network.ActorNetwork(\n",
    "        input_tensor_spec=observation_spec,\n",
    "        output_tensor_spec=action_spec, fc_layer_params=(32, 32),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "    target_critic_network = ddpg.critic_network.CriticNetwork(\n",
    "        input_tensor_spec=(observation_spec, action_spec),\n",
    "        joint_fc_layer_params=(32, 32),\n",
    "        activation_fn=tf.keras.activations.relu)\n",
    "    \n",
    "\n",
    "    agent_params = {\n",
    "        \"time_step_spec\": environments[\"train\"][f\"building_{idx+1}\"].time_step_spec(),\n",
    "        \"action_spec\": environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "        \"actor_network\": actor_net,\n",
    "        \"critic_network\": critic_net,\n",
    "        \"actor_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
    "        \"critic_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-2),\n",
    "        \"ou_stddev\": 0.9,\n",
    "        \"ou_damping\": 0.3,\n",
    "        \"target_actor_network\": target_actor_network,\n",
    "        \"target_critic_network\": target_critic_network,\n",
    "        \"target_update_tau\": 0.05,\n",
    "        \"target_update_period\": 5,\n",
    "        \"dqda_clipping\": None,\n",
    "        \"td_errors_loss_fn\": tf.compat.v1.losses.huber_loss,\n",
    "        \"gamma\": 0.99,\n",
    "        \"reward_scale_factor\": 1.0,\n",
    "        \"train_step_counter\": global_step,\n",
    "    }\n",
    "\n",
    "    # Create the DdpgAgent with unpacked parameters\n",
    "    tf_agent = ddpg_agent.DdpgAgent(**agent_params)\n",
    "\n",
    "    tf_agent.initialize()\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    return tf_agent, eval_policy, collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wandb_logging(project=\"DDPG_battery_testing\", name=\"Exp\", num_iterations=1500, batch_size=1, a_lr=\"1e-4\", c_lr=\"1e-3\"):\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=\"DDPG_battery_testing\",\n",
    "        job_type=\"train_eval_test\",\n",
    "        name=name,\n",
    "        config={\n",
    "            \"train_steps\": num_iterations,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"actor_learning_rate\": 1e-4,\n",
    "            \"critic_learning_rate\": 1e-3}\n",
    "    )\n",
    "    artifact = wandb.Artifact(name='save', type=\"checkpoint\")\n",
    "\n",
    "    \"\"\"train_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir='checkpoints/ddpg/',\n",
    "            max_to_keep=1,\n",
    "            agent=tf_agent,\n",
    "            policy=tf_agent.policy,\n",
    "            replay_buffer=replay_buffer,\n",
    "            global_step=global_step\n",
    "        )\n",
    "        train_checkpointer.initialize_or_restore()\"\"\"\n",
    "\n",
    "    return artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rl_training_pipeline(tf_agent, env_train, replay_buffer_capacity,collect_policy, initial_collect_steps, collect_steps_per_iteration):\n",
    "    \n",
    "    #Setup replay buffer -> TFUniform to give each sample an equal selection chance\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            data_spec=tf_agent.collect_data_spec,\n",
    "            batch_size= env_train.batch_size,\n",
    "            max_length=replay_buffer_capacity,\n",
    "        )\n",
    "\n",
    "    # Populate replay buffer with inital experience before actual training (for num_steps times)\n",
    "    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=env_train,\n",
    "        policy=collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=initial_collect_steps,\n",
    "    )\n",
    "\n",
    "    # After the initial collection phase, the collect driver takes over for the continuous collection of data during the training process\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        env=env_train,\n",
    "        policy=collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=collect_steps_per_iteration,\n",
    "    )\n",
    "\n",
    "    # For better performance\n",
    "    initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "    collect_driver.run = common.function(collect_driver.run)\n",
    "    tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data\n",
    "    initial_collect_driver.run()\n",
    "    time_step = env_train.reset()\n",
    "    policy_state = collect_policy.get_initial_state(env_train.batch_size)\n",
    "\n",
    "    # The dataset is created from the replay buffer in a more structured and efficient way to provide mini-batches\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "        sample_batch_size=env_train.batch_size, num_steps=2).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    #Feed batches of experience to the agent for training\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    return iterator, collect_driver, time_step, policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: Started Federated training round ---------- 1 / 3\n",
      "Cluster 1: Started Federated training round ---------- 1 / 3\n",
      "Cluster 2: Started Federated training round ---------- 1 / 3\n",
      "Cluster 3: Started Federated training round ---------- 1 / 3\n",
      "Cluster 4: Started Federated training round ---------- 1 / 3\n",
      "Cluster 5: Started Federated training round ---------- 1 / 3\n",
      "Cluster 6: Started Federated training round ---------- 1 / 3\n",
      "Cluster 7: Started Federated training round ---------- 1 / 3\n",
      "Cluster 8: Started Federated training round ---------- 1 / 3\n",
      "Cluster 9: Started Federated training round ---------- 1 / 3\n",
      "Cluster 0: Started Federated training round ---------- 2 / 3\n",
      "Cluster 1: Started Federated training round ---------- 2 / 3\n",
      "Cluster 2: Started Federated training round ---------- 2 / 3\n",
      "Cluster 3: Started Federated training round ---------- 2 / 3\n",
      "Cluster 4: Started Federated training round ---------- 2 / 3\n",
      "Cluster 5: Started Federated training round ---------- 2 / 3\n",
      "Cluster 6: Started Federated training round ---------- 2 / 3\n",
      "Cluster 7: Started Federated training round ---------- 2 / 3\n",
      "Cluster 8: Started Federated training round ---------- 2 / 3\n",
      "Cluster 9: Started Federated training round ---------- 2 / 3\n",
      "Cluster 0: Started Federated training round ---------- 3 / 3\n",
      "Cluster 1: Started Federated training round ---------- 3 / 3\n",
      "Cluster 2: Started Federated training round ---------- 3 / 3\n",
      "Cluster 3: Started Federated training round ---------- 3 / 3\n",
      "Cluster 4: Started Federated training round ---------- 3 / 3\n",
      "Cluster 5: Started Federated training round ---------- 3 / 3\n",
      "Cluster 6: Started Federated training round ---------- 3 / 3\n",
      "Cluster 7: Started Federated training round ---------- 3 / 3\n",
      "Cluster 8: Started Federated training round ---------- 3 / 3\n",
      "Cluster 9: Started Federated training round ---------- 3 / 3\n"
     ]
    }
   ],
   "source": [
    "# Setup Agent networks\n",
    "batch_size = 1\n",
    "replay_buffer_capacity = 20000 #Before: 1000000 -> But only <18.000 samples per dataset\n",
    "initial_collect_steps = 2000\n",
    "collect_steps_per_iteration = 20 #2000\n",
    "num_iterations = 500 #10000\n",
    "eval_interval = 4000 #3000\n",
    "\n",
    "global_weights = {\"actor_net\": {}, \"critic_net\": {}, \"target_actor_network\": {}, \"target_critic_network\": {}}\n",
    "\n",
    "#Initalize a global model for each Cluster of similar buildings\n",
    "for cluster in range(num_clusters):\n",
    "       \n",
    "    # 1. Build global agent per cluster\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    first_building_in_cluster = cluster_buildings[cluster][0]\n",
    "\n",
    "    global_tf_agent, global_eval_policy, global_collect_policy = get_ddpg_agent(\n",
    "            observation_spec = environments[\"train\"][f\"building_{first_building_in_cluster}\"].observation_spec(),\n",
    "            action_spec = environments[\"train\"][f\"building_{first_building_in_cluster}\"].action_spec(),\n",
    "            custom_layers = [CustomLayers.get_dense_layers(layers=1, units=32)],\n",
    "            global_step = global_step\n",
    "            )\n",
    "    \n",
    "    # 2. Initially store weights\n",
    "    global_weights[\"actor_net\"][cluster] = global_tf_agent._actor_network.get_weights()\n",
    "    global_weights[\"critic_net\"][cluster] = global_tf_agent._critic_network.get_weights()\n",
    "    global_weights[\"target_actor_network\"][cluster] = global_tf_agent._target_actor_network.get_weights()\n",
    "    global_weights[\"target_critic_network\"][cluster] = global_tf_agent._target_critic_network.get_weights()\n",
    "\n",
    "#Start Federated Learning - For each federated round\n",
    "for federated_round  in range(federated_rounds):\n",
    "    \n",
    "    #Iterate through each cluster\n",
    "    for cluster_number, buildings_in_cluster in cluster_buildings.items():\n",
    "        print(f\"Cluster {cluster_number}: Started Federated training round ----------\", federated_round+1, f\"/ {federated_rounds}\")\n",
    "        \n",
    "        local_actor_weight_list = list()\n",
    "        local_critic_weight_list = list()\n",
    "        local_target_actor_weight_list = list()\n",
    "        local_target_critic_weight_list = list()\n",
    "\n",
    "        #Iterate through the buildings per cluster\n",
    "        for building_index in buildings_in_cluster:\n",
    "            \n",
    "            #0. Reset global step\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "            \n",
    "            #1. Initalize local agent and set global weights\n",
    "            local_tf_agent, local_eval_policy, local_collect_policy = get_ddpg_agent(\n",
    "                observation_spec = environments[\"train\"][f\"building_{building_index}\"].observation_spec(),\n",
    "                action_spec = environments[\"train\"][f\"building_{building_index}\"].action_spec(),\n",
    "                custom_layers = [CustomLayers.get_dense_layers(layers=1, units=32)],\n",
    "                global_step = global_step\n",
    "                )\n",
    "            local_tf_agent._actor_network.set_weights(global_weights[\"actor_net\"][cluster])\n",
    "            local_tf_agent._critic_network.set_weights(global_weights[\"critic_net\"][cluster])\n",
    "            local_tf_agent._target_actor_network.set_weights(global_weights[\"target_actor_network\"][cluster])\n",
    "            local_tf_agent._target_critic_network.set_weights(global_weights[\"target_critic_network\"][cluster])\n",
    "\n",
    "            #2. Prepare training pipeline: Setup iterator, replay buffer, driver\n",
    "            local_iterator, local_collect_driver, local_time_step, local_policy_state = setup_rl_training_pipeline(\n",
    "                local_tf_agent, environments[\"train\"][f\"building_{building_index}\"], replay_buffer_capacity, local_collect_policy, initial_collect_steps, collect_steps_per_iteration\n",
    "                )\n",
    "\n",
    "            #3. Setup wandb logging\n",
    "            #artifact = initialize_wandb_logging(name=f\"Exp_building{building_index}_rd{federated_round+1}\", num_iterations=num_iterations)\n",
    "\n",
    "            #4. Start training\n",
    "            #print(f\"Start training building {building_index+1} - Round {federated_round+1}\")\n",
    "            \n",
    "            eval_metrics = [tf_metrics.AverageReturnMetric(batch_size=batch_size)]\n",
    "            test_metrics = [tf_metrics.AverageReturnMetric(batch_size=batch_size)]\n",
    "\n",
    "            while global_step.numpy() < num_iterations:\n",
    "\n",
    "                #if global_step.numpy() % 50 == 0:\n",
    "                #    print(global_step.numpy(), \"/ \", num_iterations)\n",
    "\n",
    "                local_time_step, local_policy_state = local_collect_driver.run(time_step=local_time_step, policy_state=local_policy_state)\n",
    "                local_experience, _ = next(local_iterator)\n",
    "                local_train_loss = local_tf_agent.train(local_experience)\n",
    "                \n",
    "                metrics = {}\n",
    "                if global_step.numpy() % eval_interval == 0:\n",
    "                    #train_checkpointer.save(global_step)\n",
    "                    metrics = metric_utils.eager_compute(eval_metrics,environments[\"eval\"][f\"building_{building_index}\"],\n",
    "                        local_eval_policy,num_episodes=1,train_step=global_step,summary_writer=None,summary_prefix='',use_function=True)\n",
    "                \n",
    "                #if global_step.numpy() % 2 == 0:\n",
    "                #    metrics[\"loss\"] = local_train_loss.loss\n",
    "                #    wandb.log(metrics)\n",
    "            \n",
    "            #5. Add local agent weights to list\n",
    "            local_actor_weight_list.append(local_tf_agent._actor_network.get_weights())\n",
    "            local_critic_weight_list.append(local_tf_agent._critic_network.get_weights())\n",
    "            local_target_actor_weight_list.append(local_tf_agent._target_actor_network.get_weights())\n",
    "            local_target_critic_weight_list.append(local_tf_agent._target_critic_network.get_weights())\n",
    "\n",
    "        # Performe Federated aggregation\n",
    "        average_actor_weights = avg_weights_with_noise_fedprox(local_actor_weight_list)\n",
    "        average_critic_weights = avg_weights_with_noise_fedprox(local_critic_weight_list) \n",
    "        average_target_actor_weights = avg_weights_with_noise_fedprox(local_target_actor_weight_list) \n",
    "        average_target_critic_weights = avg_weights_with_noise_fedprox(local_target_critic_weight_list)    \n",
    "        \n",
    "        global_tf_agent._actor_network.set_weights(average_actor_weights)\n",
    "        global_tf_agent._critic_network.set_weights(average_critic_weights)\n",
    "        global_tf_agent._target_actor_network.set_weights(average_target_actor_weights)\n",
    "        global_tf_agent._target_critic_network.set_weights(average_target_critic_weights)\n",
    "\n",
    "        \n",
    "        os.makedirs(f\"/models/cluster_{cluster_number}/FLround{federated_round+1}\", exist_ok=True)\n",
    "        \n",
    "        current_directory = os.getcwd()                    \n",
    "        np.savez_compressed(os.path.join(current_directory,f\"/models/cluster_{cluster_number}/FLround{federated_round+1}/actor_weights.npz\"), *global_tf_agent._actor_network.get_weights())\n",
    "        np.savez_compressed(os.path.join(current_directory,f\"/models/cluster_{cluster_number}/FLround{federated_round+1}/critic_weights.npz\"), *global_tf_agent._critic_network.get_weights())\n",
    "        np.savez_compressed(os.path.join(current_directory,f\"/models/cluster_{cluster_number}/FLround{federated_round+1}/target_actor_weights.npz\"), *global_tf_agent._target_actor_network.get_weights())\n",
    "        np.savez_compressed(os.path.join(current_directory,f\"/models/cluster_{cluster_number}/FLround{federated_round+1}/target_critic_weights.npz\"), *global_tf_agent._target_critic_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "Building:  7  - round:  0\n",
      "Start testing ...\n",
      "Building:  14  - round:  0\n",
      "Start testing ...\n",
      "Building:  18  - round:  0\n",
      "Start testing ...\n",
      "Building:  22  - round:  0\n",
      "Start testing ...\n",
      "Building:  23  - round:  0\n",
      "Start testing ...\n",
      "Building:  25  - round:  0\n",
      "Start testing ...\n",
      "Problem at: C:\\Users\\rs1044\\AppData\\Local\\Temp\\ipykernel_26500\\2961659840.py 3 initialize_wandb_logging\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m eval_policy \u001b[38;5;241m=\u001b[39m tf_agent\u001b[38;5;241m.\u001b[39mpolicy\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart testing ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m artifact \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_wandb_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExp_building\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbuilding_index\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_rd\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfederated_round\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39meager_compute(\n\u001b[0;32m     77\u001b[0m     test_metrics,\n\u001b[0;32m     78\u001b[0m     environments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilding_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     79\u001b[0m     eval_policy,\n\u001b[0;32m     80\u001b[0m     num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     82\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog(metrics)\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36minitialize_wandb_logging\u001b[1;34m(project, name, num_iterations, batch_size, a_lr, c_lr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_wandb_logging\u001b[39m(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDPG_battery_testing\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExp\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, a_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1e-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, c_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1e-3\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDDPG_battery_testing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_eval_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactor_learning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcritic_learning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     artifact \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mArtifact(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"train_checkpointer = common.Checkpointer(\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m            ckpt_dir='checkpoints/ddpg/',\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m            max_to_keep=1,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m        train_checkpointer.initialize_or_restore()\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1199\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[0;32m   1198\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m-> 1199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1201\u001b[0m     error_seen \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1176\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1174\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1177\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:756\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating run to backend with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m second timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    755\u001b[0m run_init_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run(run)\n\u001b[1;32m--> 756\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_init_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    762\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrun_result\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    598\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_rounds=1\n",
    "\n",
    "for cluster_number, buildings_in_cluster in cluster_buildings.items():\n",
    "    print(f\"Cluster {cluster_number}:\")\n",
    "    for building_index in buildings_in_cluster:\n",
    "        \n",
    "        for round in range(num_rounds):\n",
    "            print(\"Building: \", building_index, \" - round: \", round)\n",
    "            \n",
    "            # Load global agent\n",
    "            #current_directory = os.getcwd()\n",
    "            #weights_path = os.path.join(current_directory, f\"models/cluster_{cluster_number}/FLround{3}\")\n",
    "            weights_path = f\"models/cluster_{cluster_number}/FLround{3}\"\n",
    "            \n",
    "            # Load weights into global_tf_agent         \n",
    "            actor_net = ddpg.actor_network.ActorNetwork(\n",
    "                input_tensor_spec=environments[\"train\"][f\"building_{building_index}\"].observation_spec(),\n",
    "                output_tensor_spec=environments[\"train\"][f\"building_{building_index}\"].action_spec(), \n",
    "                fc_layer_params=(32, 32),\n",
    "                activation_fn=tf.keras.activations.relu)\n",
    "            \n",
    "            critic_net = ddpg.critic_network.CriticNetwork(\n",
    "                input_tensor_spec=(environments[\"train\"][f\"building_{building_index}\"].observation_spec(), environments[\"train\"][f\"building_{building_index}\"].action_spec()),\n",
    "                joint_fc_layer_params=(32, 32),\n",
    "                activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "            target_actor_network = ddpg.actor_network.ActorNetwork(\n",
    "                input_tensor_spec=environments[\"train\"][f\"building_{building_index}\"].observation_spec(),\n",
    "                output_tensor_spec=environments[\"train\"][f\"building_{building_index}\"].action_spec(), fc_layer_params=(32, 32),\n",
    "                activation_fn=tf.keras.activations.relu)\n",
    "\n",
    "            target_critic_network = ddpg.critic_network.CriticNetwork(\n",
    "                input_tensor_spec=(environments[\"train\"][f\"building_{building_index}\"].observation_spec(), environments[\"train\"][f\"building_{building_index}\"].action_spec()),\n",
    "                joint_fc_layer_params=(32, 32),\n",
    "                activation_fn=tf.keras.activations.relu)\n",
    "            \n",
    "\n",
    "            agent_params = {\n",
    "                \"time_step_spec\": environments[\"train\"][f\"building_{idx+1}\"].time_step_spec(),\n",
    "                \"action_spec\": environments[\"train\"][f\"building_{idx+1}\"].action_spec(),\n",
    "                \"actor_network\": actor_net,\n",
    "                \"critic_network\": critic_net,\n",
    "                \"actor_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
    "                \"critic_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-2),\n",
    "                \"ou_stddev\": 0.9,\n",
    "                \"ou_damping\": 0.3,\n",
    "                \"target_actor_network\": target_actor_network,\n",
    "                \"target_critic_network\": target_critic_network,\n",
    "                \"target_update_tau\": 0.05,\n",
    "                \"target_update_period\": 5,\n",
    "                \"dqda_clipping\": None,\n",
    "                \"td_errors_loss_fn\": tf.compat.v1.losses.huber_loss,\n",
    "                \"gamma\": 0.99,\n",
    "                \"reward_scale_factor\": 1.0,\n",
    "                \"train_step_counter\": global_step,\n",
    "            }\n",
    "\n",
    "            # Create the DdpgAgent with unpacked parameters\n",
    "            tf_agent = ddpg_agent.DdpgAgent(**agent_params)\n",
    "\n",
    "            actor_weights = np.load(f\"C:\\models/cluster_{cluster_number}/FLround{3}/actor_weights.npz\", allow_pickle=True)\n",
    "            critic_weights = np.load(f\"C:\\models/cluster_{cluster_number}/FLround{3}/critic_weights.npz\", allow_pickle=True)\n",
    "            target_actor_weights = np.load(f\"C:\\models/cluster_{cluster_number}/FLround{3}/target_actor_weights.npz\", allow_pickle=True)\n",
    "            target_critic_weights = np.load(f\"C:\\models/cluster_{cluster_number}/FLround{3}/target_critic_weights.npz\", allow_pickle=True)\n",
    "\n",
    "            tf_agent._actor_network.set_weights([actor_weights[f'arr_{i}'] for i in range(len(actor_weights.files))])\n",
    "            tf_agent._critic_network.set_weights([critic_weights[f'arr_{i}'] for i in range(len(critic_weights.files))])\n",
    "            tf_agent._target_actor_network.set_weights([target_actor_weights[f'arr_{i}'] for i in range(len(target_actor_weights.files))])\n",
    "            tf_agent._target_critic_network.set_weights([target_critic_weights[f'arr_{i}'] for i in range(len(target_critic_weights.files))])\n",
    "\n",
    "            tf_agent.initialize()\n",
    "            eval_policy = tf_agent.policy\n",
    "            \n",
    "            print(\"Start testing ...\")\n",
    "            artifact = initialize_wandb_logging(name=f\"Exp_building{building_index}_rd{federated_round+1}\", num_iterations=num_iterations)\n",
    "            metrics = metric_utils.eager_compute(\n",
    "                test_metrics,\n",
    "                environments[\"test\"][f\"building_{building_index}\"],\n",
    "                eval_policy,\n",
    "                num_episodes=1\n",
    "                )\n",
    "            wandb.log(metrics)\n",
    "            #artifact.add_dir(local_path='checkpoints/ddpg/')\n",
    "            wandb.log_artifact(artifact)\n",
    "            wandb.finish()\n",
    "            tf.compat.v1.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
