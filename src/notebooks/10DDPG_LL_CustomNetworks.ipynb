{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "\n",
    "logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils.reinforcementLearningHelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1 / State Space: 6 / Action Space: 1 / Upper bound: 2.3\n"
     ]
    }
   ],
   "source": [
    "#Setup Environments of selected buildings for training, evaluation, and testing\n",
    "\n",
    "environments, observation_spec, action_spec  = setup_energymanagement_environments(num_buildings=30)\n",
    "\n",
    "#Check environment setup\n",
    "print(\n",
    "    \"Batch size:\", environments[\"train\"][f\"building_1\"].batch_size, \n",
    "    \"/ State Space: {} / Action Space: {}\".format(observation_spec.shape[0], action_spec.shape[0]),\n",
    "    \"/ Upper bound: {}\".format(round(environments[\"train\"][f\"building_1\"].action_spec().maximum.item(), 3)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Agent networks\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "batch_size = 128\n",
    "replay_buffer_capacity = 20000 \n",
    "initial_collect_steps = 2000\n",
    "collect_steps_per_iteration = 20 \n",
    "num_iterations = 10000 \n",
    "eval_interval = 9990\n",
    "\n",
    "num_rounds = 1\n",
    "num_buildings = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.networks import encoding_network\n",
    "from tf_agents.utils import common as common_utils\n",
    "from tf_agents.networks import utils\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "class ActorNetwork(network.Network):\n",
    "\n",
    "    def __init__(self, observation_spec,action_spec, custom_layers=None, dropout_rate=0.2, name='CustomActorNetwork'):\n",
    "        \n",
    "        super(ActorNetwork, self).__init__(input_tensor_spec=observation_spec, state_spec=(), name=name)\n",
    "\n",
    "        self._action_spec = action_spec\n",
    "    \n",
    "        # Initialize the custom Keras layers\n",
    "        if custom_layers is not None:\n",
    "            self._custom_layers = custom_layers\n",
    "        else:\n",
    "            self._custom_layers = [\n",
    "                tf.keras.layers.Reshape((observation_spec.shape[0], -1)),  # Adjust as per your actual sequence length and features\n",
    "                tf.keras.layers.LSTM(8, return_sequences=True),\n",
    "                tf.keras.layers.LSTM(8),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "            \n",
    "            #[\n",
    "            #    tf.keras.layers.Reshape((observation_spec.shape[0], -1)),\n",
    "            #    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "            #    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "            #    tf.keras.layers.BatchNormalization(),\n",
    "            #    tf.keras.layers.Dropout(dropout_rate),\n",
    "            #    tf.keras.layers.Flatten(),  # Flatten the output before passing it to dense layers\n",
    "            #    tf.keras.layers.Dense(256, activation='relu'),\n",
    "            #    tf.keras.layers.BatchNormalization(),\n",
    "            #    tf.keras.layers.Dropout(dropout_rate),\n",
    "            #]\n",
    "            \n",
    "            #[\n",
    "            #    tf.keras.layers.Dense(256, activation='relu'),\n",
    "            #    tf.keras.layers.BatchNormalization(),\n",
    "            #    tf.keras.layers.Dropout(dropout_rate),\n",
    "            #    tf.keras.layers.Dense(256, activation='relu'),\n",
    "            #    tf.keras.layers.BatchNormalization(),\n",
    "            #    tf.keras.layers.Dropout(dropout_rate),\n",
    "            #]\n",
    "            \n",
    "        # Initialize Output layer -> output_dim = action_spec\n",
    "        self._action_layer = tf.keras.layers.Dense(units= action_spec.shape[0], activation='tanh')\n",
    "\n",
    "\n",
    "    def call(self, observations, step_type=(), network_state=()):\n",
    "        \n",
    "        #Preprocess Input\n",
    "        outer_rank = nest_utils.get_outer_rank(observations, self.input_tensor_spec)\n",
    "        batch_squash = utils.BatchSquash(outer_rank)\n",
    "        observations = tf.nest.map_structure(batch_squash.flatten, observations)\n",
    "        observations_flat = tf.nest.flatten(observations)\n",
    "\n",
    "        # Custom Layers\n",
    "        state = tf.concat(observations_flat, axis=-1)\n",
    "        for layer in self._custom_layers:\n",
    "            state = layer(state)\n",
    "        \n",
    "        #Output layer\n",
    "        actions = self._action_layer(state)\n",
    "        actions = common_utils.scale_to_spec(actions, self._action_spec)\n",
    "        actions = batch_squash.unflatten(actions)\n",
    "\n",
    "        return tf.nest.pack_sequence_as(self._action_spec, [actions]), network_state\n",
    "\n",
    "\n",
    "class CriticNetwork(network.Network):\n",
    "\n",
    "    def __init__(self, observation_spec, action_spec, custom_layers=None, dropout_rate=0.2, name='CustomCriticNetwork'):\n",
    "        \n",
    "        super(CriticNetwork, self).__init__(input_tensor_spec=(observation_spec, action_spec), state_spec=(), name=name)\n",
    "\n",
    "        self._obs_spec = observation_spec\n",
    "        self._action_spec = action_spec\n",
    "                \n",
    "        # Encoding layer concatenates state and action inputs, adds dense layer:\n",
    "        kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1./3., mode='fan_in', distribution='uniform')\n",
    "        combiner = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self._encoder = encoding_network.EncodingNetwork(\n",
    "            (observation_spec, action_spec),\n",
    "            fc_layer_params=(64,),\n",
    "            preprocessing_combiner = combiner,\n",
    "            activation_fn = tf.keras.activations.relu,\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            batch_squash=True)\n",
    "\n",
    "        # Initialize the custom tf layers here:\n",
    "        if custom_layers is not None:\n",
    "            self._custom_layers = custom_layers\n",
    "        else:\n",
    "            self._custom_layers = [\n",
    "                tf.keras.layers.Dense(256, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "                tf.keras.layers.Dense(256, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "     \n",
    "        # Initialize the value layer -> output_dim = 1 (Q-Value)\n",
    "        self._value_layer = tf.keras.layers.Dense(\n",
    "            units= 1,\n",
    "            activation=tf.keras.activations.linear,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.003, maxval=0.003),\n",
    "            name='Value')  # Q-function output\n",
    "\n",
    "\n",
    "    def call(self, observations, step_type=(), network_state=()):\n",
    "        # Forward pass through the custom tf layers here (defined above):\n",
    "        state, network_state = self._encoder(observations, step_type=step_type, network_state=network_state)\n",
    "                          \n",
    "        # Apply custom layers\n",
    "        for layer in self._custom_layers:\n",
    "            state = layer(state)\n",
    "        \n",
    "        value = self._value_layer(state)\n",
    "    \n",
    "        return tf.reshape(value, [-1]), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Weights initialized: -0.40083313\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "observation_spec = tensor_spec.TensorSpec([6], tf.float32)\n",
    "action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n",
    "dummy_obs = tf.random.uniform([1, 6])\n",
    "\n",
    "actor_network = ActorNetwork(observation_spec, action_spec)\n",
    "# Create a dummy step_type and network_state to match the call signature\n",
    "dummy_step_type = tf.constant([0])\n",
    "dummy_network_state = ()\n",
    "\n",
    "# Call the network with dummy data to initialize weights\n",
    "_ = actor_network(dummy_obs, dummy_step_type, dummy_network_state)\n",
    "\n",
    "# Now, you should be able to get and set weights\n",
    "weights = actor_network.get_weights()\n",
    "print(\"Weights initialized:\", weights[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights initialized: 0.37675747\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "# Assuming CriticNetwork is defined somewhere else as per your setup\n",
    "critic_network = CriticNetwork(observation_spec, action_spec)\n",
    "\n",
    "observation_spec = tensor_spec.TensorSpec([6], tf.float32)\n",
    "action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n",
    "dummy_obs = tf.random.uniform([1, 6])\n",
    "dummy_action = tf.random.uniform([1, 1], minval=-1, maxval=1)\n",
    "\n",
    "# Adjusted to fit the critic network call signature which typically takes both observations and actions\n",
    "_ = critic_network((dummy_obs, dummy_action), dummy_step_type, dummy_network_state)\n",
    "\n",
    "weights_c = critic_network.get_weights()\n",
    "print(\"Weights initialized:\", weights_c[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_custom_ddpg_agent(observation_spec, action_spec, global_step, environments): \n",
    "    \n",
    "    actor_network = ActorNetwork(observation_spec, action_spec)\n",
    "    critic_network = CriticNetwork(observation_spec, action_spec)\n",
    "    target_actor_network = ActorNetwork(observation_spec, action_spec)\n",
    "    target_critic_network = CriticNetwork(observation_spec, action_spec)\n",
    "    \n",
    "    agent_params = {\n",
    "        \"time_step_spec\": environments[\"train\"][f\"building_{1}\"].time_step_spec(),\n",
    "        \"action_spec\": environments[\"train\"][f\"building_{1}\"].action_spec(),\n",
    "        \"actor_network\": actor_network,\n",
    "        \"critic_network\": critic_network,\n",
    "        \"actor_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3), #1e-3\n",
    "        \"critic_optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4), #1e-2\n",
    "        \"ou_stddev\": 0.9, #0.9,\n",
    "        \"ou_damping\": 0.15,\n",
    "        \"target_actor_network\": target_actor_network,\n",
    "        \"target_critic_network\": target_critic_network,\n",
    "        \"target_update_tau\": 0.05,\n",
    "        \"target_update_period\": 100, #5,\n",
    "        \"dqda_clipping\": 0.5,\n",
    "        \"td_errors_loss_fn\": tf.compat.v1.losses.huber_loss,\n",
    "        \"gamma\": 1, #0.99,\n",
    "        \"reward_scale_factor\": 1,\n",
    "        \"train_step_counter\": global_step,\n",
    "    }\n",
    "\n",
    "    # Create the DdpgAgent with unpacked parameters\n",
    "    ddpg_tf_agent = ddpg_agent.DdpgAgent(**agent_params)\n",
    "\n",
    "    ddpg_tf_agent.initialize()\n",
    "    eval_policy = ddpg_tf_agent.policy\n",
    "    collect_policy = ddpg_tf_agent.collect_policy\n",
    "\n",
    "    return ddpg_tf_agent, eval_policy, collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building:  1  - round:  0\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\utils\\reinforcementLearningHelper.py:340: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df, pd.DataFrame({'Building': [building_index], 'Total Profit': [wandb.summary[\"Final Profit\"]]})], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building:  1  - Total Profit:  162.54106470999923\n",
      "Building:  2  - round:  0\n",
      "Building:  2  - Total Profit:  -24.31308116627295\n",
      "Building:  3  - round:  0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function AverageReturnMetric.reset at 0x00000287690F6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x0000028767B596F0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TFDeque.mean at 0x00000287684E3BE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function AverageReturnMetric.reset at 0x00000287684E3B50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x0000028767B5BE80>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TFDeque.mean at 0x00000287684E0DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Building:  3  - Total Profit:  172.84293097528013\n",
      "Building:  4  - round:  0\n",
      "Building:  4  - Total Profit:  9.961887182752822\n",
      "Building:  5  - round:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m artifact \u001b[38;5;241m=\u001b[39m initialize_wandb_logging(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExp_DDPG_LL_Home\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilding_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rd\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39mnum_iterations)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#4. Train, evaluate agent and store weights\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m result_df, metrics \u001b[38;5;241m=\u001b[39m \u001b[43magent_training_and_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_driver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilding_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#5. End and log wandb\u001b[39;00m\n\u001b[0;32m     30\u001b[0m end_and_log_wandb(metrics, artifact)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\src\\notebooks\\..\\utils\\reinforcementLearningHelper.py:326\u001b[0m, in \u001b[0;36magent_training_and_evaluation\u001b[1;34m(global_step, num_test_iterations, collect_driver, time_step, policy_state, iterator, tf_agent, eval_policy, building_index, result_df, eval_interval, environments)\u001b[0m\n\u001b[0;32m    321\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m [tf_metrics\u001b[38;5;241m.\u001b[39mAverageReturnMetric()]\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m global_step\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m<\u001b[39m num_test_iterations:\n\u001b[0;32m    324\u001b[0m     \n\u001b[0;32m    325\u001b[0m     \u001b[38;5;66;03m#Training\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     time_step, policy_state \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     experience, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m    328\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m tf_agent\u001b[38;5;241m.\u001b[39mtrain(experience)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Federated-Reinforcement-Learning-for-Battery-Charging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LOCAL LEARNING\n",
    "\n",
    "result_df = pd.DataFrame(columns=['Building', 'Total Profit'])\n",
    "\n",
    "for idx in range(num_buildings):\n",
    "    for round in range(num_rounds):\n",
    "        building_index=idx+1\n",
    "        print(\"Building: \", building_index, \" - round: \", round)\n",
    "        #0. Reset global step\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "        \n",
    "        #1. Initalize agent\n",
    "        tf_agent, eval_policy, collect_policy = initialize_custom_ddpg_agent(observation_spec, action_spec, global_step, environments)\n",
    "\n",
    "        #2. Prepare training pipeline: Setup iterator, replay buffer, driver\n",
    "        iterator, collect_driver, time_step, policy_state = setup_rl_training_pipeline(\n",
    "            tf_agent, environments[\"train\"][f\"building_{building_index}\"], replay_buffer_capacity,collect_policy, \n",
    "            initial_collect_steps, collect_steps_per_iteration, batch_size\n",
    "            )\n",
    "\n",
    "        #3. Setup wandb logging\n",
    "        artifact = initialize_wandb_logging(name=f\"Exp_DDPG_LL_Home{building_index}_rd{round}\", num_iterations=num_iterations)\n",
    "\n",
    "        #4. Train, evaluate agent and store weights\n",
    "        result_df, metrics = agent_training_and_evaluation(global_step, num_iterations, collect_driver, \n",
    "                time_step, policy_state, iterator, tf_agent, eval_policy, building_index, result_df, eval_interval, environments)\n",
    "            \n",
    "        #5. End and log wandb\n",
    "        end_and_log_wandb(metrics, artifact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
